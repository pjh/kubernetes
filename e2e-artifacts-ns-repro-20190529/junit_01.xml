<?xml version="1.0" encoding="UTF-8"?>
  <testsuite tests="183" failures="25" time="32587.58054587">
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to create an internal type load balancer [Slow] [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001880119">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Cluster level logging implemented by Stackdriver should ingest events [Feature:StackdriverLogging]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should allow substituting values in a container&#39;s args [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="207.355602155"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment deployment should support rollover [Conformance]" classname="Kubernetes e2e suite" time="65.230960038"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes CSI Topology test using GCE PD driver [Serial] should provision zonal PD with delayed volume binding and mount the volume to a pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.001917219">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI Volume expansion [Feature:ExpandCSIVolumes] should expand volume by restarting pod if attach=on, nodeExpansion=on" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Mounted volume expand Should verify mounted devices can be resized" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController should update PodDisruptionBudget status" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify PVC creation with an invalid VSAN capability along with a compatible zone combination specified in storage class fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.001817311">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide container&#39;s limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.351412072"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] should scale up twice [Feature:ClusterAutoscalerScalability2]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicaSet should serve a basic image on each replica with a private image" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify if a non-existing SPBM policy is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.00205508">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Upgrade [Feature:Upgrade] master upgrade should maintain a functioning cluster [Feature:MasterUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should update nodePort: http [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run rc should create an rc from an image  [Conformance]" classname="Kubernetes e2e suite" time="33.253666778"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run deployment should create a deployment from an image  [Conformance]" classname="Kubernetes e2e suite" time="26.6658724"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Nodes [Disruptive] Resize [Slow] should be able to delete nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Disk Format [Feature:vsphere] verify disk format type - zeroedthick is honored for dynamically provisioned pv using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be submitted and removed  [Conformance]" classname="Kubernetes e2e suite" time="24.083640269"></testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support seccomp alpha runtime/default annotation [Feature:Seccomp] [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]" classname="Kubernetes e2e suite" time="237.608076541"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull non-existing image from gcr.io [NodeConformance]" classname="Kubernetes e2e suite" time="11.254811432"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce multiple, stacked policies with overlapping podSelectors [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks detach in a disrupted environment [Slow] [Disruptive] when node is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.0020316">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should disable node pool autoscaling [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs should create a non-pre-bound PV and PVC: test write access " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Cluster level logging implemented by Stackdriver should ingest logs [Feature:StackdriverLogging]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should release NodePorts on delete" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should conform to Ingress spec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting a non-existent configmap should exit with the Forbidden error, not a NotFound error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="96.35893803"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should scale up GPU pool from 1 [GpuType:] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.002208843">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-service-catalog] [Feature:PodPreset] PodPreset should not modify the pod on conflict" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment deployment should support rollback" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001922053">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001692693">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl expose should create services for rc  [Conformance]" classname="Kubernetes e2e suite" time="39.749381297"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001226102">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should *not* be restarted with a non-local redirect http liveness probe" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes CSI Topology test using GCE PD driver [Serial] should fail to schedule a pod with a zone missing from AllowedTopologies; PD is provisioned with immediate volume binding" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DaemonRestart [Disruptive] Controller Manager should not create/delete replicas across restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run default should create an rc or deployment from an image  [Conformance]" classname="Kubernetes e2e suite" time="8.635609685"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] host cleanup with volume mounts [sig-storage][HostCleanup][Flaky] Host cleanup after disrupting NFS volume [NFS] after stopping the nfs-server and deleting the (sleeping) client pod, the NFS mount and the pod&#39;s UID directory should be removed." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]" classname="Kubernetes e2e suite" time="14.34566101"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on multiple zones specified in storage class " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should not emit unexpected warnings" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="42.86642607"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Etcd failure [Disruptive] should recover from SIGKILL" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning errors [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [sig-windows] Networking Granular Checks: Pods should function for intra-pod communication: udp" classname="Kubernetes e2e suite" time="58.535221673"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]" classname="Kubernetes e2e suite" time="38.454079738"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should check NodePort out-of-range" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [ReplicationController] should eagerly create replacement pod during network partition when termination grace is non-zero" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 4 PVs and 2 PVCs: test write access [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should scrape container metrics from all nodes." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PV Protection Verify &#34;immediate&#34; deletion of a PV that is not bound to a PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS when invoking the Recycle reclaim policy should test that a PV becomes Available and is clean after the PVC is deleted." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with Custom Metric of type Object from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pods are pending due to host port conflict [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should provision storage in the allowedTopologies with delayed binding [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver [IPv4] Change stubDomain should be able to change stubDomain configuration [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should create NEGs for all ports with the Ingress annotation, and NEGs for the standalone annotation otherwise" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Local volume that cannot be mounted [Slow] should fail due to non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] Multi-AZ Cluster Volumes [sig-storage] should schedule pods in the same zones as statically provisioned PVs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DaemonRestart [Disruptive] Scheduler should continue assigning pods to nodes across restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner Default should create and delete default persistent volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001848121">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret Should fail non-optional pod creation due to secret object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should have accelerator metrics [Feature:StackdriverAcceleratorMonitoring]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume limits should verify that all nodes have volume limits" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [sig-windows] Networking Granular Checks: Pods should function for intra-pod communication: http" classname="Kubernetes e2e suite" time="82.876350522"></testcase>
      <testcase name="[sig-network] Services should work after restarting kube-proxy [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] Services should be able to create a functioning NodePort service for Windows" classname="Kubernetes e2e suite" time="126.699868047"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Reboot [Disruptive] [Feature:Reboot] each node by ordering clean reboot and ensure they function upon restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001914141">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.001680264">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should be restarted with a local redirect http liveness probe" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001661754">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.451454467"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="16.115381942"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [Feature:Example] [k8s.io] Secret should create a pod that reads a secret" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001530553">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver [Feature:Networking-IPv6] Forward PTR lookup should forward PTR records lookup to upstream nameserver [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should have cluster metrics [Feature:StackdriverMonitoring]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should have session affinity work for LoadBalancer service with ESIPP off [Slow] [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should mutate pod and apply defaults after mutation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Downward API [Serial] [Disruptive] [NodeFeature:EphemeralStorage] Downward API tests for local ephemeral storage should provide default limits.ephemeral-storage from node allocatable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify clean up of stale dummy VM for dynamically provisioned pvc using SPBM policy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run pod should create a pod from an image when restart is Never  [Conformance]" classname="Kubernetes e2e suite" time="10.728945606"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pending pods are small and one node is broken [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:Performance] should allow starting 30 pods per node using ReplicationController with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.002220784">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide container&#39;s memory request [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="18.479228751"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ClusterDns [Feature:Example] should create pod that uses dns" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.000752052">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should mutate custom resource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Flexvolumes should be mountable when attachable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] PreemptionExecutionPath runs ReplicaSets to verify preemption running path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should support port-forward" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support seccomp alpha unconfined annotation on the pod [Feature:Seccomp] [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.000652355">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment test Deployment ReplicaSet orphaning and adoption regarding controllerRef" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]" classname="Kubernetes e2e suite" time="68.465616066"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  StatefulSet with pod affinity [Slow] should use volumes spread across nodes when pod has anti-affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.00198143">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001702318">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify if a SPBM policy is not honored on a non-compatible datastore for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement should create and delete pod with the same volume source attach/detach to different worker nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="Recreate [Feature:Recreate] recreate nodes and ensure they function upon restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.442820195"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify PVC creation fails if only storage policy is specified in the storage class (No shared datastores exist among all the nodes)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI workload information using mock driver should be passed when podInfoOnMount=true" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Advanced Audit [DisabledForLargeClusters][Flaky] should audit API calls to create, get, update, patch, delete, list, watch pods." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI Volume expansion [Feature:ExpandCSIVolumes] should expand volume without restarting pod if nodeExpansion=off" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should allow ingress access on one named port [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Reboot [Disruptive] [Feature:Reboot] each node by dropping all inbound packets for a while and ensure they function afterwards" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.001119069">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should work for type=NodePort" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Kubelet [Serial] [Slow] [k8s.io] [sig-node] regular resource usage tracking resource tracking for 100 pods per node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to change the type from ExternalName to NodePort" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should provide secure master service  [Conformance]" classname="Kubernetes e2e suite" time="8.000957593"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should get a host IP [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="32.14512698"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="18.69407295"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController should create a PodDisruptionBudget" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to create a functioning NodePort service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for endpoint-Service: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] vsphere cloud provider stress [Feature:vsphere] vsphere stress tests" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes [Feature:ReclaimPolicy] [sig-storage] persistentvolumereclaim:vsphere should retain persistent volume when reclaimPolicy set to retain when associated claim is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for API chunking should support continue listing from the last key if the original version has been compacted away, though the list is inconsistent" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should not deadlock when a pod&#39;s predecessor fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] deletion should be idempotent" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl create quota should reject quota with invalid scopes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl client-side validation should create/apply a CR with unknown fields for CRD with no validation schema" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Deploy clustered applications [Feature:StatefulSet] [Slow] should creating a working redis cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule a pod w/ RW PD(s) mounted to 1 or more containers, write to PD, verify content, delete pod, and repeat in rapid succession [Slow] using 4 containers and 1 PDs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl logs should be able to retrieve and filter logs  [Conformance]" classname="Kubernetes e2e suite" time="22.766077323"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify an if a SPBM policy and VSAN capabilities cannot be honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should provide DNS for ExternalName services [Conformance]" classname="Kubernetes e2e suite" time="70.74984218"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.00192686">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should correctly scale down after a node is not needed when there is non autoscaled pool[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Feature:CustomResourcePublishOpenAPI] removes definition from spec when one versin gets changed to not be served" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PVC Protection Verify that PVC in active use by a pod is not removed immediately" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to switch session affinity for LoadBalancer service with ESIPP on [Slow] [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support volume SELinux relabeling when using hostPID [Flaky] [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide container&#39;s cpu request [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.383362298"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.00198561">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 100 pods per node using ReplicationController with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="90.760384651"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001886344">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]" classname="Kubernetes e2e suite" time="9.334481634"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.002222153">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPath should support subPath [NodeConformance]" classname="Kubernetes e2e suite" time="24.632647485"></testcase>
      <testcase name="[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should scrape metrics from annotated pods." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] Windows volume mounts  check volume mount permissions container should have readOnly permissions on emptyDir" classname="Kubernetes e2e suite" time="41.766403608"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthenticator] The kubelet&#39;s main port 10250 should reject requests with no credentials" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PVC Protection Verify &#34;immediate&#34; deletion of a PVC that is not in active use by a pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide container&#39;s memory limit [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="16.424894445"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with readonly rootfs when readOnlyRootFilesystem=true [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.001886772">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] new files should be created with FSGroup ownership when container is root" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide container&#39;s cpu limit [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.588209225"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Service endpoints latency should not be very high  [Conformance]" classname="Kubernetes e2e suite" time="33.764651429"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]" classname="Kubernetes e2e suite" time="8.416388861"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should deny crd creation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] RuntimeClass should run a Pod requesting a RuntimeClass with a configured handler [NodeFeature:RuntimeHandler]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001898194">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Ports Security Check [Feature:KubeletSecurity] should not have port 10255 open on its all public IP addresses" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment deployment reaping should cascade to its replica sets and pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should serve multiport endpoints from pods  [Conformance]" classname="Kubernetes e2e suite" time="52.80687577"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using Job.batch with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: maxUnavailable allow single eviction, percentage =&gt; should allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify PVC creation with incompatible zone along with compatible storagePolicy and datastore combination specified in storage class fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid hostFailuresToTolerate and cacheReservation values is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image from gcr.io [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Verify Volume Attach Through vpxd Restart [Feature:vsphere][Serial][Disruptive] verify volume remains attached through vpxd restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 3 PVs and 3 PVCs: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="16.467786966"></testcase>
      <testcase name="[sig-network] Services should be able to change the type from NodePort to ExternalName" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="24.614385949"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume FStype [Feature:vsphere] verify fstype - ext3 formatted volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Deploy clustered applications [Feature:StatefulSet] [Slow] should creating a working zookeeper cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume FStype [Feature:vsphere] verify invalid fstype" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.003397747">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a persistent volume claim with a storage class. [sig-storage]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide podname as non-root with fsgroup [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should be able to deny custom resource creation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should test that deleting a claim before the volume is provisioned deletes the volume." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting a non-existent secret should exit with the Forbidden error, not a NotFound error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume as non-root with FSGroup [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.003317028">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should resign the bootstrap tokens when the clusterInfo ConfigMap updated [Serial][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] [Feature:Windows] Memory Limits [Serial] [Slow] attempt to deploy past allocatable memory limits should fail deployments of pods once there isn&#39;t enough memory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should be able to scale a node group down to 0[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid objectSpaceReservation and iopsLimit values is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Certificates API should support building a client with a CSR" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify dynamically created pv with multiple zones specified in the storage class, shows both the zones on its labels" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Cluster level logging implemented by Stackdriver [Feature:StackdriverLogging] [Soak] should ingest logs from applications running for a prolonged amount of time" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]" classname="Kubernetes e2e suite" time="20.117834552"></testcase>
      <testcase name="[sig-storage] Pod Disks detach in a disrupted environment [Slow] [Disruptive] when node&#39;s API object is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001844314">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: maxUnavailable deny evictions, integer =&gt; should not allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="158.625731319"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] CA ignores unschedulable pods while scheduling schedulable pods [Feature:ClusterAutoscalerScalability6]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.001210418">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001118935">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DaemonRestart [Disruptive] Kubelet should not restart containers across restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] gpu Upgrade [Feature:GPUUpgrade] cluster downgrade should be able to run gpu pod after downgrade [Feature:GPUClusterDowngrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Kibana Logging Instances Is Alive [Feature:Elasticsearch] should check that the Kibana logging instance is alive" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Feature:CustomResourcePublishOpenAPI] works for multiple CRDs of same group but different versions" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should return command exit codes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Pod with node different from PV&#39;s NodeAffinity should fail scheduling due to different NodeSelector" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes CSI Topology test using GCE PD driver [Serial] should fail to schedule a pod with a zone missing from AllowedTopologies; PD is provisioned with delayed volume binding" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should update endpoints: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule pods each with a PD, delete pod and verify detach [Slow] for RW PD with pod delete grace period of &#34;immediate (0s)&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 2 PVs and 4 PVCs: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] PodSecurityPolicy should allow pods under the privileged policy.PodSecurityPolicy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t be able to scale down when rescheduling a pod is required, but pdb doesn&#39;t allow drain[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001055258">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]" classname="Kubernetes e2e suite" time="36.132274283"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl api-versions should check if v1 is in available api versions  [Conformance]" classname="Kubernetes e2e suite" time="8.4401508"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should support https-only annotation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]" classname="Kubernetes e2e suite" time="14.377167734"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should support exec through an HTTP proxy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should provision storage [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [StatefulSet] should not reschedule stateful pods if there is a network partition [Slow] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should *not* be restarted with a exec &#34;cat /tmp/health&#34; liveness probe [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="257.192903363"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001996936">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001724082">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="16.485187885"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects NO client request should support a client that connects, sends DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner allowedTopologies should create persistent volume in the zone specified in allowedTopologies of storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="44.499388163"></testcase>
      <testcase name="[sig-apps] CronJob should schedule multiple jobs concurrently" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001505047">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:ScopeSelectors] should verify ResourceQuota with terminating scopes through scope selectors." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with External Metric with target value from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.42444167"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image from docker hub [NodeConformance]" classname="Kubernetes e2e suite" time="16.511338398"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]" classname="Kubernetes e2e suite" time="25.039730193"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid diskStripes and objectSpaceReservation values and a VSAN datastore is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001996685">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] kube-proxy migration [Feature:KubeProxyDaemonSetMigration] Upgrade kube-proxy from static pods to a DaemonSet should maintain a functioning cluster [Feature:KubeProxyDaemonSetUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to change the type from ExternalName to ClusterIP" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Docker Containers should be able to override the image&#39;s default command (docker entrypoint) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="18.465848322"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] A node shouldn&#39;t be able to create another node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support pod.Spec.SecurityContext.SupplementalGroups [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.001995472">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Node Poweroff [Feature:vsphere] [Slow] [Disruptive] verify volume status after node power off" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.001777949">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks detach in a disrupted environment [Slow] [Disruptive] when pod is evicted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to change the type from ClusterIP to ExternalName" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Etcd failure [Disruptive] should recover from network partition with master" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:HighDensityPerformance] should allow starting 95 pods per node using ReplicationController with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should resolve DNS of partial qualified names for services " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001820154">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] should scale down empty nodes [Feature:ClusterAutoscalerScalability3]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Ports Security Check [Feature:KubeletSecurity] should not be able to proxy to cadvisor port 4194 using proxy subresource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify PVC creation fails if no zones are specified in the storage class (No shared datastores exist among all the nodes)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking should provide Internet connection for containers [Feature:Networking-IPv6][Experimental]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should scrape metrics from annotated services." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: enough pods, absolute =&gt; should allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull from private registry without secret [NodeConformance]" classname="Kubernetes e2e suite" time="12.336931343"></testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Shouldn&#39;t perform scale up operation and should list unhealthy status if most of the cluster is broken[Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.002069521">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume expand Verify if editing PVC allows resize" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Mounted flexvolume expand[Slow] Should verify mounted flex volumes can be resized" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PVC Protection Verify that scheduling of a pod that uses PVC that is being deleted fails and the pod becomes Unschedulable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] A node shouldn&#39;t be able to delete another node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.00183597">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.000978069">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.000941216">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl patch should add annotations for pods in rc  [Conformance]" classname="Kubernetes e2e suite" time="31.853183932"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should implement legacy replacement when the update strategy is OnDelete" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.001902083">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should remove from active list jobs that have been deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node Random with 0 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.001834457">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI Volume expansion [Feature:ExpandCSIVolumes] should expand volume by restarting pod if attach=off, nodeExpansion=on" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001947963">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against 2 pods with different priority class." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should delete a job [Conformance]" classname="Kubernetes e2e suite" time="58.069806051"></testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support container.SecurityContext.RunAsUser [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI attach test using mock driver should preserve attachment policy when no CSIDriver present" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: enough pods, replicaSet, percentage =&gt; should allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.00235483">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="49.338395259"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001761767">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should succeed in writing subpaths in container [Feature:VolumeSubpathEnvExpansion][NodeAlphaFeature:VolumeSubpathEnvExpansion][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.001508324">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Local volume that cannot be mounted [Slow] should fail due to wrong node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="70.383272439"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001984888">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]" classname="Kubernetes e2e suite" time="8.904177909"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should serve a basic image on each replica with a private image" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t scale down when non expendable pod is running [Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.000692678">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on a VSAN capability, datastore and compatible zone specified in storage class" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap federations [Feature:Federation] should be able to change federation configuration [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Update Demo should do a rolling update of a replication controller  [Conformance]" classname="Kubernetes e2e suite" time="168.208681238"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Mount propagation should propagate mounts to the host" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume as non-root with FSGroup [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]" classname="Kubernetes e2e suite" time="78.906359518"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide podname as non-root with fsgroup and defaultMode [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should support pod readiness gates [NodeFeature:PodReadinessGate]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should conform to Ingress spec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]" classname="Kubernetes e2e suite" time="24.611748903"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume FStype [Feature:vsphere] verify fstype - default value should be ext4" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Firewall rule [Slow] [Serial] should create valid firewall rules for LoadBalancer type service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] nonexistent volume subPath should have the correct mode and owner using FSGroup" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.002256523">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should release no longer matching pods [Conformance]" classname="Kubernetes e2e suite" time="8.208794323"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should correctly scale down after a node is not needed [Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Docker Containers should be able to override the image&#39;s default arguments (docker cmd) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="20.567279548"></testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Restart [Disruptive] should restart all nodes and ensure all nodes and pods recover" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] RuntimeClass should reject a Pod requesting a RuntimeClass with an unconfigured handler" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Sysctls [NodeFeature:Sysctls] should not launch unsafe, but not explicitly enabled sysctls on the node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]" classname="Kubernetes e2e suite" time="26.580353066"></testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should provide basic identity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 50 pods per node using ReplicationController with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using Deployment.extensions with 0 secrets, 2 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Mounted flexvolume volume expand [Slow] [Feature:ExpandInUsePersistentVolumes] should be resizable when mounted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down by draining multiple pods one by one as dictated by pdb[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPath should support r/w [NodeConformance]" classname="Kubernetes e2e suite" time="24.71614712"></testcase>
      <testcase name="[sig-storage] Volume Placement should create and delete pod with the same volume source on the same worker node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:PerformanceDNS][Serial] Should answer DNS query for maximum number of services per cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] NFSPersistentVolumes[Disruptive][Flaky] when kubelet restarts Should test that a file written to the mount before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl version should check is all data is printed  [Conformance]" classname="Kubernetes e2e suite" time="8.218569416"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthenticator] The kubelet can delegate ServiceAccount tokens to the API server" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should be restarted with a docker exec liveness probe with timeout " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] NodeLease when the NodeLease feature is enabled the kubelet should create and update a lease in the kube-node-lease namespace" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner Default should be disabled by changing the default annotation [Serial] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] AppArmor load AppArmor profiles can disable an AppArmor profile, using unconfined" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001869234">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects a client request should support a client that connects, sends DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Feature:VolumeSubpathEnvExpansion][NodeAlphaFeature:VolumeSubpathEnvExpansion][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should contain last line of the log" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should cap back-off at MaxContainerBackOff [Slow][NodeConformance]" classname="Kubernetes e2e suite" time="1683.902013589"></testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="67.399460824"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001934875">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.001392345">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] ReplicaSet Should scale from 5 pods to 3 pods and from 3 to 1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.00130656">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001540211">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] gpu Upgrade [Feature:GPUUpgrade] cluster upgrade should be able to run gpu pod after upgrade [Feature:GPUClusterUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl label should update the label on a resource  [Conformance]" classname="Kubernetes e2e suite" time="20.557486234"></testcase>
      <testcase name="[sig-scheduling] Multi-AZ Cluster Volumes [sig-storage] should only be allowed to provision PDs in zones where nodes exist" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should support exec through kubectl proxy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.002062668">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.002100993">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should support cascading deletion of custom resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting an existing secret should exit with the Forbidden error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects NO client request should support a client that connects, sends DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root with FSGroup [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should support a &#39;default-deny&#39; policy [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic Snapshot] snapshottable should create snapshot with defaults [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001811">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should mutate configmap" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001681657">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should provide DNS for pods for Hostname and Subdomain" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]" classname="Kubernetes e2e suite" time="94.386452781"></testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] volume on tmpfs should have the correct mode using FSGroup" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap Should fail non-optional pod creation due to configMap object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [Feature:Example] [k8s.io] Liveness liveness pods should be automatically restarted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to up and down services" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should have their auto-restart back-off timer reset on image update [Slow][NodeConformance]" classname="Kubernetes e2e suite" time="437.373411074"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.002165275">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Provisioning On Clustered Datastore [Feature:vsphere] verify static provisioning on clustered datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: no PDB =&gt; should allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] stateful Upgrade [Feature:StatefulUpgrade] [k8s.io] stateful upgrade should maintain a functioning cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support volume SELinux relabeling when using hostIPC [Flaky] [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should delete successful finished jobs with limit of one successful job" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.00206915">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="30.534526555"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.412550965"></testcase>
      <testcase name="[k8s.io] EquivalenceCache [Serial] validates GeneralPredicates is properly invalidated when a pod is scheduled [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.002088176">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with invalid hostFailuresToTolerate value is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Docker Containers should be able to override the image&#39;s default command and arguments [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.401424967"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should have session affinity work for service with type clusterIP" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Kubelet [Serial] [Slow] [k8s.io] [sig-node] regular resource usage tracking resource tracking for 0 pods per node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI Volume expansion [Feature:ExpandCSIVolumes] should not expand volume if resizingOnDriver=off, resizingOnSC=on" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t trigger additional scale-ups during processing scale-up [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PV Protection Verify that PV bound to a PVC is not removed immediately" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="32.329734325"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Sysctls [NodeFeature:Sysctls] should reject invalid sysctls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere should test that deleting the Namespace of a PVC and Pod causes the successful detach of vsphere volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should deny crd creation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="22.553934907"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should not delete the token secret when the secret is not expired" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001851734">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]" classname="Kubernetes e2e suite" time="29.4167496"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce policy based on PodSelector [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap Should fail non-optional pod creation due to configMap object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Advanced Audit [DisabledForLargeClusters][Flaky] should audit API calls to create, get, update, patch, delete, list, watch configmaps." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001772938">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.001683624">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001578147">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001259529">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.000688811">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]" classname="Kubernetes e2e suite" time="8.046908717"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should allow egress access on one named port [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="80.333506105"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.002013581">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.001773566">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [Feature:VolumeSubpathEnvExpansion][NodeAlphaFeature:VolumeSubpathEnvExpansion][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.395806761"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] Pods should be evicted from unready Node [Feature:TaintEviction] All pods on the unreachable node should be marked as NotReady upon the node turn NotReady AND all pods should be evicted after eviction timeout passes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should support inline execution and attach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : projected" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Ports Security Check [Feature:KubeletSecurity] should not have port 4194 open on its all public IP addresses" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] [Feature:Windows] Density [Serial] [Slow] create a batch of pods latency/resource should be within limit when create 10 pods with 0s interval" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn&#39;t evict pod with tolerations from tainted nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.415321066"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceConversionWebhook [Feature:CustomResourceWebhookConversion] Should be able to convert a non homogeneous list of CRs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Kubelet [Serial] [Slow] [k8s.io] [sig-node] experimental resource usage tracking [Feature:ExperimentalResourceUsageTracking] resource tracking for 100 pods per node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Pod garbage collector [Feature:PodGarbageCollector] [Slow] should handle the creation of 1000 pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting a secret for a workload the node has access to should succeed" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] GKE local SSD [Feature:GKELocalSSD] should write and read from node local SSD [Feature:GKELocalSSD]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image from docker hub [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should delete the signed bootstrap tokens from clusterInfo ConfigMap when bootstrap token is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance]" classname="Kubernetes e2e suite" time="16.239649284"></testcase>
      <testcase name="[sig-instrumentation] MetricsGrabber should grab all metrics from API server." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with non-vsan datastore is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]" classname="Kubernetes e2e suite" time="8.747549824"></testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should not reconcile manually modified health check for ingress" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.19515198"></testcase>
      <testcase name="[k8s.io] Variable Expansion should not change the subpath mount on a container restart if the environment variable changes [Feature:VolumeSubpathEnvExpansion][NodeAlphaFeature:VolumeSubpathEnvExpansion][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for Table transformation should return generic metadata details across all namespaces for nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl client-side validation should create/apply a valid CR with arbitrary-extra properties for CRD with partially-specified validation schema" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.002182772">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="22.573580626000002"></testcase>
      <testcase name="[sig-storage] Zone Support Verify PVC creation with compatible policy and datastore without any zones specified in the storage class fails (No shared datastores exist among all the nodes)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t scale up when expendable pod is created [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should run Custom Metrics - Stackdriver Adapter for old resource model [Feature:StackdriverCustomMetrics]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota Should be able to update and delete ResourceQuota." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule pods each with a PD, delete pod and verify detach [Slow] for read-only PD with pod delete grace period of &#34;default (30s)&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001218663">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.001055153">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using ReplicationController with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons with quotas" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.001031591">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a container with runAsUser should run the container with uid 0 [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create volume metrics in Volume Manager" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001057779">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should scale up GPU pool from 0 [GpuType:] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: too few pods, absolute =&gt; should not allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should honor timeout" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should use same NodePort with same port but different protocols" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on multiple zones specified in the storage class. (No shared datastores exist among both zones)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support volume SELinux relabeling [Flaky] [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull from private registry with secret [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pod requesting volume is pending [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.33619438"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should provide DNS for the cluster  [Conformance]" classname="Kubernetes e2e suite" time="26.440792851"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001397721">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.00144751">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for pod-Service: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning [k8s.io] GlusterDynamicProvisioner should create and delete persistent volumes [fast]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001270037">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should scale up when non expendable pod is created [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should create ingress with pre-shared certificate" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001368317">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when podInfoOnMount=nil" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes [Feature:LabelSelector] [sig-storage] Selector-Label Volume Binding:vsphere should bind volume with claim for given label" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.00089857">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Upgrade [Feature:Upgrade] cluster upgrade should maintain a functioning cluster [Feature:ClusterUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic Snapshot] snapshottable should create snapshot with defaults [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl create quota should create a quota with scopes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GenericPersistentVolume[Disruptive] When kubelet restarts Should test that a volume mounted to a pod that is deleted while the kubelet is down unmounts when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-service-catalog] [Feature:PodPreset] PodPreset should create a pod preset" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.000850665">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Upgrade [Feature:Upgrade] node upgrade should maintain a functioning cluster [Feature:NodeUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should have session affinity work for NodePort service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should run Custom Metrics - Stackdriver Adapter for new resource model [Feature:StackdriverCustomMetrics]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] SSH should SSH to all nodes and run commands" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.000890312">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Disk Format [Feature:vsphere] verify disk format type - thin is honored for dynamically provisioned pv using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.428492796"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 [Slow] Nginx should conform to Ingress spec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="228.836107824"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI online volume expansion [Feature:ExpandCSIVolumes][Feature:ExpandInUseVolumes] should expand volume without restarting pod if attach=on, nodeExpansion=on" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should run a job to completion when tasks sometimes fail and are not locally restarted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should be updated [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="72.842166964"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: too few pods, replicaSet, percentage =&gt; should not allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:ScopeSelectors] should verify ResourceQuota with best effort scope using scope-selectors." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should scale down GPU pool from 1 [GpuType:] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should update nodePort: udp [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="112.641347197"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GenericPersistentVolume[Disruptive] When kubelet restarts Should test that a file written to the mount before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when CSIDriver does not exist" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]" classname="Kubernetes e2e suite" time="772.368278516">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;May 29 18:46:15.580: Couldn&#39;t delete ns: &#34;replication-controller-217&#34;: namespace replication-controller-217 was not deleted with limit: timed out waiting for the condition, pods remaining: 1 (&amp;errors.errorString{s:&#34;namespace replication-controller-217 was not deleted with limit: timed out waiting for the condition, pods remaining: 1&#34;})&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:334</failure>
          <system-out>[BeforeEach] [sig-apps] ReplicationController&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 18:33:23.213: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename replication-controller&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[It] should adopt matching pods on creation [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: Given a Pod with a &#39;name&#39; label pod-adoption is created&#xA;�[1mSTEP�[0m: When a replication controller with a matching selector is created&#xA;�[1mSTEP�[0m: Then the orphan pod is adopted&#xA;[AfterEach] [sig-apps] ReplicationController&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;May 29 18:36:13.646: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;May 29 18:36:13.689: INFO: Condition Ready of node e2e-test-peterhornyack-windows-node-group-jpxd is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule 2019-05-29 18:35:18 -0700 PDT} {node.kubernetes.io/not-ready  NoExecute 2019-05-29 18:35:20 -0700 PDT}]. Failure&#xA;�[1mSTEP�[0m: Destroying namespace &#34;replication-controller-217&#34; for this suite.&#xA;May 29 18:46:13.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 18:46:14.287: INFO: namespace: replication-controller-217, resource: pods, items remaining: 1&#xA;May 29 18:46:15.497: INFO: namespace: replication-controller-217, DeletionTimetamp: 2019-05-29 18:36:13 -0700 PDT, Finalizers: [kubernetes], Phase: Terminating&#xA;May 29 18:46:15.538: INFO: namespace: replication-controller-217, total namespaces: 5, active: 4, terminating: 1&#xA;May 29 18:46:15.580: INFO: POD           NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 18:46:15.580: INFO: pod-adoption  e2e-test-peterhornyack-windows-node-group-jpxd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:33:23 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:36:11 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:36:11 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:33:23 -0700 PDT  }]&#xA;May 29 18:46:15.580: INFO: &#xA;May 29 18:46:15.580: INFO: Couldn&#39;t delete ns: &#34;replication-controller-217&#34;: namespace replication-controller-217 was not deleted with limit: timed out waiting for the condition, pods remaining: 1 (&amp;errors.errorString{s:&#34;namespace replication-controller-217 was not deleted with limit: timed out waiting for the condition, pods remaining: 1&#34;})&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="16.385267295"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] shouldn&#39;t scale down with underutilized nodes due to host port conflicts [Feature:ClusterAutoscalerScalability5]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.001723946">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment deployment should support proportional scaling [Conformance]" classname="Kubernetes e2e suite" time="237.659069928"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to update NodePorts with two same port numbers but different protocols" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] new files should be created with FSGroup ownership when container is non-root" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce policy based on Ports [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Generated clientset should create pods, set the deletionTimestamp and deletionGracePeriodSeconds of the pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info dump should check if cluster-info dump succeeds" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] Deployment Should scale from 1 pod to 3 pods and from 3 to 5" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl replace should update a single-container pod&#39;s image  [Conformance]" classname="Kubernetes e2e suite" time="31.632560955"></testcase>
      <testcase name="[sig-storage] Zone Support Verify PVC creation fails if the availability zone specified in the storage class have no shared datastores under it." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for client IP based session affinity: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Stress with local volumes [Serial] should be able to process many pods and reuse local volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide container&#39;s cpu request [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="18.540322046"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes GCEPD should test that deleting the Namespace of a PVC and Pod causes the successful detach of Persistent Disk" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001241473">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image from gcr.io [NodeConformance]" classname="Kubernetes e2e suite" time="307.258575256"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.002206129">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]" classname="Kubernetes e2e suite" time="20.403565331"></testcase>
      <testcase name="[k8s.io] [sig-node] PreStop graceful pod terminated should wait until preStop hook completes the process" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]" classname="Kubernetes e2e suite" time="8.911142647"></testcase>
      <testcase name="[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should support configurable pod resolv.conf" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement should create and delete pod with multiple volumes from different datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.001921973">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] HA-master [Feature:HAMaster] survive addition/removal replicas multizone workers [Serial][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull from private registry with secret [NodeConformance]" classname="Kubernetes e2e suite" time="14.49746847"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.002399333">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001750277">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="84.028873963"></testcase>
      <testcase name="[sig-storage] Volume Provisioning On Clustered Datastore [Feature:vsphere] verify dynamic provision with spbm policy on clustered datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should adopt matching orphans and release non-matching pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret Should fail non-optional pod creation due to the key in the secret object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001989154">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should prevent NodePort collisions" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 should support forwarding over websockets" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : secret" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should only target nodes with endpoints" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should support orphan deletion of custom resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking IPerf IPv6 [Experimental] [Feature:Networking-IPv6] [Slow] [Feature:Networking-Performance] should transfer ~ 1GB onto the service endpoint 1 servers (maximum of 1 clients)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl copy should copy a file from a running Pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale up with two External metrics from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] Deployment Should scale from 5 pods to 3 pods and from 3 to 1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with External Metric with target average value from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl apply should reuse port when apply to an existing SVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] DNS horizontal autoscaling [Serial] [Slow] kube-dns-autoscaler should scale kube-dns pods when cluster size changed" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should handle updates to ExternalTrafficPolicy field" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be rejected when no endpoints exist" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Provisioning on Datastore [Feature:vsphere] verify dynamically provisioned pv using storageclass fails on an invalid datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] files with FSGroup ownership should support (root,0644,tmpfs)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning Invalid AWS KMS key should report an error and create no PV" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when podInfoOnMount=false" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.001788127">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001320414">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should contain correct container CPU metric." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="890.856006248">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;Timed out after 180.001s.&#xA;Expected&#xA;    &lt;string&gt;: content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T01:58:35.0388835Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    &#xA;to contain substring&#xA;    &lt;string&gt;: builder=&#34;foo&#34;&#xA;    &#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:181</failure>
          <system-out>[BeforeEach] [sig-storage] Downward API volume&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 18:58:34.920: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename downward-api&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-storage] Downward API volume&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39&#xA;[It] should update annotations on modification [NodeConformance] [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: Creating the pod&#xA;May 29 19:00:21.983: INFO: Successfully updated pod &#34;annotationupdateb44af120-cf01-497b-88f1-4589a4971d27&#34;&#xA;[AfterEach] [sig-storage] Downward API volume&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;downward-api-7812&#34;.&#xA;�[1mSTEP�[0m: Found 5 events.&#xA;May 29 19:03:22.027: INFO: At 2019-05-29 18:58:35 -0700 PDT - event for annotationupdateb44af120-cf01-497b-88f1-4589a4971d27: {default-scheduler } Scheduled: Successfully assigned downward-api-7812/annotationupdateb44af120-cf01-497b-88f1-4589a4971d27 to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 19:03:22.027: INFO: At 2019-05-29 18:58:37 -0700 PDT - event for annotationupdateb44af120-cf01-497b-88f1-4589a4971d27: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/mounttest:1.0&#34; already present on machine&#xA;May 29 19:03:22.027: INFO: At 2019-05-29 18:58:37 -0700 PDT - event for annotationupdateb44af120-cf01-497b-88f1-4589a4971d27: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container client-container&#xA;May 29 19:03:22.027: INFO: At 2019-05-29 18:58:39 -0700 PDT - event for annotationupdateb44af120-cf01-497b-88f1-4589a4971d27: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container client-container&#xA;May 29 19:03:22.027: INFO: At 2019-05-29 19:00:25 -0700 PDT - event for annotationupdateb44af120-cf01-497b-88f1-4589a4971d27: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod downward-api-7812/annotationupdateb44af120-cf01-497b-88f1-4589a4971d27&#xA;May 29 19:03:22.115: INFO: POD                                                    NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 19:03:22.115: INFO: annotationupdateb44af120-cf01-497b-88f1-4589a4971d27   e2e-test-peterhornyack-windows-node-group-jpxd  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:58:35 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:00:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:00:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:58:35 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:03:22.115: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 19:03:22.116: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:03:22.116: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:03:22.116: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 29 19:03:22.116: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 29 19:03:22.116: INFO: &#xA;May 29 19:03:22.158: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 29 19:03:22.200: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:29866,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 19:03:18 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 19:03:18 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 19:03:18 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 19:03:18 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 19:03:22.200: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 29 19:03:22.242: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 29 19:03:22.294: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:03:22.294: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:03:22.294: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:03:22.294: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 19:03:22.294: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 19:03:22.295: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:03:22.295: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:03:22.295: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:03:22.295: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 19:03:22.295: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 19:03:22.295: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:03:22.295: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:03:22.295: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:03:22.295: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:03:22.444: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 29 19:03:22.444: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 19:03:22.486: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:29808,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{KernelDeadlock False 2019-05-29 19:02:47 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 19:02:47 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-29 19:02:47 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-29 19:02:47 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 19:02:47 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 19:02:47 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 19:02:47 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 19:02:53 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 19:02:53 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 19:02:53 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 19:02:53 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 19:03:22.486: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 19:03:22.527: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 19:03:22.587: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:03:22.587: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 29 19:03:22.587: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:03:22.587: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 29 19:03:22.587: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 19:03:22.587: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:03:22.587: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 29 19:03:22.587: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:03:22.587: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 19:03:22.587: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 19:03:22.587: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:03:22.587: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:03:22.587: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 19:03:22.587: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:03:22.588: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 29 19:03:22.588: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:03:22.588: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 29 19:03:22.588: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:03:22.588: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 19:03:22.588: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 19:03:22.738: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 19:03:22.738: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 19:03:22.779: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:29789,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentKubeletRestart False 2019-05-29 19:02:45 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 19:02:45 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 19:02:45 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 19:02:45 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-29 19:02:45 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-29 19:02:45 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 19:02:45 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 19:02:24 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 19:02:24 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 19:02:24 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 19:02:24 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 19:03:22.780: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 19:03:22.821: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 19:03:22.879: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:03:22.879: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 29 19:03:22.879: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 29 19:03:22.879: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:03:22.879: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:03:22.879: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 19:03:22.879: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 19:03:22.879: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:03:22.879: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 19:03:22.879: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 19:03:22.879: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:03:22.879: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 19:03:22.879: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:03:22.879: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 29 19:03:22.879: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 29 19:03:23.045: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 19:03:23.045: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 19:03:23.086: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:29848,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 19:03:10 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 19:03:10 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 19:03:10 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 19:03:10 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 19:03:23.087: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 19:03:23.128: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 19:03:23.334: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 19:03:23.334: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 19:03:23.376: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:29754,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 19:02:30 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 19:02:30 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 19:02:30 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 19:02:30 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 19:03:23.376: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 19:03:23.417: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 19:03:23.622: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 19:03:23.622: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 19:03:23.664: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:29878,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[{node.kubernetes.io/not-ready  NoSchedule 2019-05-29 19:03:21 -0700 PDT}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 19:03:21 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 19:03:21 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 19:03:21 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready False 2019-05-29 19:03:21 -0700 PDT 2019-05-29 19:03:21 -0700 PDT KubeletNotReady PLEG is not healthy: pleg was last seen active 3m2.5161199s ago; threshold is 3m0s.}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 19:03:23.665: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 19:03:23.706: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 19:03:23.750: INFO: annotationupdateb44af120-cf01-497b-88f1-4589a4971d27 started at 2019-05-29 18:58:35 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:03:23.750: INFO: &#x9;Container client-container ready: true, restart count 0&#xA;May 29 19:03:23.957: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 19:03:23.957: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;May 29 19:03:24.000: INFO: Condition Ready of node e2e-test-peterhornyack-windows-node-group-jpxd is false instead of true. Reason: KubeletNotReady, message: PLEG is not healthy: pleg was last seen active 3m2.5161199s ago; threshold is 3m0s.&#xA;�[1mSTEP�[0m: Destroying namespace &#34;downward-api-7812&#34; for this suite.&#xA;May 29 19:13:24.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 19:13:25.775: INFO: Couldn&#39;t delete ns: &#34;downward-api-7812&#34;: namespace downward-api-7812 was not deleted with limit: timed out waiting for the condition, namespace is empty but is not yet removed (&amp;errors.errorString{s:&#34;namespace downward-api-7812 was not deleted with limit: timed out waiting for the condition, namespace is empty but is not yet removed&#34;})&#xA;</system-out>
      </testcase>
      <testcase name="[k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="16.401529594"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.000726655">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a persistent volume claim. [sig-storage]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Sysctls [NodeFeature:Sysctls] should support unsafe sysctls which are actually whitelisted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should remove clusters as expected" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.000655719">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner External should let an external dynamic provisioner create and delete persistent volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Sysctls [NodeFeature:Sysctls] should support sysctls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] GKE node pools [Feature:GKENodePool] should create a cluster with multiple node pools [Feature:GKENodePool]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule pods each with a PD, delete pod and verify detach [Slow] for RW PD with pod delete grace period of &#34;default (30s)&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on allowed zones specified in storage class " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.000607102">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GCP Volumes NFSv3 should be mountable for NFSv3" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Advanced Audit [DisabledForLargeClusters][Flaky] should audit API calls to create, get, update, patch, delete, list, watch deployments." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.00060567">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 3 pods per node using ReplicationController with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] NFSPersistentVolumes[Disruptive][Flaky] when kubelet restarts Should test that a volume mounted to a pod that is deleted while the kubelet is down unmounts when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.000626631">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="1312.53566128">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;wait for pod &#34;pod-with-poststart-exec-hook&#34; to disappear&#xA;Expected success, but got an error:&#xA;    &lt;*errors.errorString | 0xc0002b5440&gt;: {&#xA;        s: &#34;timed out waiting for the condition&#34;,&#xA;    }&#xA;    timed out waiting for the condition&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/pods.go:177</failure>
          <system-out>[BeforeEach] [k8s.io] Container Lifecycle Hook&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 19:13:42.182: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename container-lifecycle-hook&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] when create a pod with lifecycle hook&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61&#xA;�[1mSTEP�[0m: create the container to handle the HTTPGet hook request.&#xA;[It] should execute poststart exec hook properly [NodeConformance] [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: create the pod with lifecycle hook&#xA;�[1mSTEP�[0m: check poststart hook&#xA;�[1mSTEP�[0m: delete the pod with lifecycle hook&#xA;May 29 19:22:26.824: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:26.866: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:28.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:28.907: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:30.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:30.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:32.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:32.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:34.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:34.913: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:36.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:36.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:38.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:38.907: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:40.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:40.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:42.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:42.907: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:44.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:44.909: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:46.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:46.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:48.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:48.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:50.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:50.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:52.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:52.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:54.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:54.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:56.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:56.907: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:22:58.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:22:58.907: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:00.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:00.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:02.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:02.907: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:04.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:04.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:06.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:06.907: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:08.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:08.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:10.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:10.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:12.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:12.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:14.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:14.911: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:16.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:16.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:18.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:18.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:20.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:20.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:22.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:22.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:24.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:24.913: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:26.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:26.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:28.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:28.907: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:30.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:30.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:32.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:32.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:34.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:34.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:36.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:36.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:38.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:38.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:40.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:40.907: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:42.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:42.907: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:44.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:44.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:46.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:46.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:48.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:48.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:50.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:50.913: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:52.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:52.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:54.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:54.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:56.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:56.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:23:58.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:23:58.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:00.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:00.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:02.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:02.907: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:04.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:04.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:06.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:06.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:08.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:08.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:10.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:10.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:12.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:12.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:14.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:14.913: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:16.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:16.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:18.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:18.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:20.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:20.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:22.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:22.909: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:24.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:24.909: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:26.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:26.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:28.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:28.907: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:30.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:30.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:32.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:32.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:34.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:34.910: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:36.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:36.909: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:38.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:38.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:40.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:40.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:42.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:42.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:44.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:44.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:46.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:46.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:48.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:48.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:50.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:50.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:52.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:52.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:54.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:54.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:56.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:56.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:24:58.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:24:58.909: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:00.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:00.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:02.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:02.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:04.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:04.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:06.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:06.907: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:08.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:08.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:10.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:10.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:12.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:12.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:14.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:14.912: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:16.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:16.915: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:18.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:18.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:20.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:20.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:22.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:22.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:24.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:24.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:26.866: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:26.908: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;May 29 19:25:26.908: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear&#xA;May 29 19:25:26.950: INFO: Pod pod-with-poststart-exec-hook still exists&#xA;[AfterEach] [k8s.io] Container Lifecycle Hook&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;container-lifecycle-hook-9969&#34;.&#xA;�[1mSTEP�[0m: Found 10 events.&#xA;May 29 19:25:26.993: INFO: At 2019-05-29 19:13:42 -0700 PDT - event for pod-handle-http-request: {default-scheduler } Scheduled: Successfully assigned container-lifecycle-hook-9969/pod-handle-http-request to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 19:25:26.993: INFO: At 2019-05-29 19:13:44 -0700 PDT - event for pod-handle-http-request: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/netexec:1.1&#34; already present on machine&#xA;May 29 19:25:26.993: INFO: At 2019-05-29 19:13:44 -0700 PDT - event for pod-handle-http-request: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container pod-handle-http-request&#xA;May 29 19:25:26.993: INFO: At 2019-05-29 19:13:47 -0700 PDT - event for pod-handle-http-request: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container pod-handle-http-request&#xA;May 29 19:25:26.993: INFO: At 2019-05-29 19:16:35 -0700 PDT - event for pod-handle-http-request: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod container-lifecycle-hook-9969/pod-handle-http-request&#xA;May 29 19:25:26.993: INFO: At 2019-05-29 19:18:24 -0700 PDT - event for pod-with-poststart-exec-hook: {default-scheduler } Scheduled: Successfully assigned container-lifecycle-hook-9969/pod-with-poststart-exec-hook to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 19:25:26.993: INFO: At 2019-05-29 19:18:26 -0700 PDT - event for pod-with-poststart-exec-hook: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/hostexec:1.1&#34; already present on machine&#xA;May 29 19:25:26.994: INFO: At 2019-05-29 19:18:26 -0700 PDT - event for pod-with-poststart-exec-hook: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container pod-with-poststart-exec-hook&#xA;May 29 19:25:26.994: INFO: At 2019-05-29 19:18:28 -0700 PDT - event for pod-with-poststart-exec-hook: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container pod-with-poststart-exec-hook&#xA;May 29 19:25:26.994: INFO: At 2019-05-29 19:20:35 -0700 PDT - event for pod-with-poststart-exec-hook: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod container-lifecycle-hook-9969/pod-with-poststart-exec-hook&#xA;May 29 19:25:27.082: INFO: POD                                                    NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 19:25:27.082: INFO: pod-handle-http-request                                e2e-test-peterhornyack-windows-node-group-jpxd  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:13:42 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:18:24 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:18:24 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:13:42 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: pod-with-poststart-exec-hook                           e2e-test-peterhornyack-windows-node-group-jpxd  Running  15s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:18:24 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:22:25 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:22:25 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:18:24 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 19:25:27.082: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 19:25:27.083: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:25:27.083: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 29 19:25:27.083: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 29 19:25:27.083: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 19:25:27.083: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:25:27.083: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:25:27.083: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 19:25:27.083: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:25:27.083: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 19:25:27.083: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 29 19:25:27.083: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 29 19:25:27.083: INFO: &#xA;May 29 19:25:27.125: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 29 19:25:27.173: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:33046,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 19:25:22 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 19:25:22 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 19:25:22 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 19:25:22 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 19:25:27.173: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 29 19:25:27.214: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 29 19:25:27.261: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:25:27.261: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:25:27.261: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 19:25:27.261: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 19:25:27.261: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:25:27.261: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:25:27.261: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:25:27.261: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:25:27.261: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:25:27.261: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:25:27.261: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:25:27.261: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 19:25:27.261: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 19:25:27.261: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:25:27.409: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 29 19:25:27.409: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 19:25:27.455: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:32999,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentDockerRestart False 2019-05-29 19:25:01 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 19:25:01 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 19:25:01 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-29 19:25:01 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 19:25:01 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-29 19:25:01 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-29 19:25:01 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 19:24:56 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 19:24:56 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 19:24:56 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 19:24:56 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 19:25:27.455: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 19:25:27.497: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 19:25:27.547: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:25:27.547: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 29 19:25:27.547: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:25:27.547: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 29 19:25:27.547: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 19:25:27.547: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:25:27.547: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 29 19:25:27.547: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:25:27.547: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 19:25:27.547: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 19:25:27.547: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:25:27.547: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:25:27.547: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 19:25:27.547: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:25:27.547: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 29 19:25:27.547: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:25:27.547: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 29 19:25:27.547: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:25:27.547: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 19:25:27.548: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 19:25:27.698: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 19:25:27.698: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 19:25:27.740: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:33056,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{ReadonlyFilesystem False 2019-05-29 19:24:58 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-29 19:24:58 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 19:24:58 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 19:24:58 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 19:24:58 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-29 19:24:58 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-29 19:24:58 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 19:25:27 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 19:25:27 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 19:25:27 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 19:25:27 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 19:25:27.740: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 19:25:27.782: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 19:25:27.832: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:25:27.832: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 19:25:27.832: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 19:25:27.832: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:25:27.832: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 19:25:27.832: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:25:27.832: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 29 19:25:27.832: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 29 19:25:27.832: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:25:27.832: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 29 19:25:27.832: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 29 19:25:27.832: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 19:25:27.832: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 19:25:27.832: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 19:25:27.832: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 19:25:27.991: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 19:25:27.991: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 19:25:28.033: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:33022,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 19:25:12 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 19:25:12 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 19:25:12 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 19:25:12 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 19:25:28.033: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 19:25:28.074: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 19:25:28.278: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 19:25:28.278: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 19:25:28.319: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:32927,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 19:24:31 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 19:24:31 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 19:24:31 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 19:24:31 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 19:25:28.320: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 19:25:28.361: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 19:25:28.568: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 19:25:28.568: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 19:25:28.610: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:32937,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 19:24:33 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 19:24:33 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 19:24:33 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 19:24:33 -0700 PDT 2019-05-29 19:24:33 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 19:25:28.610: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 19:25:28.651: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 19:25:28.700: INFO: pod-handle-http-request started at 2019-05-29 19:13:42 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:25:28.700: INFO: &#x9;Container pod-handle-http-request ready: true, restart count 0&#xA;May 29 19:25:28.700: INFO: pod-with-poststart-exec-hook started at 2019-05-29 19:18:24 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 19:25:28.700: INFO: &#x9;Container pod-with-poststart-exec-hook ready: true, restart count 0&#xA;May 29 19:25:32.784: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 19:25:32.784: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;container-lifecycle-hook-9969&#34; for this suite.&#xA;May 29 19:35:32.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 19:35:34.382: INFO: namespace: container-lifecycle-hook-9969, resource: pods, items remaining: 2&#xA;May 29 19:35:34.632: INFO: namespace: container-lifecycle-hook-9969, DeletionTimetamp: 2019-05-29 19:25:32 -0700 PDT, Finalizers: [kubernetes], Phase: Terminating&#xA;May 29 19:35:34.673: INFO: namespace: container-lifecycle-hook-9969, total namespaces: 5, active: 4, terminating: 1&#xA;May 29 19:35:34.717: INFO: POD                           NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 19:35:34.717: INFO: pod-handle-http-request       e2e-test-peterhornyack-windows-node-group-jpxd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:13:42 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:34:28 -0700 PDT ContainersNotReady containers with unready status: [pod-handle-http-request]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:34:28 -0700 PDT ContainersNotReady containers with unready status: [pod-handle-http-request]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:13:42 -0700 PDT  }]&#xA;May 29 19:35:34.717: INFO: pod-with-poststart-exec-hook  e2e-test-peterhornyack-windows-node-group-jpxd  Running  15s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:18:24 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:22:25 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:22:25 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:18:24 -0700 PDT  }]&#xA;May 29 19:35:34.717: INFO: &#xA;May 29 19:35:34.717: INFO: Couldn&#39;t delete ns: &#34;container-lifecycle-hook-9969&#34;: namespace container-lifecycle-hook-9969 was not deleted with limit: timed out waiting for the condition, pods remaining: 2 (&amp;errors.errorString{s:&#34;namespace container-lifecycle-hook-9969 was not deleted with limit: timed out waiting for the condition, pods remaining: 2&#34;})&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should be able to switch between IG and NEG modes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GCP Volumes GlusterFS should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Cadvisor should be healthy on every node." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should preserve source pod IP for traffic thru service cluster IP" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should orphan pods created by rc if deleteOptions.OrphanDependents is nil" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl get componentstatuses should get componentstatuses" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates local ephemeral storage resource limits of pods that are allowed to run [Feature:LocalStorageCapacityIsolation]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001506747">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001156507">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.001182215">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="22.554760892"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="32.86670467"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] Multi-AZ Clusters should spread the pods of a service across zones" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should successfully scrape all targets" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] gpu Upgrade [Feature:GPUUpgrade] master upgrade should NOT disrupt gpu pod [Feature:GPUMasterUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should unconditionally reject operations on fail closed webhook" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should add node to the particular mig [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [Feature:Example] [k8s.io] Downward API should create a pod that prints his name and namespace" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [Feature:VolumeSubpathEnvExpansion][NodeAlphaFeature:VolumeSubpathEnvExpansion][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.002058157">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide podname as non-root with fsgroup and defaultMode [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.002263548">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes CSI Topology test using GCE PD driver [Serial] should provision zonal PD with immediate volume binding and AllowedTopologies set and mount the volume to a pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] crictl should be able to run crictl on the node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Advanced Audit [DisabledForLargeClusters][Flaky] should audit API calls to create and delete custom resource definition." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl client-side validation should create/apply a valid CR for CRD with validation schema" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.002015145">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [Job] should create new pods when node is partitioned" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance]" classname="Kubernetes e2e suite" time="30.468183348"></testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]" classname="Kubernetes e2e suite" time="1489.584521115">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0002b5ba0&gt;: {&#xA;        s: &#34;watch closed before UntilWithoutRetry timeout&#34;,&#xA;    }&#xA;    watch closed before UntilWithoutRetry timeout&#xA;occurred&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:650</failure>
          <system-out>[BeforeEach] [sig-apps] StatefulSet&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 19:37:00.622: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename statefulset&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-apps] StatefulSet&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59&#xA;[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74&#xA;�[1mSTEP�[0m: Creating service test in namespace statefulset-5035&#xA;[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: Initializing watcher for selector baz=blah,foo=bar&#xA;�[1mSTEP�[0m: Creating stateful set ss in namespace statefulset-5035&#xA;�[1mSTEP�[0m: Waiting until all stateful set ss replicas will be running in namespace statefulset-5035&#xA;May 29 19:37:01.001: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false&#xA;May 29 19:37:11.044: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true&#xA;�[1mSTEP�[0m: Confirming that stateful set scale up will halt with unhealthy stateful pod&#xA;May 29 19:37:11.086: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config exec --namespace=statefulset-5035 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true&#39;&#xA;May 29 19:37:11.744: INFO: stderr: &#34;+ mv -v /usr/share/nginx/html/index.html /tmp/\n&#34;&#xA;May 29 19:37:11.744: INFO: stdout: &#34;&#39;/usr/share/nginx/html/index.html&#39; -&gt; &#39;/tmp/index.html&#39;\n&#34;&#xA;May 29 19:37:11.744: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: &#39;/usr/share/nginx/html/index.html&#39; -&gt; &#39;/tmp/index.html&#39;&#xA;&#xA;May 29 19:37:11.786: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true&#xA;May 29 19:37:21.829: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false&#xA;May 29 19:37:21.829: INFO: Waiting for statefulset status.replicas updated to 0&#xA;May 29 19:37:22.001: INFO: Verifying statefulset ss doesn&#39;t scale past 1 for another 9.999999549s&#xA;May 29 19:37:23.043: INFO: Verifying statefulset ss doesn&#39;t scale past 1 for another 8.957605525s&#xA;May 29 19:37:24.086: INFO: Verifying statefulset ss doesn&#39;t scale past 1 for another 7.915175158s&#xA;May 29 19:37:25.128: INFO: Verifying statefulset ss doesn&#39;t scale past 1 for another 6.872687013s&#xA;May 29 19:37:26.170: INFO: Verifying statefulset ss doesn&#39;t scale past 1 for another 5.83052385s&#xA;May 29 19:37:27.212: INFO: Verifying statefulset ss doesn&#39;t scale past 1 for another 4.78862854s&#xA;May 29 19:37:28.254: INFO: Verifying statefulset ss doesn&#39;t scale past 1 for another 3.746647667s&#xA;May 29 19:37:29.296: INFO: Verifying statefulset ss doesn&#39;t scale past 1 for another 2.704379514s&#xA;May 29 19:37:30.338: INFO: Verifying statefulset ss doesn&#39;t scale past 1 for another 1.662292055s&#xA;May 29 19:37:31.380: INFO: Verifying statefulset ss doesn&#39;t scale past 1 for another 620.325607ms&#xA;�[1mSTEP�[0m: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5035&#xA;May 29 19:37:32.423: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config exec --namespace=statefulset-5035 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true&#39;&#xA;May 29 19:37:33.099: INFO: stderr: &#34;+ mv -v /tmp/index.html /usr/share/nginx/html/\n&#34;&#xA;May 29 19:37:33.099: INFO: stdout: &#34;&#39;/tmp/index.html&#39; -&gt; &#39;/usr/share/nginx/html/index.html&#39;\n&#34;&#xA;May 29 19:37:33.099: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: &#39;/tmp/index.html&#39; -&gt; &#39;/usr/share/nginx/html/index.html&#39;&#xA;&#xA;May 29 19:37:33.141: INFO: Found 1 stateful pods, waiting for 3&#xA;May 29 19:37:43.184: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:37:53.183: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:38:03.183: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:38:13.184: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:38:23.183: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:38:33.183: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:38:43.191: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:38:53.187: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:39:03.184: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:39:13.191: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:39:23.184: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:39:33.184: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:39:43.184: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:39:53.184: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:40:03.184: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:40:13.184: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:40:23.185: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 19:40:33.184: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 19:40:33.184: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 19:40:33.184: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false&#xA;May 29 19:40:43.183: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 19:40:43.183: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 19:40:43.183: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true&#xA;�[1mSTEP�[0m: Verifying that stateful set ss was scaled up in order&#xA;�[1mSTEP�[0m: Scale down will halt with unhealthy stateful pod&#xA;May 29 19:40:43.266: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config exec --namespace=statefulset-5035 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true&#39;&#xA;May 29 19:40:43.930: INFO: stderr: &#34;+ mv -v /usr/share/nginx/html/index.html /tmp/\n&#34;&#xA;May 29 19:40:43.930: INFO: stdout: &#34;&#39;/usr/share/nginx/html/index.html&#39; -&gt; &#39;/tmp/index.html&#39;\n&#34;&#xA;May 29 19:40:43.930: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: &#39;/usr/share/nginx/html/index.html&#39; -&gt; &#39;/tmp/index.html&#39;&#xA;&#xA;May 29 19:40:43.930: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config exec --namespace=statefulset-5035 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true&#39;&#xA;May 29 19:40:44.569: INFO: stderr: &#34;+ mv -v /usr/share/nginx/html/index.html /tmp/\n&#34;&#xA;May 29 19:40:44.569: INFO: stdout: &#34;&#39;/usr/share/nginx/html/index.html&#39; -&gt; &#39;/tmp/index.html&#39;\n&#34;&#xA;May 29 19:40:44.569: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: &#39;/usr/share/nginx/html/index.html&#39; -&gt; &#39;/tmp/index.html&#39;&#xA;&#xA;May 29 19:40:44.570: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config exec --namespace=statefulset-5035 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true&#39;&#xA;May 29 19:40:45.221: INFO: stderr: &#34;+ mv -v /usr/share/nginx/html/index.html /tmp/\n&#34;&#xA;May 29 19:40:45.221: INFO: stdout: &#34;&#39;/usr/share/nginx/html/index.html&#39; -&gt; &#39;/tmp/index.html&#39;\n&#34;&#xA;May 29 19:40:45.221: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: &#39;/usr/share/nginx/html/index.html&#39; -&gt; &#39;/tmp/index.html&#39;&#xA;&#xA;May 29 19:40:45.221: INFO: Waiting for statefulset status.replicas updated to 0&#xA;May 29 19:40:45.264: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1&#xA;May 29 19:40:55.349: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false&#xA;May 29 19:40:55.349: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false&#xA;May 29 19:40:55.349: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false&#xA;May 29 19:40:55.479: INFO: Verifying statefulset ss doesn&#39;t scale past 3 for another 9.999999566s&#xA;May 29 19:40:56.522: INFO: Verifying statefulset ss doesn&#39;t scale past 3 for another 8.957889865s&#xA;May 29 19:40:57.565: INFO: Verifying statefulset ss doesn&#39;t scale past 3 for another 7.91530123s&#xA;May 29 19:40:58.608: INFO: Verifying statefulset ss doesn&#39;t scale past 3 for another 6.872173526s&#xA;May 29 19:40:59.651: INFO: Verifying statefulset ss doesn&#39;t scale past 3 for another 5.828770879s&#xA;May 29 19:41:00.693: INFO: Verifying statefulset ss doesn&#39;t scale past 3 for another 4.786478537s&#xA;May 29 19:41:01.735: INFO: Verifying statefulset ss doesn&#39;t scale past 3 for another 3.744424661s&#xA;May 29 19:41:02.778: INFO: Verifying statefulset ss doesn&#39;t scale past 3 for another 2.702089458s&#xA;May 29 19:41:03.820: INFO: Verifying statefulset ss doesn&#39;t scale past 3 for another 1.659387595s&#xA;May 29 19:41:04.864: INFO: Verifying statefulset ss doesn&#39;t scale past 3 for another 616.77917ms&#xA;�[1mSTEP�[0m: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5035&#xA;May 29 19:41:05.907: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config exec --namespace=statefulset-5035 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true&#39;&#xA;May 29 19:41:06.567: INFO: stderr: &#34;+ mv -v /tmp/index.html /usr/share/nginx/html/\n&#34;&#xA;May 29 19:41:06.567: INFO: stdout: &#34;&#39;/tmp/index.html&#39; -&gt; &#39;/usr/share/nginx/html/index.html&#39;\n&#34;&#xA;May 29 19:41:06.567: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: &#39;/tmp/index.html&#39; -&gt; &#39;/usr/share/nginx/html/index.html&#39;&#xA;&#xA;May 29 19:41:06.567: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config exec --namespace=statefulset-5035 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true&#39;&#xA;May 29 19:41:07.202: INFO: stderr: &#34;+ mv -v /tmp/index.html /usr/share/nginx/html/\n&#34;&#xA;May 29 19:41:07.202: INFO: stdout: &#34;&#39;/tmp/index.html&#39; -&gt; &#39;/usr/share/nginx/html/index.html&#39;\n&#34;&#xA;May 29 19:41:07.202: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: &#39;/tmp/index.html&#39; -&gt; &#39;/usr/share/nginx/html/index.html&#39;&#xA;&#xA;May 29 19:41:07.202: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config exec --namespace=statefulset-5035 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true&#39;&#xA;May 29 19:41:07.845: INFO: stderr: &#34;+ mv -v /tmp/index.html /usr/share/nginx/html/\n&#34;&#xA;May 29 19:41:07.845: INFO: stdout: &#34;&#39;/tmp/index.html&#39; -&gt; &#39;/usr/share/nginx/html/index.html&#39;\n&#34;&#xA;May 29 19:41:07.845: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: &#39;/tmp/index.html&#39; -&gt; &#39;/usr/share/nginx/html/index.html&#39;&#xA;&#xA;May 29 19:41:07.846: INFO: Scaling statefulset ss to 0&#xA;�[1mSTEP�[0m: Verifying that stateful set ss was scaled down in reverse order&#xA;[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85&#xA;May 29 19:51:08.196: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config describe po ss-0 --namespace=statefulset-5035&#39;&#xA;May 29 19:51:09.054: INFO: stderr: &#34;&#34;&#xA;May 29 19:51:09.054: INFO: stdout: &#34;Name:               ss-0\nNamespace:          statefulset-5035\nPriority:           0\nPriorityClassName:  &lt;none&gt;\nNode:               e2e-test-peterhornyack-windows-node-group-1vjk/10.40.0.5\nStart Time:         Wed, 29 May 2019 19:37:00 -0700\nLabels:             baz=blah\n                    controller-revision-hash=ss-577b4dc465\n                    foo=bar\n                    statefulset.kubernetes.io/pod-name=ss-0\nAnnotations:        &lt;none&gt;\nStatus:             Running\nIP:                 10.64.2.43\nControlled By:      StatefulSet/ss\nContainers:\n  nginx:\n    Container ID:   docker://07f4a1011da28ed59d9d89edf9bf2855c53966a75fb8366337007b424a1a3a60\n    Image:          e2eteam/nginx:1.14-alpine\n    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14\n    Port:           &lt;none&gt;\n    Host Port:      &lt;none&gt;\n    State:          Running\n      Started:      Wed, 29 May 2019 19:37:06 -0700\n    Ready:          True\n    Restart Count:  0\n    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-46t4t (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-46t4t:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-46t4t\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  &lt;none&gt;\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type     Reason     Age                 From                                                     Message\n  ----     ------     ----                ----                                                     -------\n  Normal   Scheduled  14m                 default-scheduler                                        Successfully assigned statefulset-5035/ss-0 to e2e-test-peterhornyack-windows-node-group-1vjk\n  Normal   Pulled     14m                 kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Container image \&#34;e2eteam/nginx:1.14-alpine\&#34; already present on machine\n  Normal   Created    14m                 kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Created container nginx\n  Normal   Started    14m                 kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Started container nginx\n  Warning  Unhealthy  10m (x22 over 13m)  kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Readiness probe failed: HTTP probe failed with statuscode: 404\n&#34;&#xA;May 29 19:51:09.054: INFO: &#xA;Output of kubectl describe ss-0:&#xA;Name:               ss-0&#xA;Namespace:          statefulset-5035&#xA;Priority:           0&#xA;PriorityClassName:  &lt;none&gt;&#xA;Node:               e2e-test-peterhornyack-windows-node-group-1vjk/10.40.0.5&#xA;Start Time:         Wed, 29 May 2019 19:37:00 -0700&#xA;Labels:             baz=blah&#xA;                    controller-revision-hash=ss-577b4dc465&#xA;                    foo=bar&#xA;                    statefulset.kubernetes.io/pod-name=ss-0&#xA;Annotations:        &lt;none&gt;&#xA;Status:             Running&#xA;IP:                 10.64.2.43&#xA;Controlled By:      StatefulSet/ss&#xA;Containers:&#xA;  nginx:&#xA;    Container ID:   docker://07f4a1011da28ed59d9d89edf9bf2855c53966a75fb8366337007b424a1a3a60&#xA;    Image:          e2eteam/nginx:1.14-alpine&#xA;    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14&#xA;    Port:           &lt;none&gt;&#xA;    Host Port:      &lt;none&gt;&#xA;    State:          Running&#xA;      Started:      Wed, 29 May 2019 19:37:06 -0700&#xA;    Ready:          True&#xA;    Restart Count:  0&#xA;    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1&#xA;    Environment:    &lt;none&gt;&#xA;    Mounts:&#xA;      /var/run/secrets/kubernetes.io/serviceaccount from default-token-46t4t (ro)&#xA;Conditions:&#xA;  Type              Status&#xA;  Initialized       True &#xA;  Ready             True &#xA;  ContainersReady   True &#xA;  PodScheduled      True &#xA;Volumes:&#xA;  default-token-46t4t:&#xA;    Type:        Secret (a volume populated by a Secret)&#xA;    SecretName:  default-token-46t4t&#xA;    Optional:    false&#xA;QoS Class:       BestEffort&#xA;Node-Selectors:  &lt;none&gt;&#xA;Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s&#xA;                 node.kubernetes.io/unreachable:NoExecute for 300s&#xA;Events:&#xA;  Type     Reason     Age                 From                                                     Message&#xA;  ----     ------     ----                ----                                                     -------&#xA;  Normal   Scheduled  14m                 default-scheduler                                        Successfully assigned statefulset-5035/ss-0 to e2e-test-peterhornyack-windows-node-group-1vjk&#xA;  Normal   Pulled     14m                 kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;  Normal   Created    14m                 kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Created container nginx&#xA;  Normal   Started    14m                 kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Started container nginx&#xA;  Warning  Unhealthy  10m (x22 over 13m)  kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Readiness probe failed: HTTP probe failed with statuscode: 404&#xA;&#xA;May 29 19:51:09.055: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config logs ss-0 --namespace=statefulset-5035 --tail=100&#39;&#xA;May 29 19:51:09.372: INFO: stderr: &#34;&#34;&#xA;May 29 19:51:09.372: INFO: stdout: &#34;&#34;&#xA;May 29 19:51:09.372: INFO: &#xA;Last 100 log lines of ss-0:&#xA;&#xA;May 29 19:51:09.372: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config describe po ss-1 --namespace=statefulset-5035&#39;&#xA;May 29 19:51:09.728: INFO: stderr: &#34;&#34;&#xA;May 29 19:51:09.729: INFO: stdout: &#34;Name:                      ss-1\nNamespace:                 statefulset-5035\nPriority:                  0\nPriorityClassName:         &lt;none&gt;\nNode:                      e2e-test-peterhornyack-windows-node-group-jpxd/10.40.0.3\nStart Time:                Wed, 29 May 2019 19:37:33 -0700\nLabels:                    baz=blah\n                           controller-revision-hash=ss-577b4dc465\n                           foo=bar\n                           statefulset.kubernetes.io/pod-name=ss-1\nAnnotations:               &lt;none&gt;\nStatus:                    Terminating (lasts 9m)\nTermination Grace Period:  30s\nIP:                        10.64.1.95\nControlled By:             StatefulSet/ss\nContainers:\n  nginx:\n    Container ID:   docker://fd4b03627891108f265d046e4993cc4ed93d96ee3ce06d2c438b8d9a50f362de\n    Image:          e2eteam/nginx:1.14-alpine\n    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14\n    Port:           &lt;none&gt;\n    Host Port:      &lt;none&gt;\n    State:          Running\n      Started:      Wed, 29 May 2019 19:37:38 -0700\n    Ready:          True\n    Restart Count:  0\n    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-46t4t (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-46t4t:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-46t4t\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  &lt;none&gt;\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type     Reason     Age                 From                                                     Message\n  ----     ------     ----                ----                                                     -------\n  Normal   Scheduled  13m                 default-scheduler                                        Successfully assigned statefulset-5035/ss-1 to e2e-test-peterhornyack-windows-node-group-jpxd\n  Normal   Pulled     13m                 kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Container image \&#34;e2eteam/nginx:1.14-alpine\&#34; already present on machine\n  Normal   Created    13m                 kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Created container nginx\n  Normal   Started    13m                 kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Started container nginx\n  Warning  Unhealthy  10m (x22 over 10m)  kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Readiness probe failed: HTTP probe failed with statuscode: 404\n&#34;&#xA;May 29 19:51:09.729: INFO: &#xA;Output of kubectl describe ss-1:&#xA;Name:                      ss-1&#xA;Namespace:                 statefulset-5035&#xA;Priority:                  0&#xA;PriorityClassName:         &lt;none&gt;&#xA;Node:                      e2e-test-peterhornyack-windows-node-group-jpxd/10.40.0.3&#xA;Start Time:                Wed, 29 May 2019 19:37:33 -0700&#xA;Labels:                    baz=blah&#xA;                           controller-revision-hash=ss-577b4dc465&#xA;                           foo=bar&#xA;                           statefulset.kubernetes.io/pod-name=ss-1&#xA;Annotations:               &lt;none&gt;&#xA;Status:                    Terminating (lasts 9m)&#xA;Termination Grace Period:  30s&#xA;IP:                        10.64.1.95&#xA;Controlled By:             StatefulSet/ss&#xA;Containers:&#xA;  nginx:&#xA;    Container ID:   docker://fd4b03627891108f265d046e4993cc4ed93d96ee3ce06d2c438b8d9a50f362de&#xA;    Image:          e2eteam/nginx:1.14-alpine&#xA;    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14&#xA;    Port:           &lt;none&gt;&#xA;    Host Port:      &lt;none&gt;&#xA;    State:          Running&#xA;      Started:      Wed, 29 May 2019 19:37:38 -0700&#xA;    Ready:          True&#xA;    Restart Count:  0&#xA;    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1&#xA;    Environment:    &lt;none&gt;&#xA;    Mounts:&#xA;      /var/run/secrets/kubernetes.io/serviceaccount from default-token-46t4t (ro)&#xA;Conditions:&#xA;  Type              Status&#xA;  Initialized       True &#xA;  Ready             True &#xA;  ContainersReady   True &#xA;  PodScheduled      True &#xA;Volumes:&#xA;  default-token-46t4t:&#xA;    Type:        Secret (a volume populated by a Secret)&#xA;    SecretName:  default-token-46t4t&#xA;    Optional:    false&#xA;QoS Class:       BestEffort&#xA;Node-Selectors:  &lt;none&gt;&#xA;Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s&#xA;                 node.kubernetes.io/unreachable:NoExecute for 300s&#xA;Events:&#xA;  Type     Reason     Age                 From                                                     Message&#xA;  ----     ------     ----                ----                                                     -------&#xA;  Normal   Scheduled  13m                 default-scheduler                                        Successfully assigned statefulset-5035/ss-1 to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;  Normal   Pulled     13m                 kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;  Normal   Created    13m                 kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Created container nginx&#xA;  Normal   Started    13m                 kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Started container nginx&#xA;  Warning  Unhealthy  10m (x22 over 10m)  kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Readiness probe failed: HTTP probe failed with statuscode: 404&#xA;&#xA;May 29 19:51:09.729: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config logs ss-1 --namespace=statefulset-5035 --tail=100&#39;&#xA;May 29 19:51:10.031: INFO: stderr: &#34;&#34;&#xA;May 29 19:51:10.031: INFO: stdout: &#34;&#34;&#xA;May 29 19:51:10.031: INFO: &#xA;Last 100 log lines of ss-1:&#xA;&#xA;May 29 19:51:10.031: INFO: Deleting all statefulset in ns statefulset-5035&#xA;May 29 19:51:10.072: INFO: Scaling statefulset ss to 0&#xA;May 29 20:01:10.286: INFO: Waiting for statefulset status.replicas updated to 0&#xA;May 29 20:01:10.328: INFO: Waiting for stateful set status.replicas to become 0, currently 1&#xA;May 29 20:01:20.370: INFO: Waiting for stateful set status.replicas to become 0, currently 1&#xA;May 29 20:01:30.371: INFO: Waiting for stateful set status.replicas to become 0, currently 1&#xA;May 29 20:01:40.370: INFO: Deleting statefulset ss&#xA;May 29 20:01:40.506: INFO: Unexpected error occurred: Failed to scale statefulset to 0 in 10m0s. Remaining pods:&#xA;[ss-0: deletion 2019-05-29 20:01:07 -0700 PDT, phase Running, readiness false]&#xA;[AfterEach] [sig-apps] StatefulSet&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;statefulset-5035&#34;.&#xA;�[1mSTEP�[0m: Found 24 events.&#xA;May 29 20:01:40.554: INFO: At 2019-05-29 19:37:00 -0700 PDT - event for ss: {statefulset-controller } SuccessfulCreate: create Pod ss-0 in StatefulSet ss successful&#xA;May 29 20:01:40.554: INFO: At 2019-05-29 19:37:00 -0700 PDT - event for ss-0: {default-scheduler } Scheduled: Successfully assigned statefulset-5035/ss-0 to e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:01:40.554: INFO: At 2019-05-29 19:37:03 -0700 PDT - event for ss-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Pulled: Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;May 29 20:01:40.554: INFO: At 2019-05-29 19:37:03 -0700 PDT - event for ss-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Created: Created container nginx&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:37:06 -0700 PDT - event for ss-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Started: Started container nginx&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:37:12 -0700 PDT - event for ss-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Unhealthy: Readiness probe failed: HTTP probe failed with statuscode: 404&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:37:33 -0700 PDT - event for ss: {statefulset-controller } SuccessfulCreate: create Pod ss-1 in StatefulSet ss successful&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:37:33 -0700 PDT - event for ss-1: {default-scheduler } Scheduled: Successfully assigned statefulset-5035/ss-1 to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:37:35 -0700 PDT - event for ss-1: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container nginx&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:37:35 -0700 PDT - event for ss-1: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:37:38 -0700 PDT - event for ss-1: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container nginx&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:40:31 -0700 PDT - event for ss: {statefulset-controller } SuccessfulCreate: create Pod ss-2 in StatefulSet ss successful&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:40:31 -0700 PDT - event for ss-2: {default-scheduler } Scheduled: Successfully assigned statefulset-5035/ss-2 to e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:40:34 -0700 PDT - event for ss-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Created: Created container nginx&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:40:34 -0700 PDT - event for ss-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Pulled: Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:40:35 -0700 PDT - event for ss-1: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod statefulset-5035/ss-1&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:40:37 -0700 PDT - event for ss-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Started: Started container nginx&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:40:44 -0700 PDT - event for ss-1: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Unhealthy: Readiness probe failed: HTTP probe failed with statuscode: 404&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:40:46 -0700 PDT - event for ss-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Unhealthy: Readiness probe failed: HTTP probe failed with statuscode: 404&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:41:07 -0700 PDT - event for ss: {statefulset-controller } SuccessfulDelete: delete Pod ss-2 in StatefulSet ss successful&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:41:16 -0700 PDT - event for ss: {statefulset-controller } SuccessfulDelete: delete Pod ss-1 in StatefulSet ss successful&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 19:52:33 -0700 PDT - event for ss-1: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Killing: Stopping container nginx&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 20:00:37 -0700 PDT - event for ss: {statefulset-controller } SuccessfulDelete: delete Pod ss-0 in StatefulSet ss successful&#xA;May 29 20:01:40.555: INFO: At 2019-05-29 20:00:37 -0700 PDT - event for ss-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Killing: Stopping container nginx&#xA;May 29 20:01:40.643: INFO: POD                                                    NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 29 20:01:40.643: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:01:40.643: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 29 20:01:40.643: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:01:40.643: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:01:40.643: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:01:40.643: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:01:40.643: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:01:40.643: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 29 20:01:40.643: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 29 20:01:40.643: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 29 20:01:40.643: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 29 20:01:40.643: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:01:40.644: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 20:01:40.644: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 20:01:40.644: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:01:40.644: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 29 20:01:40.644: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 29 20:01:40.644: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:01:40.644: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:01:40.644: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:01:40.644: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:01:40.644: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:01:40.644: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:01:40.644: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 29 20:01:40.644: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 29 20:01:40.644: INFO: &#xA;May 29 20:01:40.687: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 29 20:01:40.728: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:38333,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:01:29 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:01:29 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:01:29 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:01:29 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 20:01:40.728: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 29 20:01:40.769: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 29 20:01:40.816: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:01:40.816: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:01:40.816: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:01:40.816: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 20:01:40.816: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:01:40.816: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:01:40.816: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:01:40.816: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:01:40.816: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:01:40.816: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:01:40.816: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:01:40.816: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:01:40.816: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 20:01:40.816: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:01:40.975: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 29 20:01:40.975: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 20:01:41.017: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:38325,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{KernelDeadlock False 2019-05-29 20:01:24 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 20:01:24 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-29 20:01:24 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-29 20:01:24 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 20:01:24 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 20:01:24 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 20:01:24 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:00:59 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:00:59 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:00:59 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:00:59 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 20:01:41.017: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 20:01:41.059: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 20:01:41.108: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:01:41.108: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:01:41.108: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 20:01:41.108: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:01:41.108: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 29 20:01:41.108: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:01:41.108: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 29 20:01:41.108: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:01:41.108: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 20:01:41.108: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:01:41.108: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:01:41.108: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 29 20:01:41.108: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:01:41.108: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 29 20:01:41.108: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:01:41.108: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:01:41.108: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 29 20:01:41.108: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:01:41.108: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 20:01:41.108: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:01:41.260: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 20:01:41.260: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 20:01:41.301: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:38338,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentDockerRestart False 2019-05-29 20:01:17 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 20:01:17 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 20:01:17 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-29 20:01:17 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-29 20:01:17 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 20:01:17 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-29 20:01:17 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:01:31 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:01:31 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:01:31 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:01:31 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 20:01:41.302: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 20:01:41.343: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 20:01:41.393: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:01:41.393: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 29 20:01:41.393: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 29 20:01:41.393: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:01:41.393: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 29 20:01:41.393: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 29 20:01:41.393: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:01:41.393: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:01:41.393: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 20:01:41.393: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:01:41.393: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:01:41.393: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 20:01:41.393: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:01:41.393: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:01:41.393: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 20:01:41.553: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 20:01:41.553: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:01:41.596: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:38303,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:01:15 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:01:15 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:01:15 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:01:15 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 20:01:41.596: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:01:41.638: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:01:41.841: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:01:41.841: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:01:41.883: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:38350,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:01:35 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:01:35 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:01:35 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:01:35 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 20:01:41.883: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:01:41.925: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:01:42.132: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:01:42.132: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:01:42.174: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:38357,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:01:37 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:01:37 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:01:37 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:01:37 -0700 PDT 2019-05-29 20:00:37 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 20:01:42.174: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:01:42.215: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:01:42.440: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:01:42.440: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;statefulset-5035&#34; for this suite.&#xA;May 29 20:01:48.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 20:01:50.206: INFO: namespace statefulset-5035 deletion completed in 7.723440297s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="317.23706989"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001974578">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001723096">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with invalid capability name objectSpaceReserve is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule pods each with a PD, delete pod and verify detach [Slow] for read-only PD with pod delete grace period of &#34;immediate (0s)&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001683909">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Deploy clustered applications [Feature:StatefulSet] [Slow] should creating a working mysql cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should reconcile LB health check interval [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Pod with node different from PV&#39;s NodeAffinity should fail scheduling due to different NodeAffinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should sign the new added bootstrap tokens" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Cluster level logging implemented by Stackdriver should ingest system logs from all nodes [Feature:StackdriverLogging]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should handle in-cluster config" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="121.959536162"></testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="316.558265062">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0005e7fe0&gt;: {&#xA;        s: &#34;expected pod \&#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30\&#34; success: Gave up after waiting 5m0s for pod \&#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30\&#34; to be \&#34;success or failure\&#34;&#34;,&#xA;    }&#xA;    expected pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34; success: Gave up after waiting 5m0s for pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34; to be &#34;success or failure&#34;&#xA;occurred&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2285</failure>
          <system-out>[BeforeEach] [sig-storage] Secrets&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 20:09:09.410: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename secrets&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[It] should be consumable from pods in volume [NodeConformance] [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: Creating secret with name secret-test-2ab88ccb-0494-4676-afac-def35452bead&#xA;�[1mSTEP�[0m: Creating a pod to test consume secrets&#xA;May 29 20:09:09.709: INFO: Waiting up to 5m0s for pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34; in namespace &#34;secrets-5024&#34; to be &#34;success or failure&#34;&#xA;May 29 20:09:09.751: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 41.77451ms&#xA;May 29 20:09:11.793: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.083585919s&#xA;May 29 20:09:13.835: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4.126126908s&#xA;May 29 20:09:15.877: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 6.168128819s&#xA;May 29 20:09:17.919: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 8.210208675s&#xA;May 29 20:09:19.961: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 10.251861614s&#xA;May 29 20:09:22.003: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 12.294029025s&#xA;May 29 20:09:24.045: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 14.335984126s&#xA;May 29 20:09:26.087: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 16.377734252s&#xA;May 29 20:09:28.133: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 18.423523234s&#xA;May 29 20:09:30.174: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 20.465279786s&#xA;May 29 20:09:32.217: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 22.507292841s&#xA;May 29 20:09:34.259: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 24.549400315s&#xA;May 29 20:09:36.301: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 26.5918187s&#xA;May 29 20:09:38.343: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 28.634100575s&#xA;May 29 20:09:40.385: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 30.676149506s&#xA;May 29 20:09:42.427: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 32.718206561s&#xA;May 29 20:09:44.470: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 34.760428432s&#xA;May 29 20:09:46.511: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 36.802185053s&#xA;May 29 20:09:48.553: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 38.844015568s&#xA;May 29 20:09:50.595: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 40.886203686s&#xA;May 29 20:09:52.639: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 42.929343648s&#xA;May 29 20:09:54.698: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 44.98853899s&#xA;May 29 20:09:56.740: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 47.030700851s&#xA;May 29 20:09:58.782: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 49.072585528s&#xA;May 29 20:10:00.824: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 51.114721836s&#xA;May 29 20:10:02.866: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 53.156799472s&#xA;May 29 20:10:04.908: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 55.198936519s&#xA;May 29 20:10:06.951: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 57.241350857s&#xA;May 29 20:10:08.992: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 59.283272492s&#xA;May 29 20:10:11.035: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m1.325298248s&#xA;May 29 20:10:13.076: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m3.367052135s&#xA;May 29 20:10:15.118: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m5.408971929s&#xA;May 29 20:10:17.160: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m7.45059022s&#xA;May 29 20:10:19.201: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m9.492119813s&#xA;May 29 20:10:21.243: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m11.533694632s&#xA;May 29 20:10:23.285: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m13.575903461s&#xA;May 29 20:10:25.328: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m15.618657537s&#xA;May 29 20:10:27.370: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m17.661027465s&#xA;May 29 20:10:29.412: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m19.703042779s&#xA;May 29 20:10:31.454: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m21.745227028s&#xA;May 29 20:10:33.497: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m23.787374454s&#xA;May 29 20:10:35.538: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m25.829241497s&#xA;May 29 20:10:37.581: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m27.871751555s&#xA;May 29 20:10:39.623: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m29.913860265s&#xA;May 29 20:10:41.665: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m31.955707033s&#xA;May 29 20:10:43.707: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m33.997901415s&#xA;May 29 20:10:45.749: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m36.040108372s&#xA;May 29 20:10:47.791: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m38.081838607s&#xA;May 29 20:10:49.833: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m40.123816562s&#xA;May 29 20:10:51.875: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m42.165905711s&#xA;May 29 20:10:53.917: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m44.207676081s&#xA;May 29 20:10:55.958: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m46.249276819s&#xA;May 29 20:10:58.001: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m48.291395103s&#xA;May 29 20:11:00.043: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m50.333720062s&#xA;May 29 20:11:02.086: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m52.376567521s&#xA;May 29 20:11:04.128: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m54.41873872s&#xA;May 29 20:11:06.176: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m56.466362667s&#xA;May 29 20:11:08.221: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m58.51198842s&#xA;May 29 20:11:10.263: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m0.55331963s&#xA;May 29 20:11:12.304: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m2.594861383s&#xA;May 29 20:11:14.346: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m4.637111488s&#xA;May 29 20:11:16.398: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m6.688963897s&#xA;May 29 20:11:18.440: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m8.731237745s&#xA;May 29 20:11:20.483: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m10.773311502s&#xA;May 29 20:11:22.525: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m12.815300332s&#xA;May 29 20:11:24.568: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m14.859189962s&#xA;May 29 20:11:26.610: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m16.901130716s&#xA;May 29 20:11:28.652: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m18.94290417s&#xA;May 29 20:11:30.694: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m20.985263589s&#xA;May 29 20:11:32.740: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m23.030467486s&#xA;May 29 20:11:34.781: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m25.072095163s&#xA;May 29 20:11:36.823: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m27.113987071s&#xA;May 29 20:11:38.865: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m29.155857212s&#xA;May 29 20:11:40.907: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m31.1977835s&#xA;May 29 20:11:42.953: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m33.244066758s&#xA;May 29 20:11:44.995: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m35.285906655s&#xA;May 29 20:11:47.037: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m37.327778961s&#xA;May 29 20:11:49.079: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m39.36965149s&#xA;May 29 20:11:51.122: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m41.41252778s&#xA;May 29 20:11:53.164: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m43.454311933s&#xA;May 29 20:11:55.205: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m45.49627189s&#xA;May 29 20:11:57.248: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m47.538706782s&#xA;May 29 20:11:59.290: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m49.580802147s&#xA;May 29 20:12:01.337: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m51.628010265s&#xA;May 29 20:12:03.380: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m53.670582149s&#xA;May 29 20:12:05.422: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m55.71247858s&#xA;May 29 20:12:07.464: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m57.754554883s&#xA;May 29 20:12:09.505: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m59.796027768s&#xA;May 29 20:12:11.547: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m1.837981184s&#xA;May 29 20:12:13.589: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m3.880156283s&#xA;May 29 20:12:15.632: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m5.922451779s&#xA;May 29 20:12:17.674: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m7.964914693s&#xA;May 29 20:12:19.717: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m10.008087587s&#xA;May 29 20:12:21.759: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m12.049627988s&#xA;May 29 20:12:23.806: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m14.096571813s&#xA;May 29 20:12:25.850: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m16.140478885s&#xA;May 29 20:12:27.891: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m18.182072949s&#xA;May 29 20:12:29.933: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m20.224137121s&#xA;May 29 20:12:31.975: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m22.265651737s&#xA;May 29 20:12:34.017: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m24.30802623s&#xA;May 29 20:12:36.059: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m26.349844106s&#xA;May 29 20:12:38.101: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m28.391938594s&#xA;May 29 20:12:40.146: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m30.436628495s&#xA;May 29 20:12:42.188: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m32.478727468s&#xA;May 29 20:12:44.230: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m34.520865886s&#xA;May 29 20:12:46.273: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m36.56363506s&#xA;May 29 20:12:48.315: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m38.605312684s&#xA;May 29 20:12:50.356: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m40.647059891s&#xA;May 29 20:12:52.398: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m42.68881519s&#xA;May 29 20:12:54.444: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m44.734650923s&#xA;May 29 20:12:56.486: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m46.776507457s&#xA;May 29 20:12:58.528: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m48.818455012s&#xA;May 29 20:13:00.576: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m50.867096406s&#xA;May 29 20:13:02.624: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m52.914763259s&#xA;May 29 20:13:04.666: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m54.957059282s&#xA;May 29 20:13:06.708: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m56.998829246s&#xA;May 29 20:13:08.750: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m59.040831691s&#xA;May 29 20:13:10.792: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m1.082773946s&#xA;May 29 20:13:12.834: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m3.124832694s&#xA;May 29 20:13:14.876: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m5.166877494s&#xA;May 29 20:13:16.918: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m7.208420028s&#xA;May 29 20:13:18.960: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m9.250340809s&#xA;May 29 20:13:21.001: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m11.292164394s&#xA;May 29 20:13:23.043: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m13.334171128s&#xA;May 29 20:13:25.085: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m15.376216495s&#xA;May 29 20:13:27.129: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m17.420151667s&#xA;May 29 20:13:29.172: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m19.463102751s&#xA;May 29 20:13:31.216: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m21.506580949s&#xA;May 29 20:13:33.258: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m23.548726036s&#xA;May 29 20:13:35.300: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m25.590668271s&#xA;May 29 20:13:37.342: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m27.632508967s&#xA;May 29 20:13:39.384: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m29.674945505s&#xA;May 29 20:13:41.428: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m31.718694402s&#xA;May 29 20:13:43.470: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m33.760581291s&#xA;May 29 20:13:45.511: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m35.802182265s&#xA;May 29 20:13:47.554: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m37.844468598s&#xA;May 29 20:13:49.596: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m39.88654508s&#xA;May 29 20:13:51.638: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m41.928426256s&#xA;May 29 20:13:53.680: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m43.970680633s&#xA;May 29 20:13:55.722: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m46.012373334s&#xA;May 29 20:13:57.763: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m48.054099545s&#xA;May 29 20:13:59.805: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m50.095783496s&#xA;May 29 20:14:01.847: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m52.137734033s&#xA;May 29 20:14:03.889: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m54.179392982s&#xA;May 29 20:14:05.931: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m56.221430088s&#xA;May 29 20:14:07.972: INFO: Pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m58.262987015s&#xA;May 29 20:14:10.062: INFO: Failed to get logs from node &#34;e2e-test-peterhornyack-windows-node-group-jpxd&#34; pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34; container &#34;secret-volume-test&#34;: the server rejected our request for an unknown reason (get pods pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30)&#xA;�[1mSTEP�[0m: delete the pod&#xA;May 29 20:14:10.109: INFO: Waiting for pod pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30 to disappear&#xA;May 29 20:14:10.150: INFO: Pod pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30 still exists&#xA;May 29 20:14:12.151: INFO: Waiting for pod pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30 to disappear&#xA;May 29 20:14:12.192: INFO: Pod pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30 still exists&#xA;May 29 20:14:14.151: INFO: Waiting for pod pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30 to disappear&#xA;May 29 20:14:14.192: INFO: Pod pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30 still exists&#xA;May 29 20:14:16.151: INFO: Waiting for pod pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30 to disappear&#xA;May 29 20:14:16.192: INFO: Pod pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30 no longer exists&#xA;May 29 20:14:16.192: INFO: Unexpected error occurred: expected pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34; success: Gave up after waiting 5m0s for pod &#34;pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#34; to be &#34;success or failure&#34;&#xA;[AfterEach] [sig-storage] Secrets&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;secrets-5024&#34;.&#xA;�[1mSTEP�[0m: Found 5 events.&#xA;May 29 20:14:16.235: INFO: At 2019-05-29 20:09:09 -0700 PDT - event for pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30: {default-scheduler } Scheduled: Successfully assigned secrets-5024/pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30 to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:14:16.235: INFO: At 2019-05-29 20:09:11 -0700 PDT - event for pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/mounttest:1.0&#34; already present on machine&#xA;May 29 20:14:16.235: INFO: At 2019-05-29 20:09:11 -0700 PDT - event for pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container secret-volume-test&#xA;May 29 20:14:16.235: INFO: At 2019-05-29 20:09:14 -0700 PDT - event for pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container secret-volume-test&#xA;May 29 20:14:16.235: INFO: At 2019-05-29 20:12:51 -0700 PDT - event for pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod secrets-5024/pod-secrets-b0d9ec09-3ba0-4730-95e1-0e797f720e30&#xA;May 29 20:14:16.323: INFO: POD                                                    NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 29 20:14:16.323: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:14:16.323: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 29 20:14:16.323: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:14:16.323: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:14:16.323: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:14:16.323: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:14:16.323: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:14:16.323: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 29 20:14:16.323: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 29 20:14:16.323: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 29 20:14:16.323: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 29 20:14:16.324: INFO: &#xA;May 29 20:14:16.367: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 29 20:14:16.408: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:40117,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:13:31 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:13:31 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:13:31 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:13:31 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 20:14:16.409: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 29 20:14:16.450: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 29 20:14:16.495: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:14:16.495: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 20:14:16.495: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:14:16.495: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:14:16.495: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:14:16.495: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:14:16.495: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:14:16.496: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:14:16.496: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:14:16.496: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:14:16.496: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:14:16.496: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:14:16.496: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 20:14:16.496: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:14:16.684: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 29 20:14:16.684: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 20:14:16.725: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:40184,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentDockerRestart False 2019-05-29 20:13:29 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 20:13:29 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 20:13:29 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-29 20:13:29 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 20:13:29 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-29 20:13:29 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-29 20:13:29 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:14:00 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:14:00 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:14:00 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:14:00 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 20:14:16.726: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 20:14:16.767: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 20:14:16.822: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:14:16.822: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:14:16.822: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 20:14:16.822: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:14:16.822: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 29 20:14:16.822: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:14:16.822: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 29 20:14:16.822: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:14:16.822: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 20:14:16.822: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:14:16.822: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:14:16.822: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 29 20:14:16.822: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:14:16.822: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 29 20:14:16.822: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:14:16.822: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:14:16.822: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 29 20:14:16.822: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:14:16.822: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 20:14:16.822: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:14:16.981: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 20:14:16.981: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 20:14:17.023: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:40119,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentUnregisterNetDevice False 2019-05-29 20:13:22 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-29 20:13:22 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 20:13:22 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-29 20:13:22 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 20:13:22 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 20:13:22 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 20:13:22 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:13:32 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:13:32 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:13:32 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:13:32 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 20:14:17.024: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 20:14:17.065: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 20:14:17.121: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:14:17.121: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 20:14:17.121: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:14:17.121: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:14:17.121: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 20:14:17.122: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:14:17.122: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 29 20:14:17.122: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 29 20:14:17.122: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:14:17.122: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 29 20:14:17.122: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 29 20:14:17.122: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:14:17.122: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:14:17.122: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 20:14:17.122: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:14:17.289: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 20:14:17.289: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:14:17.331: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:40227,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:14:16 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:14:16 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:14:16 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:14:16 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 20:14:17.331: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:14:17.372: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:14:17.576: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:14:17.576: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:14:17.619: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:40132,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:13:36 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:13:36 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:13:36 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:13:36 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 20:14:17.620: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:14:17.662: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:14:17.872: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:14:17.872: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:14:17.914: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:40160,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:13:49 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:13:49 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:13:49 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:13:49 -0700 PDT 2019-05-29 20:12:49 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 20:14:17.914: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:14:17.955: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:14:18.199: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:14:18.199: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;secrets-5024&#34; for this suite.&#xA;May 29 20:14:24.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 20:14:25.968: INFO: namespace secrets-5024 deletion completed in 7.726772937s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Feature:CustomResourcePublishOpenAPI] works for multiple CRDs of same group and version but different kinds" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]" classname="Kubernetes e2e suite" time="1414.290272899">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;May 29 20:29:27.591: Failed waiting for state update: timed out waiting for the condition&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/statefulset_utils.go:337</failure>
          <system-out>[BeforeEach] [sig-apps] StatefulSet&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 20:14:25.969: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename statefulset&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-apps] StatefulSet&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59&#xA;[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74&#xA;�[1mSTEP�[0m: Creating service test in namespace statefulset-1120&#xA;[It] should perform canary updates and phased rolling updates of template modifications [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: Creating a new StatefulSet&#xA;May 29 20:14:26.312: INFO: Found 1 stateful pods, waiting for 3&#xA;May 29 20:14:36.355: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:14:46.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:14:56.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:15:06.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:15:16.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:15:26.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:15:36.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:15:46.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:15:56.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:16:06.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:16:16.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:16:26.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:16:36.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:16:46.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:16:56.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:17:06.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:17:16.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:17:26.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:17:36.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:17:46.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:17:56.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:18:06.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:18:16.357: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:18:26.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:18:36.356: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:18:46.356: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 20:18:46.356: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 20:18:46.356: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false&#xA;May 29 20:18:56.359: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 20:18:56.359: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 20:18:56.359: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true&#xA;�[1mSTEP�[0m: Updating stateful set template: update image from e2eteam/nginx:1.14-alpine to e2eteam/nginx:1.15-alpine&#xA;May 29 20:18:56.582: INFO: Updating stateful set ss2&#xA;�[1mSTEP�[0m: Creating a new revision&#xA;�[1mSTEP�[0m: Not applying an update when the partition is greater than the number of replicas&#xA;�[1mSTEP�[0m: Performing a canary update&#xA;May 29 20:18:56.763: INFO: Updating stateful set ss2&#xA;May 29 20:18:56.847: INFO: Waiting for Pod statefulset-1120/ss2-2 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;�[1mSTEP�[0m: Restoring Pods to the correct revision when they are deleted&#xA;May 29 20:19:07.106: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 20:19:17.158: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 20:19:17.158: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 20:19:17.158: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false&#xA;May 29 20:19:27.153: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 20:19:27.153: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 20:19:27.153: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true&#xA;�[1mSTEP�[0m: Performing a phased rolling update&#xA;May 29 20:19:27.338: INFO: Updating stateful set ss2&#xA;May 29 20:19:27.421: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:19:37.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:19:47.505: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:19:57.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:20:07.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:20:17.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:20:27.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:20:37.507: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:20:47.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:20:57.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:21:07.505: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:21:17.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:21:27.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:21:37.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:21:47.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:21:57.507: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:22:07.507: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:22:17.505: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:22:27.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:22:37.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:22:47.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:22:57.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:23:07.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:23:17.507: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:23:27.508: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:23:37.505: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:23:47.505: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:23:57.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:24:07.507: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:24:17.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:24:27.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:24:37.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:24:47.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:24:57.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:25:07.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:25:17.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:25:27.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:25:37.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:25:47.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:25:57.505: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:26:07.507: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:26:17.508: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:26:27.507: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:26:37.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:26:47.505: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:26:57.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:27:07.505: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:27:17.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:27:27.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:27:37.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:27:47.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:27:57.514: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:28:07.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:28:17.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:28:27.508: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:28:37.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:28:47.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:28:57.506: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:29:07.505: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:29:17.507: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:29:27.507: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:29:27.591: INFO: Waiting for Pod statefulset-1120/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 20:29:27.591: INFO: Failed waiting for state update: timed out waiting for the condition&#xA;[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85&#xA;May 29 20:29:27.635: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config describe po ss2-0 --namespace=statefulset-1120&#39;&#xA;May 29 20:29:28.527: INFO: stderr: &#34;&#34;&#xA;May 29 20:29:28.527: INFO: stdout: &#34;Name:               ss2-0\nNamespace:          statefulset-1120\nPriority:           0\nPriorityClassName:  &lt;none&gt;\nNode:               e2e-test-peterhornyack-windows-node-group-1vjk/10.40.0.5\nStart Time:         Wed, 29 May 2019 20:19:06 -0700\nLabels:             baz=blah\n                    controller-revision-hash=ss2-577b4dc465\n                    foo=bar\n                    statefulset.kubernetes.io/pod-name=ss2-0\nAnnotations:        &lt;none&gt;\nStatus:             Running\nIP:                 10.64.2.45\nControlled By:      StatefulSet/ss2\nContainers:\n  nginx:\n    Container ID:   docker://9b3e1bc1a33f74d860769df75b6a38869ab9d28879d1a1c77decf54571972054\n    Image:          e2eteam/nginx:1.14-alpine\n    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14\n    Port:           &lt;none&gt;\n    Host Port:      &lt;none&gt;\n    State:          Running\n      Started:      Wed, 29 May 2019 20:19:11 -0700\n    Ready:          True\n    Restart Count:  0\n    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-plsvz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-plsvz:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-plsvz\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  &lt;none&gt;\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                     Message\n  ----    ------     ----  ----                                                     -------\n  Normal  Scheduled  10m   default-scheduler                                        Successfully assigned statefulset-1120/ss2-0 to e2e-test-peterhornyack-windows-node-group-1vjk\n  Normal  Pulled     10m   kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Container image \&#34;e2eteam/nginx:1.14-alpine\&#34; already present on machine\n  Normal  Created    10m   kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Created container nginx\n  Normal  Started    10m   kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Started container nginx\n&#34;&#xA;May 29 20:29:28.527: INFO: &#xA;Output of kubectl describe ss2-0:&#xA;Name:               ss2-0&#xA;Namespace:          statefulset-1120&#xA;Priority:           0&#xA;PriorityClassName:  &lt;none&gt;&#xA;Node:               e2e-test-peterhornyack-windows-node-group-1vjk/10.40.0.5&#xA;Start Time:         Wed, 29 May 2019 20:19:06 -0700&#xA;Labels:             baz=blah&#xA;                    controller-revision-hash=ss2-577b4dc465&#xA;                    foo=bar&#xA;                    statefulset.kubernetes.io/pod-name=ss2-0&#xA;Annotations:        &lt;none&gt;&#xA;Status:             Running&#xA;IP:                 10.64.2.45&#xA;Controlled By:      StatefulSet/ss2&#xA;Containers:&#xA;  nginx:&#xA;    Container ID:   docker://9b3e1bc1a33f74d860769df75b6a38869ab9d28879d1a1c77decf54571972054&#xA;    Image:          e2eteam/nginx:1.14-alpine&#xA;    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14&#xA;    Port:           &lt;none&gt;&#xA;    Host Port:      &lt;none&gt;&#xA;    State:          Running&#xA;      Started:      Wed, 29 May 2019 20:19:11 -0700&#xA;    Ready:          True&#xA;    Restart Count:  0&#xA;    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1&#xA;    Environment:    &lt;none&gt;&#xA;    Mounts:&#xA;      /var/run/secrets/kubernetes.io/serviceaccount from default-token-plsvz (ro)&#xA;Conditions:&#xA;  Type              Status&#xA;  Initialized       True &#xA;  Ready             True &#xA;  ContainersReady   True &#xA;  PodScheduled      True &#xA;Volumes:&#xA;  default-token-plsvz:&#xA;    Type:        Secret (a volume populated by a Secret)&#xA;    SecretName:  default-token-plsvz&#xA;    Optional:    false&#xA;QoS Class:       BestEffort&#xA;Node-Selectors:  &lt;none&gt;&#xA;Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s&#xA;                 node.kubernetes.io/unreachable:NoExecute for 300s&#xA;Events:&#xA;  Type    Reason     Age   From                                                     Message&#xA;  ----    ------     ----  ----                                                     -------&#xA;  Normal  Scheduled  10m   default-scheduler                                        Successfully assigned statefulset-1120/ss2-0 to e2e-test-peterhornyack-windows-node-group-1vjk&#xA;  Normal  Pulled     10m   kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;  Normal  Created    10m   kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Created container nginx&#xA;  Normal  Started    10m   kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Started container nginx&#xA;&#xA;May 29 20:29:28.528: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config logs ss2-0 --namespace=statefulset-1120 --tail=100&#39;&#xA;May 29 20:29:28.845: INFO: stderr: &#34;&#34;&#xA;May 29 20:29:28.845: INFO: stdout: &#34;&#34;&#xA;May 29 20:29:28.845: INFO: &#xA;Last 100 log lines of ss2-0:&#xA;&#xA;May 29 20:29:28.846: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config describe po ss2-1 --namespace=statefulset-1120&#39;&#xA;May 29 20:29:29.205: INFO: stderr: &#34;&#34;&#xA;May 29 20:29:29.205: INFO: stdout: &#34;Name:                      ss2-1\nNamespace:                 statefulset-1120\nPriority:                  0\nPriorityClassName:         &lt;none&gt;\nNode:                      e2e-test-peterhornyack-windows-node-group-jpxd/10.40.0.3\nStart Time:                Wed, 29 May 2019 20:14:33 -0700\nLabels:                    baz=blah\n                           controller-revision-hash=ss2-577b4dc465\n                           foo=bar\n                           statefulset.kubernetes.io/pod-name=ss2-1\nAnnotations:               &lt;none&gt;\nStatus:                    Terminating (lasts 9m)\nTermination Grace Period:  30s\nIP:                        10.64.1.99\nControlled By:             StatefulSet/ss2\nContainers:\n  nginx:\n    Container ID:   docker://8c163ab4408ba06e77c51e42d8cf9682a4af7d429066a23699f607007aec188b\n    Image:          e2eteam/nginx:1.14-alpine\n    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14\n    Port:           &lt;none&gt;\n    Host Port:      &lt;none&gt;\n    State:          Running\n      Started:      Wed, 29 May 2019 20:14:37 -0700\n    Ready:          False\n    Restart Count:  0\n    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-plsvz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             False \n  ContainersReady   False \n  PodScheduled      True \nVolumes:\n  default-token-plsvz:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-plsvz\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  &lt;none&gt;\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                     Message\n  ----    ------     ----  ----                                                     -------\n  Normal  Scheduled  14m   default-scheduler                                        Successfully assigned statefulset-1120/ss2-1 to e2e-test-peterhornyack-windows-node-group-jpxd\n  Normal  Pulled     14m   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Container image \&#34;e2eteam/nginx:1.14-alpine\&#34; already present on machine\n  Normal  Created    14m   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Created container nginx\n  Normal  Started    14m   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Started container nginx\n  Normal  Killing    46s   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Stopping container nginx\n&#34;&#xA;May 29 20:29:29.205: INFO: &#xA;Output of kubectl describe ss2-1:&#xA;Name:                      ss2-1&#xA;Namespace:                 statefulset-1120&#xA;Priority:                  0&#xA;PriorityClassName:         &lt;none&gt;&#xA;Node:                      e2e-test-peterhornyack-windows-node-group-jpxd/10.40.0.3&#xA;Start Time:                Wed, 29 May 2019 20:14:33 -0700&#xA;Labels:                    baz=blah&#xA;                           controller-revision-hash=ss2-577b4dc465&#xA;                           foo=bar&#xA;                           statefulset.kubernetes.io/pod-name=ss2-1&#xA;Annotations:               &lt;none&gt;&#xA;Status:                    Terminating (lasts 9m)&#xA;Termination Grace Period:  30s&#xA;IP:                        10.64.1.99&#xA;Controlled By:             StatefulSet/ss2&#xA;Containers:&#xA;  nginx:&#xA;    Container ID:   docker://8c163ab4408ba06e77c51e42d8cf9682a4af7d429066a23699f607007aec188b&#xA;    Image:          e2eteam/nginx:1.14-alpine&#xA;    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14&#xA;    Port:           &lt;none&gt;&#xA;    Host Port:      &lt;none&gt;&#xA;    State:          Running&#xA;      Started:      Wed, 29 May 2019 20:14:37 -0700&#xA;    Ready:          False&#xA;    Restart Count:  0&#xA;    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1&#xA;    Environment:    &lt;none&gt;&#xA;    Mounts:&#xA;      /var/run/secrets/kubernetes.io/serviceaccount from default-token-plsvz (ro)&#xA;Conditions:&#xA;  Type              Status&#xA;  Initialized       True &#xA;  Ready             False &#xA;  ContainersReady   False &#xA;  PodScheduled      True &#xA;Volumes:&#xA;  default-token-plsvz:&#xA;    Type:        Secret (a volume populated by a Secret)&#xA;    SecretName:  default-token-plsvz&#xA;    Optional:    false&#xA;QoS Class:       BestEffort&#xA;Node-Selectors:  &lt;none&gt;&#xA;Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s&#xA;                 node.kubernetes.io/unreachable:NoExecute for 300s&#xA;Events:&#xA;  Type    Reason     Age   From                                                     Message&#xA;  ----    ------     ----  ----                                                     -------&#xA;  Normal  Scheduled  14m   default-scheduler                                        Successfully assigned statefulset-1120/ss2-1 to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;  Normal  Pulled     14m   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;  Normal  Created    14m   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Created container nginx&#xA;  Normal  Started    14m   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Started container nginx&#xA;  Normal  Killing    46s   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Stopping container nginx&#xA;&#xA;May 29 20:29:29.205: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config logs ss2-1 --namespace=statefulset-1120 --tail=100&#39;&#xA;May 29 20:29:29.514: INFO: stderr: &#34;&#34;&#xA;May 29 20:29:29.514: INFO: stdout: &#34;&#34;&#xA;May 29 20:29:29.514: INFO: &#xA;Last 100 log lines of ss2-1:&#xA;&#xA;May 29 20:29:29.514: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config describe po ss2-2 --namespace=statefulset-1120&#39;&#xA;May 29 20:29:29.879: INFO: stderr: &#34;&#34;&#xA;May 29 20:29:29.879: INFO: stdout: &#34;Name:               ss2-2\nNamespace:          statefulset-1120\nPriority:           0\nPriorityClassName:  &lt;none&gt;\nNode:               e2e-test-peterhornyack-windows-node-group-9q9v/10.40.0.4\nStart Time:         Wed, 29 May 2019 20:19:13 -0700\nLabels:             baz=blah\n                    controller-revision-hash=ss2-5f4db4b9f4\n                    foo=bar\n                    statefulset.kubernetes.io/pod-name=ss2-2\nAnnotations:        &lt;none&gt;\nStatus:             Running\nIP:                 10.64.3.89\nControlled By:      StatefulSet/ss2\nContainers:\n  nginx:\n    Container ID:   docker://0a08bf525c3128473face83deaa8dbb81ced7290b062a35b59e10b0996962d53\n    Image:          e2eteam/nginx:1.15-alpine\n    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14\n    Port:           &lt;none&gt;\n    Host Port:      &lt;none&gt;\n    State:          Running\n      Started:      Wed, 29 May 2019 20:19:16 -0700\n    Ready:          True\n    Restart Count:  0\n    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-plsvz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-plsvz:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-plsvz\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  &lt;none&gt;\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                     Message\n  ----    ------     ----  ----                                                     -------\n  Normal  Scheduled  10m   default-scheduler                                        Successfully assigned statefulset-1120/ss2-2 to e2e-test-peterhornyack-windows-node-group-9q9v\n  Normal  Pulled     10m   kubelet, e2e-test-peterhornyack-windows-node-group-9q9v  Container image \&#34;e2eteam/nginx:1.15-alpine\&#34; already present on machine\n  Normal  Created    10m   kubelet, e2e-test-peterhornyack-windows-node-group-9q9v  Created container nginx\n  Normal  Started    10m   kubelet, e2e-test-peterhornyack-windows-node-group-9q9v  Started container nginx\n&#34;&#xA;May 29 20:29:29.879: INFO: &#xA;Output of kubectl describe ss2-2:&#xA;Name:               ss2-2&#xA;Namespace:          statefulset-1120&#xA;Priority:           0&#xA;PriorityClassName:  &lt;none&gt;&#xA;Node:               e2e-test-peterhornyack-windows-node-group-9q9v/10.40.0.4&#xA;Start Time:         Wed, 29 May 2019 20:19:13 -0700&#xA;Labels:             baz=blah&#xA;                    controller-revision-hash=ss2-5f4db4b9f4&#xA;                    foo=bar&#xA;                    statefulset.kubernetes.io/pod-name=ss2-2&#xA;Annotations:        &lt;none&gt;&#xA;Status:             Running&#xA;IP:                 10.64.3.89&#xA;Controlled By:      StatefulSet/ss2&#xA;Containers:&#xA;  nginx:&#xA;    Container ID:   docker://0a08bf525c3128473face83deaa8dbb81ced7290b062a35b59e10b0996962d53&#xA;    Image:          e2eteam/nginx:1.15-alpine&#xA;    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14&#xA;    Port:           &lt;none&gt;&#xA;    Host Port:      &lt;none&gt;&#xA;    State:          Running&#xA;      Started:      Wed, 29 May 2019 20:19:16 -0700&#xA;    Ready:          True&#xA;    Restart Count:  0&#xA;    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1&#xA;    Environment:    &lt;none&gt;&#xA;    Mounts:&#xA;      /var/run/secrets/kubernetes.io/serviceaccount from default-token-plsvz (ro)&#xA;Conditions:&#xA;  Type              Status&#xA;  Initialized       True &#xA;  Ready             True &#xA;  ContainersReady   True &#xA;  PodScheduled      True &#xA;Volumes:&#xA;  default-token-plsvz:&#xA;    Type:        Secret (a volume populated by a Secret)&#xA;    SecretName:  default-token-plsvz&#xA;    Optional:    false&#xA;QoS Class:       BestEffort&#xA;Node-Selectors:  &lt;none&gt;&#xA;Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s&#xA;                 node.kubernetes.io/unreachable:NoExecute for 300s&#xA;Events:&#xA;  Type    Reason     Age   From                                                     Message&#xA;  ----    ------     ----  ----                                                     -------&#xA;  Normal  Scheduled  10m   default-scheduler                                        Successfully assigned statefulset-1120/ss2-2 to e2e-test-peterhornyack-windows-node-group-9q9v&#xA;  Normal  Pulled     10m   kubelet, e2e-test-peterhornyack-windows-node-group-9q9v  Container image &#34;e2eteam/nginx:1.15-alpine&#34; already present on machine&#xA;  Normal  Created    10m   kubelet, e2e-test-peterhornyack-windows-node-group-9q9v  Created container nginx&#xA;  Normal  Started    10m   kubelet, e2e-test-peterhornyack-windows-node-group-9q9v  Started container nginx&#xA;&#xA;May 29 20:29:29.880: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config logs ss2-2 --namespace=statefulset-1120 --tail=100&#39;&#xA;May 29 20:29:30.190: INFO: stderr: &#34;&#34;&#xA;May 29 20:29:30.190: INFO: stdout: &#34;&#34;&#xA;May 29 20:29:30.190: INFO: &#xA;Last 100 log lines of ss2-2:&#xA;&#xA;May 29 20:29:30.190: INFO: Deleting all statefulset in ns statefulset-1120&#xA;May 29 20:29:30.232: INFO: Scaling statefulset ss2 to 0&#xA;May 29 20:37:50.410: INFO: Waiting for statefulset status.replicas updated to 0&#xA;May 29 20:37:50.452: INFO: Deleting statefulset ss2&#xA;[AfterEach] [sig-apps] StatefulSet&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;statefulset-1120&#34;.&#xA;�[1mSTEP�[0m: Found 37 events.&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:14:26 -0700 PDT - event for ss2: {statefulset-controller } SuccessfulCreate: create Pod ss2-0 in StatefulSet ss2 successful&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:14:26 -0700 PDT - event for ss2-0: {default-scheduler } Scheduled: Successfully assigned statefulset-1120/ss2-0 to e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:14:28 -0700 PDT - event for ss2-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Created: Created container nginx&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:14:28 -0700 PDT - event for ss2-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Pulled: Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:14:30 -0700 PDT - event for ss2-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Started: Started container nginx&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:14:33 -0700 PDT - event for ss2: {statefulset-controller } SuccessfulCreate: create Pod ss2-1 in StatefulSet ss2 successful&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:14:33 -0700 PDT - event for ss2-1: {default-scheduler } Scheduled: Successfully assigned statefulset-1120/ss2-1 to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:14:35 -0700 PDT - event for ss2-1: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container nginx&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:14:35 -0700 PDT - event for ss2-1: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:14:37 -0700 PDT - event for ss2-1: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container nginx&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:16:51 -0700 PDT - event for ss2-1: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod statefulset-1120/ss2-1&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:18:42 -0700 PDT - event for ss2: {statefulset-controller } SuccessfulCreate: create Pod ss2-2 in StatefulSet ss2 successful&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:18:42 -0700 PDT - event for ss2-2: {default-scheduler } Scheduled: Successfully assigned statefulset-1120/ss2-2 to e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:18:44 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Pulled: Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:18:44 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Created: Created container nginx&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:18:46 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Started: Started container nginx&#xA;May 29 20:37:50.623: INFO: At 2019-05-29 20:18:56 -0700 PDT - event for ss2: {statefulset-controller } SuccessfulDelete: delete Pod ss2-2 in StatefulSet ss2 successful&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:18:56 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Killing: Stopping container nginx&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:04 -0700 PDT - event for ss2-2: {default-scheduler } Scheduled: Successfully assigned statefulset-1120/ss2-2 to e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:06 -0700 PDT - event for ss2-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Killing: Stopping container nginx&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:06 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Pulled: Successfully pulled image &#34;e2eteam/nginx:1.15-alpine&#34;&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:06 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Pulling: Pulling image &#34;e2eteam/nginx:1.15-alpine&#34;&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:06 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Created: Created container nginx&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:07 -0700 PDT - event for ss2-0: {default-scheduler } Scheduled: Successfully assigned statefulset-1120/ss2-0 to e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:08 -0700 PDT - event for ss2-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Pulled: Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:08 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Started: Started container nginx&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:09 -0700 PDT - event for ss2-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Created: Created container nginx&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:11 -0700 PDT - event for ss2-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Started: Started container nginx&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:13 -0700 PDT - event for ss2-2: {default-scheduler } Scheduled: Successfully assigned statefulset-1120/ss2-2 to e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:15 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Pulled: Container image &#34;e2eteam/nginx:1.15-alpine&#34; already present on machine&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:15 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Created: Created container nginx&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:17 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Started: Started container nginx&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:19:27 -0700 PDT - event for ss2: {statefulset-controller } SuccessfulDelete: delete Pod ss2-1 in StatefulSet ss2 successful&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:28:43 -0700 PDT - event for ss2-1: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Killing: Stopping container nginx&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:29:30 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Killing: Stopping container nginx&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:36:54 -0700 PDT - event for ss2: {statefulset-controller } SuccessfulDelete: delete Pod ss2-0 in StatefulSet ss2 successful&#xA;May 29 20:37:50.624: INFO: At 2019-05-29 20:36:54 -0700 PDT - event for ss2-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Killing: Stopping container nginx&#xA;May 29 20:37:50.718: INFO: POD                                                    NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 29 20:37:50.718: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:37:50.718: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 29 20:37:50.718: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 29 20:37:50.719: INFO: &#xA;May 29 20:37:50.762: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 29 20:37:50.805: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:43646,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:37:36 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:37:36 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:37:36 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:37:36 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 20:37:50.805: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 29 20:37:50.847: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 29 20:37:50.894: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:37:50.894: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:37:50.894: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:37:50.894: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:37:50.894: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:37:50.894: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:37:50.894: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 20:37:50.894: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:37:50.894: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:37:50.894: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:37:50.894: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:37:50.894: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 20:37:50.894: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:37:50.894: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:37:51.042: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 29 20:37:51.042: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 20:37:51.084: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:43673,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{CorruptDockerOverlay2 False 2019-05-29 20:37:45 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-29 20:37:45 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 20:37:45 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-29 20:37:45 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-29 20:37:45 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 20:37:45 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 20:37:45 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:37:02 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:37:02 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:37:02 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:37:02 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 20:37:51.084: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 20:37:51.127: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 20:37:51.177: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:37:51.177: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:37:51.177: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 20:37:51.177: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:37:51.177: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 29 20:37:51.177: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:37:51.177: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 29 20:37:51.177: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:37:51.177: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 20:37:51.177: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:37:51.177: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:37:51.177: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 29 20:37:51.177: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:37:51.177: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 29 20:37:51.177: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:37:51.177: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:37:51.177: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 29 20:37:51.177: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:37:51.177: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 20:37:51.177: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:37:51.336: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 20:37:51.336: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 20:37:51.378: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:43654,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentKubeletRestart False 2019-05-29 20:37:37 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 20:37:37 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 20:37:37 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 20:37:37 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-29 20:37:37 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-29 20:37:37 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 20:37:37 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:37:33 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:37:33 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:37:33 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:37:33 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 20:37:51.378: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 20:37:51.420: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 20:37:51.469: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:37:51.469: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 20:37:51.469: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:37:51.469: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:37:51.469: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 20:37:51.469: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 20:37:51.469: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 20:37:51.469: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 20:37:51.469: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:37:51.469: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 29 20:37:51.469: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 29 20:37:51.469: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 20:37:51.469: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 29 20:37:51.469: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 29 20:37:51.469: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 20:37:51.628: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 20:37:51.628: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:37:51.670: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:43607,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:37:18 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:37:18 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:37:18 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:37:18 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 20:37:51.671: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:37:51.713: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:37:51.913: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 20:37:51.913: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:37:51.955: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:43629,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:37:28 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:37:28 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:37:28 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:37:28 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 20:37:51.955: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:37:51.996: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:37:52.195: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 20:37:52.195: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:37:52.237: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:43689,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 20:37:51 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 20:37:51 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 20:37:51 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 20:37:51 -0700 PDT 2019-05-29 20:36:51 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 20:37:52.237: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:37:52.278: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:37:52.506: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 20:37:52.506: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;statefulset-1120&#34; for this suite.&#xA;May 29 20:37:58.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 20:38:00.259: INFO: namespace statefulset-1120 deletion completed in 7.710745729s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001922618">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="553.015404452"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for node-Service: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] NodeProblemDetector [DisabledForLargeClusters] should run without error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using ReplicationController with 0 secrets, 0 configmaps, 0 token projections, and 2 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Detaching volumes should not work when mount is in progress [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner delayed binding [Slow] should create persistent volumes in the same zone as node after a pod mounting the claims is started" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001959466">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should delete the token secret when the secret expired" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]" classname="Kubernetes e2e suite" time="231.457356962"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.002166741">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere should test that a vspehre volume mounted to a pod that is deleted while the kubelet is down unmounts when the kubelet returns [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Advanced Audit [DisabledForLargeClusters][Flaky] should list pods as impersonated user." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI attach test using mock driver should not require VolumeAttach for drivers without attachment" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] [Feature:GPUDevicePlugin] run Nvidia GPU Device Plugin tests" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should correctly scale down after a node is not needed and one node is broken [Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]" classname="Kubernetes e2e suite" time="15.344408876"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] [Feature:Windows] Memory Limits [Serial] [Slow] Allocatable node memory should be equal to a calculated allocatable memory value" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to switch session affinity for LoadBalancer service with ESIPP off [Slow] [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pods are pending due to pod anti-affinity [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] MetricsGrabber should grab all metrics from a Scheduler." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] NFSPersistentVolumes[Disruptive][Flaky] when kube-controller-manager restarts should delete a bound PVC from a clientPod, restart the kube-control-manager, and ensure the kube-controller-manager does not crash" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] DNS horizontal autoscaling [DisabledForLargeClusters] kube-dns-autoscaler should scale kube-dns pods in both nonfaulty and faulty scenarios" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [Feature:ClusterSizeAutoscalingScaleUp] [Slow] Autoscaling [sig-autoscaling] Autoscaling a service from 1 pod and 3 nodes to 8 pods and &gt;=4 nodes takes less than 15 minutes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI attach test using mock driver should require VolumeAttach for drivers with attachment" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001963562">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support seccomp default which is unconfined [Feature:Seccomp] [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Attach Verify [Feature:vsphere][Serial][Disruptive] verify volume remains attached after master kubelet restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Metadata Concealment should run a check-metadata-concealment job to completion" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Ports Security Check [Feature:KubeletSecurity] should not be able to proxy to the readonly kubelet port 10255 using proxy subresource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Feature:CustomResourcePublishOpenAPI] updates the published spec when one versin gets renamed" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="257.219017642"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Conformance]" classname="Kubernetes e2e suite" time="372.808607412"></testcase>
      <testcase name="[sig-ui] Kubernetes Dashboard should check that the kubernetes-dashboard instance is alive" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should be able to scale a node group up from 0[Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should not schedule jobs when suspended [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]" classname="Kubernetes e2e suite" time="1565.205738132">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;May 29 21:17:12.590: Failed waiting for state update: timed out waiting for the condition&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/statefulset_utils.go:337</failure>
          <system-out>[BeforeEach] [sig-apps] StatefulSet&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 21:01:50.117: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename statefulset&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-apps] StatefulSet&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59&#xA;[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74&#xA;�[1mSTEP�[0m: Creating service test in namespace statefulset-9070&#xA;[It] should perform rolling updates and roll backs of template modifications [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: Creating a new StatefulSet&#xA;May 29 21:01:50.454: INFO: Found 1 stateful pods, waiting for 3&#xA;May 29 21:02:00.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:02:10.498: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:02:20.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:02:30.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:02:40.501: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:02:50.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:03:00.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:03:10.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:03:20.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:03:30.496: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:03:40.499: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:03:50.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:04:00.498: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:04:10.504: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:04:20.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:04:30.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:04:40.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:04:50.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:05:00.498: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:05:10.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:05:20.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:05:30.498: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:05:40.498: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:05:50.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:06:00.498: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:06:10.498: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:06:20.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:06:30.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:06:40.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:06:50.497: INFO: Found 2 stateful pods, waiting for 3&#xA;May 29 21:07:00.497: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 21:07:00.497: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 21:07:00.497: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false&#xA;May 29 21:07:10.497: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 21:07:10.497: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 21:07:10.497: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true&#xA;May 29 21:07:10.626: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config exec --namespace=statefulset-9070 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true&#39;&#xA;May 29 21:07:11.288: INFO: stderr: &#34;+ mv -v /usr/share/nginx/html/index.html /tmp/\n&#34;&#xA;May 29 21:07:11.288: INFO: stdout: &#34;&#39;/usr/share/nginx/html/index.html&#39; -&gt; &#39;/tmp/index.html&#39;\n&#34;&#xA;May 29 21:07:11.288: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: &#39;/usr/share/nginx/html/index.html&#39; -&gt; &#39;/tmp/index.html&#39;&#xA;&#xA;�[1mSTEP�[0m: Updating StatefulSet template: update image from e2eteam/nginx:1.14-alpine to e2eteam/nginx:1.15-alpine&#xA;May 29 21:07:11.471: INFO: Updating stateful set ss2&#xA;�[1mSTEP�[0m: Creating a new revision&#xA;�[1mSTEP�[0m: Updating Pods in reverse ordinal order&#xA;May 29 21:07:11.598: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config exec --namespace=statefulset-9070 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true&#39;&#xA;May 29 21:07:12.241: INFO: stderr: &#34;+ mv -v /tmp/index.html /usr/share/nginx/html/\n&#34;&#xA;May 29 21:07:12.241: INFO: stdout: &#34;&#39;/tmp/index.html&#39; -&gt; &#39;/usr/share/nginx/html/index.html&#39;\n&#34;&#xA;May 29 21:07:12.241: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: &#39;/tmp/index.html&#39; -&gt; &#39;/usr/share/nginx/html/index.html&#39;&#xA;&#xA;May 29 21:07:12.422: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:07:12.422: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:07:12.422: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:07:12.422: INFO: Waiting for Pod statefulset-9070/ss2-2 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:07:22.508: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:07:22.508: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:07:22.508: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:07:32.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:07:32.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:07:32.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:07:42.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:07:42.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:07:42.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:07:52.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:07:52.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:07:52.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:08:02.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:08:02.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:08:02.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:08:12.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:08:12.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:08:12.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:08:22.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:08:22.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:08:22.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:08:32.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:08:32.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:08:32.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:08:42.509: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:08:42.509: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:08:42.509: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:08:52.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:08:52.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:08:52.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:09:02.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:09:02.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:09:02.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:09:12.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:09:12.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:09:12.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:09:22.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:09:22.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:09:22.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:09:32.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:09:32.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:09:32.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:09:42.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:09:42.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:09:42.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:09:52.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:09:52.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:09:52.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:10:02.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:10:02.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:10:02.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:10:12.515: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:10:12.516: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:10:12.516: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:10:22.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:10:22.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:10:22.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:10:32.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:10:32.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:10:32.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:10:42.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:10:42.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:10:42.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:10:52.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:10:52.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:10:52.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:11:02.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:11:02.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:11:02.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:11:12.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:11:12.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:11:12.508: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:11:22.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:11:22.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:11:22.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:11:32.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:11:32.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:11:32.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:11:42.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:11:42.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:11:42.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:11:52.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:11:52.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:11:52.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:12:02.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:12:02.508: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:12:02.508: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:12:12.508: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:12:12.508: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:12:12.508: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:12:22.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:12:22.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:12:22.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:12:32.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:12:32.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:12:32.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:12:42.523: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:12:42.523: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:12:42.524: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:12:52.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:12:52.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:12:52.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:13:02.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:13:02.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:13:02.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:13:12.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:13:12.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:13:12.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:13:22.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:13:22.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:13:22.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:13:32.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:13:32.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:13:32.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:13:42.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:13:42.508: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:13:42.508: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:13:52.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:13:52.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:13:52.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:14:02.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:14:02.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:14:02.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:14:12.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:14:12.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:14:12.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:14:22.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:14:22.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:14:22.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:14:32.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:14:32.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:14:32.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:14:42.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:14:42.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:14:42.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:14:52.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:14:52.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:14:52.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:15:02.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:15:02.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:15:02.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:15:12.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:15:12.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:15:12.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:15:22.511: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:15:22.511: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:15:22.511: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:15:32.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:15:32.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:15:32.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:15:42.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:15:42.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:15:42.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:15:52.514: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:15:52.514: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:15:52.514: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:16:02.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:16:02.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:16:02.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:16:12.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:16:12.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:16:12.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:16:22.512: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:16:22.512: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:16:22.512: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:16:32.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:16:32.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:16:32.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:16:42.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:16:42.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:16:42.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:16:52.507: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:16:52.507: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:16:52.507: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:17:02.508: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:17:02.508: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:17:02.508: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:17:12.506: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:17:12.506: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:17:12.506: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:17:12.590: INFO: Waiting for StatefulSet statefulset-9070/ss2 to complete update&#xA;May 29 21:17:12.590: INFO: Waiting for Pod statefulset-9070/ss2-0 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:17:12.590: INFO: Waiting for Pod statefulset-9070/ss2-1 to have revision ss2-5f4db4b9f4 update revision ss2-577b4dc465&#xA;May 29 21:17:12.590: INFO: Failed waiting for state update: timed out waiting for the condition&#xA;[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85&#xA;May 29 21:17:12.632: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config describe po ss2-0 --namespace=statefulset-9070&#39;&#xA;May 29 21:17:13.473: INFO: stderr: &#34;&#34;&#xA;May 29 21:17:13.473: INFO: stdout: &#34;Name:               ss2-0\nNamespace:          statefulset-9070\nPriority:           0\nPriorityClassName:  &lt;none&gt;\nNode:               e2e-test-peterhornyack-windows-node-group-1vjk/10.40.0.5\nStart Time:         Wed, 29 May 2019 21:01:50 -0700\nLabels:             baz=blah\n                    controller-revision-hash=ss2-577b4dc465\n                    foo=bar\n                    statefulset.kubernetes.io/pod-name=ss2-0\nAnnotations:        &lt;none&gt;\nStatus:             Running\nIP:                 10.64.2.48\nControlled By:      StatefulSet/ss2\nContainers:\n  nginx:\n    Container ID:   docker://0d865ab018b0c54f0ae25129ad12aca1b28b72176acfadb1e59a9886551984bd\n    Image:          e2eteam/nginx:1.14-alpine\n    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14\n    Port:           &lt;none&gt;\n    Host Port:      &lt;none&gt;\n    State:          Running\n      Started:      Wed, 29 May 2019 21:01:54 -0700\n    Ready:          True\n    Restart Count:  0\n    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7bfwg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-7bfwg:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-7bfwg\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  &lt;none&gt;\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                     Message\n  ----    ------     ----  ----                                                     -------\n  Normal  Scheduled  15m   default-scheduler                                        Successfully assigned statefulset-9070/ss2-0 to e2e-test-peterhornyack-windows-node-group-1vjk\n  Normal  Pulled     15m   kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Container image \&#34;e2eteam/nginx:1.14-alpine\&#34; already present on machine\n  Normal  Created    15m   kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Created container nginx\n  Normal  Started    15m   kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Started container nginx\n&#34;&#xA;May 29 21:17:13.473: INFO: &#xA;Output of kubectl describe ss2-0:&#xA;Name:               ss2-0&#xA;Namespace:          statefulset-9070&#xA;Priority:           0&#xA;PriorityClassName:  &lt;none&gt;&#xA;Node:               e2e-test-peterhornyack-windows-node-group-1vjk/10.40.0.5&#xA;Start Time:         Wed, 29 May 2019 21:01:50 -0700&#xA;Labels:             baz=blah&#xA;                    controller-revision-hash=ss2-577b4dc465&#xA;                    foo=bar&#xA;                    statefulset.kubernetes.io/pod-name=ss2-0&#xA;Annotations:        &lt;none&gt;&#xA;Status:             Running&#xA;IP:                 10.64.2.48&#xA;Controlled By:      StatefulSet/ss2&#xA;Containers:&#xA;  nginx:&#xA;    Container ID:   docker://0d865ab018b0c54f0ae25129ad12aca1b28b72176acfadb1e59a9886551984bd&#xA;    Image:          e2eteam/nginx:1.14-alpine&#xA;    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14&#xA;    Port:           &lt;none&gt;&#xA;    Host Port:      &lt;none&gt;&#xA;    State:          Running&#xA;      Started:      Wed, 29 May 2019 21:01:54 -0700&#xA;    Ready:          True&#xA;    Restart Count:  0&#xA;    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1&#xA;    Environment:    &lt;none&gt;&#xA;    Mounts:&#xA;      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7bfwg (ro)&#xA;Conditions:&#xA;  Type              Status&#xA;  Initialized       True &#xA;  Ready             True &#xA;  ContainersReady   True &#xA;  PodScheduled      True &#xA;Volumes:&#xA;  default-token-7bfwg:&#xA;    Type:        Secret (a volume populated by a Secret)&#xA;    SecretName:  default-token-7bfwg&#xA;    Optional:    false&#xA;QoS Class:       BestEffort&#xA;Node-Selectors:  &lt;none&gt;&#xA;Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s&#xA;                 node.kubernetes.io/unreachable:NoExecute for 300s&#xA;Events:&#xA;  Type    Reason     Age   From                                                     Message&#xA;  ----    ------     ----  ----                                                     -------&#xA;  Normal  Scheduled  15m   default-scheduler                                        Successfully assigned statefulset-9070/ss2-0 to e2e-test-peterhornyack-windows-node-group-1vjk&#xA;  Normal  Pulled     15m   kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;  Normal  Created    15m   kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Created container nginx&#xA;  Normal  Started    15m   kubelet, e2e-test-peterhornyack-windows-node-group-1vjk  Started container nginx&#xA;&#xA;May 29 21:17:13.473: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config logs ss2-0 --namespace=statefulset-9070 --tail=100&#39;&#xA;May 29 21:17:13.787: INFO: stderr: &#34;&#34;&#xA;May 29 21:17:13.787: INFO: stdout: &#34;&#34;&#xA;May 29 21:17:13.787: INFO: &#xA;Last 100 log lines of ss2-0:&#xA;&#xA;May 29 21:17:13.787: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config describe po ss2-1 --namespace=statefulset-9070&#39;&#xA;May 29 21:17:14.136: INFO: stderr: &#34;&#34;&#xA;May 29 21:17:14.136: INFO: stdout: &#34;Name:                      ss2-1\nNamespace:                 statefulset-9070\nPriority:                  0\nPriorityClassName:         &lt;none&gt;\nNode:                      e2e-test-peterhornyack-windows-node-group-jpxd/10.40.0.3\nStart Time:                Wed, 29 May 2019 21:01:57 -0700\nLabels:                    baz=blah\n                           controller-revision-hash=ss2-577b4dc465\n                           foo=bar\n                           statefulset.kubernetes.io/pod-name=ss2-1\nAnnotations:               &lt;none&gt;\nStatus:                    Terminating (lasts 9m)\nTermination Grace Period:  30s\nIP:                        10.64.1.103\nControlled By:             StatefulSet/ss2\nContainers:\n  nginx:\n    Container ID:   docker://7000d7494890994ea1d31ace85e2384a3441fc8b496ec408d86396931c42f3fd\n    Image:          e2eteam/nginx:1.14-alpine\n    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14\n    Port:           &lt;none&gt;\n    Host Port:      &lt;none&gt;\n    State:          Running\n      Started:      Wed, 29 May 2019 21:02:02 -0700\n    Ready:          False\n    Restart Count:  0\n    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7bfwg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             False \n  ContainersReady   False \n  PodScheduled      True \nVolumes:\n  default-token-7bfwg:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-7bfwg\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  &lt;none&gt;\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type     Reason     Age   From                                                     Message\n  ----     ------     ----  ----                                                     -------\n  Normal   Scheduled  15m   default-scheduler                                        Successfully assigned statefulset-9070/ss2-1 to e2e-test-peterhornyack-windows-node-group-jpxd\n  Normal   Pulled     15m   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Container image \&#34;e2eteam/nginx:1.14-alpine\&#34; already present on machine\n  Normal   Created    15m   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Created container nginx\n  Normal   Started    15m   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Started container nginx\n  Warning  Unhealthy  10m   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Readiness probe failed: HTTP probe failed with statuscode: 404\n  Normal   Killing    17s   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Stopping container nginx\n&#34;&#xA;May 29 21:17:14.137: INFO: &#xA;Output of kubectl describe ss2-1:&#xA;Name:                      ss2-1&#xA;Namespace:                 statefulset-9070&#xA;Priority:                  0&#xA;PriorityClassName:         &lt;none&gt;&#xA;Node:                      e2e-test-peterhornyack-windows-node-group-jpxd/10.40.0.3&#xA;Start Time:                Wed, 29 May 2019 21:01:57 -0700&#xA;Labels:                    baz=blah&#xA;                           controller-revision-hash=ss2-577b4dc465&#xA;                           foo=bar&#xA;                           statefulset.kubernetes.io/pod-name=ss2-1&#xA;Annotations:               &lt;none&gt;&#xA;Status:                    Terminating (lasts 9m)&#xA;Termination Grace Period:  30s&#xA;IP:                        10.64.1.103&#xA;Controlled By:             StatefulSet/ss2&#xA;Containers:&#xA;  nginx:&#xA;    Container ID:   docker://7000d7494890994ea1d31ace85e2384a3441fc8b496ec408d86396931c42f3fd&#xA;    Image:          e2eteam/nginx:1.14-alpine&#xA;    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14&#xA;    Port:           &lt;none&gt;&#xA;    Host Port:      &lt;none&gt;&#xA;    State:          Running&#xA;      Started:      Wed, 29 May 2019 21:02:02 -0700&#xA;    Ready:          False&#xA;    Restart Count:  0&#xA;    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1&#xA;    Environment:    &lt;none&gt;&#xA;    Mounts:&#xA;      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7bfwg (ro)&#xA;Conditions:&#xA;  Type              Status&#xA;  Initialized       True &#xA;  Ready             False &#xA;  ContainersReady   False &#xA;  PodScheduled      True &#xA;Volumes:&#xA;  default-token-7bfwg:&#xA;    Type:        Secret (a volume populated by a Secret)&#xA;    SecretName:  default-token-7bfwg&#xA;    Optional:    false&#xA;QoS Class:       BestEffort&#xA;Node-Selectors:  &lt;none&gt;&#xA;Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s&#xA;                 node.kubernetes.io/unreachable:NoExecute for 300s&#xA;Events:&#xA;  Type     Reason     Age   From                                                     Message&#xA;  ----     ------     ----  ----                                                     -------&#xA;  Normal   Scheduled  15m   default-scheduler                                        Successfully assigned statefulset-9070/ss2-1 to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;  Normal   Pulled     15m   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;  Normal   Created    15m   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Created container nginx&#xA;  Normal   Started    15m   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Started container nginx&#xA;  Warning  Unhealthy  10m   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Readiness probe failed: HTTP probe failed with statuscode: 404&#xA;  Normal   Killing    17s   kubelet, e2e-test-peterhornyack-windows-node-group-jpxd  Stopping container nginx&#xA;&#xA;May 29 21:17:14.137: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config logs ss2-1 --namespace=statefulset-9070 --tail=100&#39;&#xA;May 29 21:17:14.452: INFO: stderr: &#34;&#34;&#xA;May 29 21:17:14.452: INFO: stdout: &#34;&#34;&#xA;May 29 21:17:14.452: INFO: &#xA;Last 100 log lines of ss2-1:&#xA;&#xA;May 29 21:17:14.453: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config describe po ss2-2 --namespace=statefulset-9070&#39;&#xA;May 29 21:17:14.806: INFO: stderr: &#34;&#34;&#xA;May 29 21:17:14.806: INFO: stdout: &#34;Name:               ss2-2\nNamespace:          statefulset-9070\nPriority:           0\nPriorityClassName:  &lt;none&gt;\nNode:               e2e-test-peterhornyack-windows-node-group-9q9v/10.40.0.4\nStart Time:         Wed, 29 May 2019 21:07:20 -0700\nLabels:             baz=blah\n                    controller-revision-hash=ss2-5f4db4b9f4\n                    foo=bar\n                    statefulset.kubernetes.io/pod-name=ss2-2\nAnnotations:        &lt;none&gt;\nStatus:             Running\nIP:                 10.64.3.93\nControlled By:      StatefulSet/ss2\nContainers:\n  nginx:\n    Container ID:   docker://c826d5396d4c19de105fe225411218284f0f5eb7c9ab9fc0917cfb2c2ce0dd62\n    Image:          e2eteam/nginx:1.15-alpine\n    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14\n    Port:           &lt;none&gt;\n    Host Port:      &lt;none&gt;\n    State:          Running\n      Started:      Wed, 29 May 2019 21:07:25 -0700\n    Ready:          True\n    Restart Count:  0\n    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7bfwg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-7bfwg:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-7bfwg\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  &lt;none&gt;\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                     Message\n  ----    ------     ----  ----                                                     -------\n  Normal  Scheduled  9m    default-scheduler                                        Successfully assigned statefulset-9070/ss2-2 to e2e-test-peterhornyack-windows-node-group-9q9v\n  Normal  Pulled     9m    kubelet, e2e-test-peterhornyack-windows-node-group-9q9v  Container image \&#34;e2eteam/nginx:1.15-alpine\&#34; already present on machine\n  Normal  Created    9m    kubelet, e2e-test-peterhornyack-windows-node-group-9q9v  Created container nginx\n  Normal  Started    9m    kubelet, e2e-test-peterhornyack-windows-node-group-9q9v  Started container nginx\n&#34;&#xA;May 29 21:17:14.806: INFO: &#xA;Output of kubectl describe ss2-2:&#xA;Name:               ss2-2&#xA;Namespace:          statefulset-9070&#xA;Priority:           0&#xA;PriorityClassName:  &lt;none&gt;&#xA;Node:               e2e-test-peterhornyack-windows-node-group-9q9v/10.40.0.4&#xA;Start Time:         Wed, 29 May 2019 21:07:20 -0700&#xA;Labels:             baz=blah&#xA;                    controller-revision-hash=ss2-5f4db4b9f4&#xA;                    foo=bar&#xA;                    statefulset.kubernetes.io/pod-name=ss2-2&#xA;Annotations:        &lt;none&gt;&#xA;Status:             Running&#xA;IP:                 10.64.3.93&#xA;Controlled By:      StatefulSet/ss2&#xA;Containers:&#xA;  nginx:&#xA;    Container ID:   docker://c826d5396d4c19de105fe225411218284f0f5eb7c9ab9fc0917cfb2c2ce0dd62&#xA;    Image:          e2eteam/nginx:1.15-alpine&#xA;    Image ID:       docker-pullable://e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14&#xA;    Port:           &lt;none&gt;&#xA;    Host Port:      &lt;none&gt;&#xA;    State:          Running&#xA;      Started:      Wed, 29 May 2019 21:07:25 -0700&#xA;    Ready:          True&#xA;    Restart Count:  0&#xA;    Readiness:      http-get http://:80/index.html delay=0s timeout=1s period=1s #success=1 #failure=1&#xA;    Environment:    &lt;none&gt;&#xA;    Mounts:&#xA;      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7bfwg (ro)&#xA;Conditions:&#xA;  Type              Status&#xA;  Initialized       True &#xA;  Ready             True &#xA;  ContainersReady   True &#xA;  PodScheduled      True &#xA;Volumes:&#xA;  default-token-7bfwg:&#xA;    Type:        Secret (a volume populated by a Secret)&#xA;    SecretName:  default-token-7bfwg&#xA;    Optional:    false&#xA;QoS Class:       BestEffort&#xA;Node-Selectors:  &lt;none&gt;&#xA;Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s&#xA;                 node.kubernetes.io/unreachable:NoExecute for 300s&#xA;Events:&#xA;  Type    Reason     Age   From                                                     Message&#xA;  ----    ------     ----  ----                                                     -------&#xA;  Normal  Scheduled  9m    default-scheduler                                        Successfully assigned statefulset-9070/ss2-2 to e2e-test-peterhornyack-windows-node-group-9q9v&#xA;  Normal  Pulled     9m    kubelet, e2e-test-peterhornyack-windows-node-group-9q9v  Container image &#34;e2eteam/nginx:1.15-alpine&#34; already present on machine&#xA;  Normal  Created    9m    kubelet, e2e-test-peterhornyack-windows-node-group-9q9v  Created container nginx&#xA;  Normal  Started    9m    kubelet, e2e-test-peterhornyack-windows-node-group-9q9v  Started container nginx&#xA;&#xA;May 29 21:17:14.806: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config logs ss2-2 --namespace=statefulset-9070 --tail=100&#39;&#xA;May 29 21:17:15.127: INFO: stderr: &#34;&#34;&#xA;May 29 21:17:15.127: INFO: stdout: &#34;&#34;&#xA;May 29 21:17:15.127: INFO: &#xA;Last 100 log lines of ss2-2:&#xA;&#xA;May 29 21:17:15.127: INFO: Deleting all statefulset in ns statefulset-9070&#xA;May 29 21:17:15.169: INFO: Scaling statefulset ss2 to 0&#xA;May 29 21:27:15.383: INFO: Waiting for statefulset status.replicas updated to 0&#xA;May 29 21:27:15.426: INFO: Waiting for stateful set status.replicas to become 0, currently 1&#xA;May 29 21:27:25.468: INFO: Waiting for stateful set status.replicas to become 0, currently 1&#xA;May 29 21:27:35.468: INFO: Waiting for stateful set status.replicas to become 0, currently 1&#xA;May 29 21:27:45.468: INFO: Deleting statefulset ss2&#xA;May 29 21:27:45.596: INFO: Unexpected error occurred: Failed to scale statefulset to 0 in 10m0s. Remaining pods:&#xA;[ss2-0: deletion 2019-05-29 21:27:34 -0700 PDT, phase Running, readiness false]&#xA;[AfterEach] [sig-apps] StatefulSet&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;statefulset-9070&#34;.&#xA;�[1mSTEP�[0m: Found 28 events.&#xA;May 29 21:27:45.639: INFO: At 2019-05-29 21:01:50 -0700 PDT - event for ss2: {statefulset-controller } SuccessfulCreate: create Pod ss2-0 in StatefulSet ss2 successful&#xA;May 29 21:27:45.639: INFO: At 2019-05-29 21:01:50 -0700 PDT - event for ss2-0: {default-scheduler } Scheduled: Successfully assigned statefulset-9070/ss2-0 to e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 21:27:45.639: INFO: At 2019-05-29 21:01:52 -0700 PDT - event for ss2-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Created: Created container nginx&#xA;May 29 21:27:45.639: INFO: At 2019-05-29 21:01:52 -0700 PDT - event for ss2-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Pulled: Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;May 29 21:27:45.639: INFO: At 2019-05-29 21:01:55 -0700 PDT - event for ss2-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Started: Started container nginx&#xA;May 29 21:27:45.639: INFO: At 2019-05-29 21:01:57 -0700 PDT - event for ss2: {statefulset-controller } SuccessfulCreate: create Pod ss2-1 in StatefulSet ss2 successful&#xA;May 29 21:27:45.639: INFO: At 2019-05-29 21:01:57 -0700 PDT - event for ss2-1: {default-scheduler } Scheduled: Successfully assigned statefulset-9070/ss2-1 to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 21:27:45.639: INFO: At 2019-05-29 21:01:59 -0700 PDT - event for ss2-1: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:01:59 -0700 PDT - event for ss2-1: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container nginx&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:02:02 -0700 PDT - event for ss2-1: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container nginx&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:05:06 -0700 PDT - event for ss2-1: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod statefulset-9070/ss2-1&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:06:55 -0700 PDT - event for ss2: {statefulset-controller } SuccessfulCreate: create Pod ss2-2 in StatefulSet ss2 successful&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:06:55 -0700 PDT - event for ss2-2: {default-scheduler } Scheduled: Successfully assigned statefulset-9070/ss2-2 to e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:06:57 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Pulled: Container image &#34;e2eteam/nginx:1.14-alpine&#34; already present on machine&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:06:57 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Created: Created container nginx&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:07:00 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Started: Started container nginx&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:07:11 -0700 PDT - event for ss2-1: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Unhealthy: Readiness probe failed: HTTP probe failed with statuscode: 404&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:07:12 -0700 PDT - event for ss2: {statefulset-controller } SuccessfulDelete: delete Pod ss2-2 in StatefulSet ss2 successful&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:07:12 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Killing: Stopping container nginx&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:07:21 -0700 PDT - event for ss2-2: {default-scheduler } Scheduled: Successfully assigned statefulset-9070/ss2-2 to e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:07:23 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Pulled: Container image &#34;e2eteam/nginx:1.15-alpine&#34; already present on machine&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:07:23 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Created: Created container nginx&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:07:26 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Started: Started container nginx&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:07:29 -0700 PDT - event for ss2: {statefulset-controller } SuccessfulDelete: delete Pod ss2-1 in StatefulSet ss2 successful&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:16:57 -0700 PDT - event for ss2-1: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Killing: Stopping container nginx&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:17:15 -0700 PDT - event for ss2-2: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Killing: Stopping container nginx&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:27:04 -0700 PDT - event for ss2: {statefulset-controller } SuccessfulDelete: delete Pod ss2-0 in StatefulSet ss2 successful&#xA;May 29 21:27:45.640: INFO: At 2019-05-29 21:27:04 -0700 PDT - event for ss2-0: {kubelet e2e-test-peterhornyack-windows-node-group-1vjk} Killing: Stopping container nginx&#xA;May 29 21:27:45.727: INFO: POD                                                    NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 29 21:27:45.728: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:27:45.728: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 29 21:27:45.729: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 29 21:27:45.729: INFO: &#xA;May 29 21:27:45.772: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 29 21:27:45.814: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:50986,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 21:26:46 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 21:26:46 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 21:26:46 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 21:26:46 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 21:27:45.814: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 29 21:27:45.855: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 29 21:27:45.901: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:27:45.901: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:27:45.901: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:27:45.901: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:27:45.901: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:27:45.901: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:27:45.901: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:27:45.901: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 21:27:45.901: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 21:27:45.901: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:27:45.901: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:27:45.901: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:27:45.901: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 21:27:45.901: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 21:27:46.064: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 29 21:27:46.064: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 21:27:46.106: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:51056,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentKubeletRestart False 2019-05-29 21:27:11 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 21:27:11 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 21:27:11 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 21:27:11 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-29 21:27:11 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 21:27:11 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-29 21:27:11 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 21:27:06 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 21:27:06 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 21:27:06 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 21:27:06 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 21:27:46.106: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 21:27:46.147: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 21:27:46.195: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:27:46.195: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 29 21:27:46.195: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 21:27:46.195: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 21:27:46.195: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 29 21:27:46.195: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:27:46.195: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 21:27:46.195: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 21:27:46.195: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 21:27:46.195: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 29 21:27:46.195: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 21:27:46.195: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 21:27:46.195: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 21:27:46.195: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 29 21:27:46.195: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 21:27:46.195: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 29 21:27:46.195: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:27:46.195: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 21:27:46.195: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 21:27:46.195: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:27:46.356: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 21:27:46.356: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 21:27:46.397: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:51112,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{ReadonlyFilesystem False 2019-05-29 21:27:02 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-29 21:27:02 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 21:27:02 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 21:27:02 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 21:27:02 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-29 21:27:02 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-29 21:27:02 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 21:27:37 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 21:27:37 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 21:27:37 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 21:27:37 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 21:27:46.398: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 21:27:46.440: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 21:27:46.488: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:27:46.488: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 29 21:27:46.488: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 29 21:27:46.488: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:27:46.488: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:27:46.488: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 21:27:46.488: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 21:27:46.488: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:27:46.488: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 21:27:46.488: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 21:27:46.488: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 21:27:46.488: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 21:27:46.488: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:27:46.488: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 29 21:27:46.488: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 29 21:27:46.662: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 21:27:46.662: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 21:27:46.704: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:51081,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 21:27:23 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 21:27:23 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 21:27:23 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 21:27:23 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 21:27:46.705: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 21:27:46.746: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 21:27:46.951: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 21:27:46.951: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 21:27:46.993: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:51104,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 21:27:33 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 21:27:33 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 21:27:33 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 21:27:33 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 21:27:46.994: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 21:27:47.035: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 21:27:47.234: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 21:27:47.234: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 21:27:47.276: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:51038,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 21:27:06 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 21:27:06 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 21:27:06 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 21:27:06 -0700 PDT 2019-05-29 21:25:06 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 21:27:47.276: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 21:27:47.318: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 21:27:47.545: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 21:27:47.545: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;statefulset-9070&#34; for this suite.&#xA;May 29 21:27:53.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 21:27:55.323: INFO: namespace statefulset-9070 deletion completed in 7.735341641s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should update endpoints: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="24.580262082"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.00204659">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should mutate custom resource with different stored version" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] single and multi-cluster ingresses should be able to exist together" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to switch session affinity for NodePort service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap Should fail non-optional pod creation due to the key in the configMap object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Update Demo should create and stop a replication controller  [Conformance]" classname="Kubernetes e2e suite" time="69.909123065"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should sync endpoints for both Ingress-referenced NEG and standalone NEG" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects a client request should support a client that connects, sends DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="222.676603752"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Network should set TCP CLOSE_WAIT timeout" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.002280972">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Advanced Audit [DisabledForLargeClusters][Flaky] should audit API calls to create, get, update, patch, delete, list, watch secrets." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Disruptive]NodeLease NodeLease deletion node lease should be deleted when corresponding node is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] should scale down underutilized nodes [Feature:ClusterAutoscalerScalability4]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.001766089">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should allow substituting values in a container&#39;s command [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="20.510323871"></testcase>
      <testcase name="[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should not provision a volume in an unmanaged GCE zone." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]" classname="Kubernetes e2e suite" time="18.292662495"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.002199627">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PV and a pre-bound PVC: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.00195136">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.001831688">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="915.73833135">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0002b5440&gt;: {&#xA;        s: &#34;timed out waiting for the condition&#34;,&#xA;    }&#xA;    timed out waiting for the condition&#xA;occurred&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/pods.go:112</failure>
          <system-out>[BeforeEach] [k8s.io] Container Lifecycle Hook&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 21:33:51.307: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename container-lifecycle-hook&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] when create a pod with lifecycle hook&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61&#xA;�[1mSTEP�[0m: create the container to handle the HTTPGet hook request.&#xA;[It] should execute prestop http hook properly [NodeConformance] [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: create the pod with lifecycle hook&#xA;May 29 21:38:59.837: INFO: Unexpected error occurred: timed out waiting for the condition&#xA;[AfterEach] [k8s.io] Container Lifecycle Hook&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;container-lifecycle-hook-4466&#34;.&#xA;�[1mSTEP�[0m: Found 9 events.&#xA;May 29 21:38:59.880: INFO: At 2019-05-29 21:33:51 -0700 PDT - event for pod-handle-http-request: {default-scheduler } Scheduled: Successfully assigned container-lifecycle-hook-4466/pod-handle-http-request to e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 21:38:59.880: INFO: At 2019-05-29 21:33:54 -0700 PDT - event for pod-handle-http-request: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Pulled: Container image &#34;e2eteam/netexec:1.1&#34; already present on machine&#xA;May 29 21:38:59.880: INFO: At 2019-05-29 21:33:54 -0700 PDT - event for pod-handle-http-request: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Created: Created container pod-handle-http-request&#xA;May 29 21:38:59.880: INFO: At 2019-05-29 21:33:56 -0700 PDT - event for pod-handle-http-request: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Started: Started container pod-handle-http-request&#xA;May 29 21:38:59.880: INFO: At 2019-05-29 21:33:59 -0700 PDT - event for pod-with-prestop-http-hook: {default-scheduler } Scheduled: Successfully assigned container-lifecycle-hook-4466/pod-with-prestop-http-hook to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 21:38:59.880: INFO: At 2019-05-29 21:34:01 -0700 PDT - event for pod-with-prestop-http-hook: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/pause:3.1&#34; already present on machine&#xA;May 29 21:38:59.880: INFO: At 2019-05-29 21:34:01 -0700 PDT - event for pod-with-prestop-http-hook: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container pod-with-prestop-http-hook&#xA;May 29 21:38:59.880: INFO: At 2019-05-29 21:34:03 -0700 PDT - event for pod-with-prestop-http-hook: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container pod-with-prestop-http-hook&#xA;May 29 21:38:59.880: INFO: At 2019-05-29 21:37:12 -0700 PDT - event for pod-with-prestop-http-hook: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod container-lifecycle-hook-4466/pod-with-prestop-http-hook&#xA;May 29 21:38:59.968: INFO: POD                                                    NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 21:38:59.968: INFO: pod-handle-http-request                                e2e-test-peterhornyack-windows-node-group-9q9v  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:33:51 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:33:58 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:33:58 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:33:51 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: pod-with-prestop-http-hook                             e2e-test-peterhornyack-windows-node-group-jpxd  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:33:59 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:33:59 -0700 PDT ContainersNotReady containers with unready status: [pod-with-prestop-http-hook]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:33:59 -0700 PDT ContainersNotReady containers with unready status: [pod-with-prestop-http-hook]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:33:59 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 29 21:38:59.969: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 29 21:38:59.970: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 21:38:59.970: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:38:59.970: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:38:59.970: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 21:38:59.970: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:38:59.970: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 21:38:59.970: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 29 21:38:59.970: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 29 21:38:59.970: INFO: &#xA;May 29 21:39:00.014: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 29 21:39:00.056: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:52837,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 21:38:48 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 21:38:48 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 21:38:48 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 21:38:48 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 21:39:00.056: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 29 21:39:00.097: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 29 21:39:00.151: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:39:00.151: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:39:00.151: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:39:00.151: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 21:39:00.151: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 21:39:00.151: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:39:00.151: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:39:00.151: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 21:39:00.151: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 21:39:00.151: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:39:00.151: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:39:00.151: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:39:00.151: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:39:00.151: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:39:00.301: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 29 21:39:00.301: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 21:39:00.342: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:52765,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentDockerRestart False 2019-05-29 21:38:16 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 21:38:16 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 21:38:16 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-29 21:38:16 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 21:38:16 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-29 21:38:16 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-29 21:38:16 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 21:38:06 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 21:38:06 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 21:38:06 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 21:38:06 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 21:39:00.342: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 21:39:00.383: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 21:39:00.434: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 21:39:00.434: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 29 21:39:00.434: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:39:00.434: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 29 21:39:00.434: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 21:39:00.434: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 21:39:00.434: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 29 21:39:00.434: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:39:00.434: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 21:39:00.434: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 21:39:00.434: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 21:39:00.434: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 29 21:39:00.434: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:39:00.434: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 21:39:00.434: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 21:39:00.434: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:39:00.434: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 21:39:00.434: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 21:39:00.434: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 21:39:00.434: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 29 21:39:00.585: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 21:39:00.585: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 21:39:00.626: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:52814,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentUnregisterNetDevice False 2019-05-29 21:38:11 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-29 21:38:11 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 21:38:11 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-29 21:38:11 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 21:38:11 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 21:38:11 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 21:38:11 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 21:38:38 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 21:38:38 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 21:38:38 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 21:38:38 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 21:39:00.627: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 21:39:00.668: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 21:39:00.724: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:39:00.724: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 21:39:00.724: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 21:39:00.724: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:39:00.724: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 21:39:00.724: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 21:39:00.724: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 21:39:00.724: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 21:39:00.724: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:39:00.724: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 29 21:39:00.724: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 29 21:39:00.724: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 21:39:00.724: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 29 21:39:00.724: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 29 21:39:00.724: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 21:39:00.893: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 21:39:00.893: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 21:39:00.935: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:52783,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 21:38:24 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 21:38:24 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 21:38:24 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 21:38:24 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 21:39:00.936: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 21:39:00.977: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 21:39:01.182: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 21:39:01.183: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 21:39:01.224: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:52806,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 21:38:34 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 21:38:34 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 21:38:34 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 21:38:34 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 21:39:01.224: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 21:39:01.266: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 21:39:01.314: INFO: pod-handle-http-request started at 2019-05-29 21:33:51 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 21:39:01.314: INFO: &#x9;Container pod-handle-http-request ready: true, restart count 0&#xA;May 29 21:39:02.786: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 21:39:02.786: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 21:39:02.828: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:52742,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 21:38:07 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 21:38:07 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 21:38:07 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 21:38:07 -0700 PDT 2019-05-29 21:37:07 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 21:39:02.828: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 21:39:02.870: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 21:39:02.918: INFO: pod-with-prestop-http-hook started at 2019-05-29 21:33:59 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 21:39:02.918: INFO: &#x9;Container pod-with-prestop-http-hook ready: false, restart count 0&#xA;May 29 21:39:05.072: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 21:39:05.072: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;container-lifecycle-hook-4466&#34; for this suite.&#xA;May 29 21:49:05.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 21:49:06.420: INFO: namespace: container-lifecycle-hook-4466, resource: pods, items remaining: 1&#xA;May 29 21:49:06.960: INFO: namespace: container-lifecycle-hook-4466, DeletionTimetamp: 2019-05-29 21:39:05 -0700 PDT, Finalizers: [kubernetes], Phase: Terminating&#xA;May 29 21:49:07.002: INFO: namespace: container-lifecycle-hook-4466, total namespaces: 5, active: 4, terminating: 1&#xA;May 29 21:49:07.044: INFO: POD                         NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 21:49:07.044: INFO: pod-with-prestop-http-hook  e2e-test-peterhornyack-windows-node-group-jpxd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:33:59 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:39:03 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:39:03 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:33:59 -0700 PDT  }]&#xA;May 29 21:49:07.044: INFO: &#xA;May 29 21:49:07.044: INFO: Couldn&#39;t delete ns: &#34;container-lifecycle-hook-4466&#34;: namespace container-lifecycle-hook-4466 was not deleted with limit: timed out waiting for the condition, pods remaining: 1 (&amp;errors.errorString{s:&#34;namespace container-lifecycle-hook-4466 was not deleted with limit: timed out waiting for the condition, pods remaining: 1&#34;})&#xA;</system-out>
      </testcase>
      <testcase name="[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should work for type=LoadBalancer" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should have session affinity work for LoadBalancer service with ESIPP on [Slow] [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]" classname="Kubernetes e2e suite" time="23.303362656"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.001405844">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.001513975">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-windows] Windows volume mounts  check volume mount permissions container should have readOnly permissions on hostMapPath" classname="Kubernetes e2e suite" time="1063.876999056">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;May 29 22:07:14.229: Couldn&#39;t delete ns: &#34;windows-volumes-1832&#34;: namespace windows-volumes-1832 was not deleted with limit: timed out waiting for the condition, pods remaining: 2 (&amp;errors.errorString{s:&#34;namespace windows-volumes-1832 was not deleted with limit: timed out waiting for the condition, pods remaining: 2&#34;})&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:334</failure>
          <system-out>[BeforeEach] [sig-windows] Windows volume mounts &#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/windows/framework.go:28&#xA;[BeforeEach] [sig-windows] Windows volume mounts &#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 21:49:30.353: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename windows-volumes&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-windows] Windows volume mounts &#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/windows/volumes.go:58&#xA;[It] container should have readOnly permissions on hostMapPath&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/windows/volumes.go:73&#xA;�[1mSTEP�[0m: creating a container with readOnly permissions on hostMap volume&#xA;May 29 21:53:08.723: INFO: ExecWithOptions {Command:[cmd /c echo windows-volume-test &gt; C:\tmp\test-file.txt] Namespace:windows-volumes-1832 PodName:pod-3da0c3ef-918e-4a31-9b3e-a46b4f77233d ContainerName:test-container Stdin:&lt;nil&gt; CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}&#xA;May 29 21:53:08.723: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: creating two containers, one with readOnly permissions the other with read-write permissions on hostMap volume&#xA;May 29 21:57:11.243: INFO: ExecWithOptions {Command:[cmd /c echo windows-volume-test &gt; C:\tmp\test-file] Namespace:windows-volumes-1832 PodName:pod-751355a7-ab47-4202-ab51-164f6dc0c639 ContainerName:test-container-rw Stdin:&lt;nil&gt; CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}&#xA;May 29 21:57:11.243: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;May 29 21:57:11.600: INFO: ExecWithOptions {Command:[cmd /c echo windows-volume-test &gt; C:\tmp\test-file] Namespace:windows-volumes-1832 PodName:pod-751355a7-ab47-4202-ab51-164f6dc0c639 ContainerName:test-container Stdin:&lt;nil&gt; CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}&#xA;May 29 21:57:11.600: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;May 29 21:57:11.947: INFO: ExecWithOptions {Command:[cmd /c type C:\tmp\test-file] Namespace:windows-volumes-1832 PodName:pod-751355a7-ab47-4202-ab51-164f6dc0c639 ContainerName:test-container Stdin:&lt;nil&gt; CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}&#xA;May 29 21:57:11.947: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;[AfterEach] [sig-windows] Windows volume mounts &#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;May 29 21:57:12.288: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;May 29 21:57:12.331: INFO: Condition Ready of node e2e-test-peterhornyack-windows-node-group-jpxd is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule 2019-05-29 21:56:09 -0700 PDT} {node.kubernetes.io/not-ready  NoExecute 2019-05-29 21:56:12 -0700 PDT}]. Failure&#xA;�[1mSTEP�[0m: Destroying namespace &#34;windows-volumes-1832&#34; for this suite.&#xA;May 29 22:07:12.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 22:07:13.050: INFO: namespace: windows-volumes-1832, resource: pods, items remaining: 2&#xA;May 29 22:07:14.145: INFO: namespace: windows-volumes-1832, DeletionTimetamp: 2019-05-29 21:57:12 -0700 PDT, Finalizers: [kubernetes], Phase: Terminating&#xA;May 29 22:07:14.187: INFO: namespace: windows-volumes-1832, total namespaces: 5, active: 4, terminating: 1&#xA;May 29 22:07:14.229: INFO: POD                                       NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 22:07:14.229: INFO: pod-3da0c3ef-918e-4a31-9b3e-a46b4f77233d  e2e-test-peterhornyack-windows-node-group-jpxd  Failed   30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:49:30 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:07:12 -0700 PDT ContainersNotReady containers with unready status: [test-container]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:07:12 -0700 PDT ContainersNotReady containers with unready status: [test-container]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:49:30 -0700 PDT  }]&#xA;May 29 22:07:14.229: INFO: pod-751355a7-ab47-4202-ab51-164f6dc0c639  e2e-test-peterhornyack-windows-node-group-jpxd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:53:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:57:09 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:57:09 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:53:09 -0700 PDT  }]&#xA;May 29 22:07:14.229: INFO: &#xA;May 29 22:07:14.229: INFO: Couldn&#39;t delete ns: &#34;windows-volumes-1832&#34;: namespace windows-volumes-1832 was not deleted with limit: timed out waiting for the condition, pods remaining: 2 (&amp;errors.errorString{s:&#34;namespace windows-volumes-1832 was not deleted with limit: timed out waiting for the condition, pods remaining: 2&#34;})&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (cpu, memory quota set) against a pod with same priority class." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Downward API [Serial] [Disruptive] [NodeFeature:EphemeralStorage] Downward API tests for local ephemeral storage should provide container&#39;s limits.ephemeral-storage and requests.ephemeral-storage as env vars" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere should test that a file written to the vspehre volume mount before kubelet restart can be read after restart [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] HA-master [Feature:HAMaster] survive addition/removal replicas different zones [Serial][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl alpha client [k8s.io] Kubectl run CronJob should create a CronJob" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="16.382483133"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should be able to deny attaching pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull image from invalid registry [NodeConformance]" classname="Kubernetes e2e suite" time="11.252407817"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  StatefulSet with pod affinity [Slow] should use volumes spread across nodes when pod management is parallel and pod has anti-affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.00198583">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run CronJob should create a CronJob" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] ReplicationController light Should scale from 1 pod to 2 pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should not scale GPU pool up if pod does not require GPUs [GpuType:] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking should check kube-proxy urls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001778233">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] ReplicationController Should scale from 5 pods to 3 pods and from 3 to 1 and verify decision stability" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 Scalability GCE [Slow] [Serial] [Feature:IngressScale] Creating and updating ingresses should happen promptly with small/medium/large amount of ingresses" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]" classname="Kubernetes e2e suite" time="8.985403833"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="161.288034172"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement should create and delete pod with multiple volumes from same datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic Snapshot] snapshottable should create snapshot with defaults [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pending pods are small [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when not explicitly set and uid != 0 [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with invalid diskStripes value is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify PVC creation with incompatible storage policy along with compatible zone and datastore combination specified in storage class fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.001204559">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.00125084">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] PodSecurityPolicy should forbid pod creation when no PSP is available" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001191032">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node ReplicationController with 0 secrets, 0 configmaps and 0 daemons with quotas" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Update Demo should scale a replication controller  [Conformance]" classname="Kubernetes e2e suite" time="1076.663084384">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;May 29 22:18:21.956: Timed out after 300 seconds waiting for name=update-demo pods to reach valid state&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/rc_util.go:257</failure>
          <system-out>[BeforeEach] [sig-cli] Kubectl client&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 22:10:32.150: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename kubectl&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-cli] Kubectl client&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214&#xA;[BeforeEach] [k8s.io] Update Demo&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266&#xA;[It] should scale a replication controller  [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: creating a replication controller&#xA;May 29 22:10:32.321: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config create -f - --namespace=kubectl-472&#39;&#xA;May 29 22:10:34.651: INFO: stderr: &#34;&#34;&#xA;May 29 22:10:34.651: INFO: stdout: &#34;replicationcontroller/update-demo-nautilus created\n&#34;&#xA;�[1mSTEP�[0m: waiting for all containers in name=update-demo pods to come up.&#xA;May 29 22:10:34.651: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:10:34.927: INFO: stderr: &#34;&#34;&#xA;May 29 22:10:34.927: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:10:34.927: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:10:35.185: INFO: stderr: &#34;&#34;&#xA;May 29 22:10:35.185: INFO: stdout: &#34;&#34;&#xA;May 29 22:10:35.185: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:10:40.186: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:10:40.456: INFO: stderr: &#34;&#34;&#xA;May 29 22:10:40.456: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:10:40.456: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:10:40.721: INFO: stderr: &#34;&#34;&#xA;May 29 22:10:40.721: INFO: stdout: &#34;&#34;&#xA;May 29 22:10:40.721: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:10:45.722: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:10:45.983: INFO: stderr: &#34;&#34;&#xA;May 29 22:10:45.983: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:10:45.983: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:10:46.218: INFO: stderr: &#34;&#34;&#xA;May 29 22:10:46.218: INFO: stdout: &#34;&#34;&#xA;May 29 22:10:46.218: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:10:51.218: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:10:51.477: INFO: stderr: &#34;&#34;&#xA;May 29 22:10:51.477: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:10:51.477: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:10:51.727: INFO: stderr: &#34;&#34;&#xA;May 29 22:10:51.727: INFO: stdout: &#34;&#34;&#xA;May 29 22:10:51.727: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:10:56.728: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:10:56.986: INFO: stderr: &#34;&#34;&#xA;May 29 22:10:56.986: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:10:56.986: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:10:57.237: INFO: stderr: &#34;&#34;&#xA;May 29 22:10:57.237: INFO: stdout: &#34;&#34;&#xA;May 29 22:10:57.237: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:11:02.237: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:11:02.486: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:02.486: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:11:02.486: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:11:02.737: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:02.737: INFO: stdout: &#34;&#34;&#xA;May 29 22:11:02.737: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:11:07.738: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:11:07.990: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:07.990: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:11:07.990: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:11:08.229: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:08.229: INFO: stdout: &#34;&#34;&#xA;May 29 22:11:08.229: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:11:13.229: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:11:13.490: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:13.490: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:11:13.491: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:11:13.741: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:13.741: INFO: stdout: &#34;&#34;&#xA;May 29 22:11:13.741: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:11:18.741: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:11:19.010: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:19.010: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:11:19.010: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:11:19.255: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:19.255: INFO: stdout: &#34;&#34;&#xA;May 29 22:11:19.255: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:11:24.256: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:11:24.516: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:24.516: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:11:24.516: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:11:24.766: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:24.766: INFO: stdout: &#34;&#34;&#xA;May 29 22:11:24.767: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:11:29.767: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:11:30.031: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:30.031: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:11:30.031: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:11:30.260: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:30.260: INFO: stdout: &#34;&#34;&#xA;May 29 22:11:30.260: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:11:35.261: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:11:35.524: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:35.524: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:11:35.525: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:11:35.781: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:35.781: INFO: stdout: &#34;&#34;&#xA;May 29 22:11:35.781: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:11:40.781: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:11:41.041: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:41.041: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:11:41.041: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:11:41.277: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:41.277: INFO: stdout: &#34;&#34;&#xA;May 29 22:11:41.277: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:11:46.278: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:11:46.546: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:46.546: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:11:46.547: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:11:46.810: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:46.810: INFO: stdout: &#34;&#34;&#xA;May 29 22:11:46.810: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:11:51.811: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:11:52.066: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:52.066: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:11:52.067: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:11:52.319: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:52.319: INFO: stdout: &#34;&#34;&#xA;May 29 22:11:52.319: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:11:57.320: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:11:57.586: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:57.586: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:11:57.586: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:11:57.841: INFO: stderr: &#34;&#34;&#xA;May 29 22:11:57.841: INFO: stdout: &#34;&#34;&#xA;May 29 22:11:57.841: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:12:02.841: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:12:03.096: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:03.096: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:12:03.096: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:12:03.348: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:03.348: INFO: stdout: &#34;&#34;&#xA;May 29 22:12:03.348: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:12:08.348: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:12:08.618: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:08.618: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:12:08.618: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:12:08.863: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:08.863: INFO: stdout: &#34;&#34;&#xA;May 29 22:12:08.863: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:12:13.863: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:12:14.121: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:14.121: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:12:14.121: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:12:14.376: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:14.376: INFO: stdout: &#34;&#34;&#xA;May 29 22:12:14.376: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:12:19.377: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:12:19.641: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:19.641: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:12:19.641: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:12:19.902: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:19.902: INFO: stdout: &#34;&#34;&#xA;May 29 22:12:19.902: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:12:24.902: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:12:25.168: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:25.168: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:12:25.168: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:12:25.419: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:25.419: INFO: stdout: &#34;&#34;&#xA;May 29 22:12:25.419: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:12:30.419: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:12:30.687: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:30.687: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:12:30.687: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:12:30.944: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:30.944: INFO: stdout: &#34;&#34;&#xA;May 29 22:12:30.944: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:12:35.944: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:12:36.211: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:36.211: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:12:36.211: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:12:36.460: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:36.460: INFO: stdout: &#34;&#34;&#xA;May 29 22:12:36.460: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:12:41.461: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:12:41.717: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:41.717: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:12:41.718: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:12:41.960: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:41.960: INFO: stdout: &#34;&#34;&#xA;May 29 22:12:41.960: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:12:46.960: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:12:47.221: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:47.221: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:12:47.222: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:12:47.472: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:47.472: INFO: stdout: &#34;&#34;&#xA;May 29 22:12:47.472: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:12:52.473: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:12:52.740: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:52.740: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:12:52.740: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:12:53.001: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:53.001: INFO: stdout: &#34;&#34;&#xA;May 29 22:12:53.001: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:12:58.002: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:12:58.259: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:58.259: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:12:58.260: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:12:58.513: INFO: stderr: &#34;&#34;&#xA;May 29 22:12:58.513: INFO: stdout: &#34;&#34;&#xA;May 29 22:12:58.513: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:13:03.513: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:13:03.766: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:03.766: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:13:03.766: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:13:04.015: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:04.015: INFO: stdout: &#34;&#34;&#xA;May 29 22:13:04.015: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:13:09.016: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:13:09.274: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:09.274: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:13:09.274: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:13:09.539: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:09.540: INFO: stdout: &#34;&#34;&#xA;May 29 22:13:09.540: INFO: update-demo-nautilus-5pqcg is created but not running&#xA;May 29 22:13:14.540: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:13:14.806: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:14.806: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;May 29 22:13:14.806: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:13:15.059: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:15.059: INFO: stdout: &#34;true&#34;&#xA;May 29 22:13:15.059: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-5pqcg -o template --template={{if (exists . &#34;spec&#34; &#34;containers&#34;)}}{{range .spec.containers}}{{if eq .name &#34;update-demo&#34;}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:13:15.326: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:15.326: INFO: stdout: &#34;e2eteam/nautilus:1.0&#34;&#xA;May 29 22:13:15.326: INFO: validating pod update-demo-nautilus-5pqcg&#xA;May 29 22:13:15.482: INFO: got data: {&#xD;&#xA;  &#34;image&#34;: &#34;nautilus.jpg&#34;&#xD;&#xA;}&#xD;&#xA;&#xA;May 29 22:13:15.483: INFO: Unmarshalled json jpg/img =&gt; {nautilus.jpg} , expecting nautilus.jpg .&#xA;May 29 22:13:15.483: INFO: update-demo-nautilus-5pqcg is verified up and running&#xA;May 29 22:13:15.483: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-mfv4h -o template --template={{if (exists . &#34;status&#34; &#34;containerStatuses&#34;)}}{{range .status.containerStatuses}}{{if (and (eq .name &#34;update-demo&#34;) (exists . &#34;state&#34; &#34;running&#34;))}}true{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:13:15.744: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:15.744: INFO: stdout: &#34;true&#34;&#xA;May 29 22:13:15.745: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods update-demo-nautilus-mfv4h -o template --template={{if (exists . &#34;spec&#34; &#34;containers&#34;)}}{{range .spec.containers}}{{if eq .name &#34;update-demo&#34;}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-472&#39;&#xA;May 29 22:13:15.996: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:15.996: INFO: stdout: &#34;e2eteam/nautilus:1.0&#34;&#xA;May 29 22:13:15.996: INFO: validating pod update-demo-nautilus-mfv4h&#xA;May 29 22:13:16.093: INFO: got data: {&#xD;&#xA;  &#34;image&#34;: &#34;nautilus.jpg&#34;&#xD;&#xA;}&#xD;&#xA;&#xA;May 29 22:13:16.093: INFO: Unmarshalled json jpg/img =&gt; {nautilus.jpg} , expecting nautilus.jpg .&#xA;May 29 22:13:16.093: INFO: update-demo-nautilus-mfv4h is verified up and running&#xA;�[1mSTEP�[0m: scaling down the replication controller&#xA;May 29 22:13:16.338: INFO: scanned /usr/local/google/home/peterhornyack for discovery docs: &lt;nil&gt;&#xA;May 29 22:13:16.338: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-472&#39;&#xA;May 29 22:13:16.751: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:16.751: INFO: stdout: &#34;replicationcontroller/update-demo-nautilus scaled\n&#34;&#xA;�[1mSTEP�[0m: waiting for all containers in name=update-demo pods to come up.&#xA;May 29 22:13:16.751: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:13:17.015: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:17.015: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:13:22.016: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:13:22.275: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:22.275: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:13:27.276: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:13:27.533: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:27.533: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:13:32.533: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:13:32.794: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:32.794: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:13:37.794: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:13:38.049: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:38.049: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:13:43.049: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:13:43.322: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:43.322: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:13:48.323: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:13:48.579: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:48.579: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:13:53.580: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:13:53.847: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:53.847: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:13:58.847: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:13:59.112: INFO: stderr: &#34;&#34;&#xA;May 29 22:13:59.112: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:14:04.112: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:14:04.393: INFO: stderr: &#34;&#34;&#xA;May 29 22:14:04.393: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:14:09.394: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:14:09.653: INFO: stderr: &#34;&#34;&#xA;May 29 22:14:09.653: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:14:14.654: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:14:14.934: INFO: stderr: &#34;&#34;&#xA;May 29 22:14:14.935: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:14:19.935: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:14:20.193: INFO: stderr: &#34;&#34;&#xA;May 29 22:14:20.193: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:14:25.193: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:14:25.490: INFO: stderr: &#34;&#34;&#xA;May 29 22:14:25.490: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:14:30.490: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:14:30.744: INFO: stderr: &#34;&#34;&#xA;May 29 22:14:30.744: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:14:35.745: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:14:36.007: INFO: stderr: &#34;&#34;&#xA;May 29 22:14:36.007: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:14:41.007: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:14:41.263: INFO: stderr: &#34;&#34;&#xA;May 29 22:14:41.263: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:14:46.263: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:14:46.530: INFO: stderr: &#34;&#34;&#xA;May 29 22:14:46.530: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:14:51.531: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:14:51.795: INFO: stderr: &#34;&#34;&#xA;May 29 22:14:51.795: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:14:56.795: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:14:57.061: INFO: stderr: &#34;&#34;&#xA;May 29 22:14:57.061: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:15:02.061: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:15:02.321: INFO: stderr: &#34;&#34;&#xA;May 29 22:15:02.321: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:15:07.322: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:15:07.568: INFO: stderr: &#34;&#34;&#xA;May 29 22:15:07.568: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:15:12.569: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:15:12.825: INFO: stderr: &#34;&#34;&#xA;May 29 22:15:12.825: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:15:17.826: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:15:18.081: INFO: stderr: &#34;&#34;&#xA;May 29 22:15:18.081: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:15:23.081: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:15:23.347: INFO: stderr: &#34;&#34;&#xA;May 29 22:15:23.347: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:15:28.348: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:15:28.591: INFO: stderr: &#34;&#34;&#xA;May 29 22:15:28.591: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:15:33.591: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:15:33.845: INFO: stderr: &#34;&#34;&#xA;May 29 22:15:33.845: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:15:38.846: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:15:39.105: INFO: stderr: &#34;&#34;&#xA;May 29 22:15:39.105: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:15:44.105: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:15:44.368: INFO: stderr: &#34;&#34;&#xA;May 29 22:15:44.368: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:15:49.372: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:15:49.616: INFO: stderr: &#34;&#34;&#xA;May 29 22:15:49.616: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:15:54.616: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:15:54.874: INFO: stderr: &#34;&#34;&#xA;May 29 22:15:54.874: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:15:59.875: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:16:00.137: INFO: stderr: &#34;&#34;&#xA;May 29 22:16:00.137: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:16:05.138: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:16:05.391: INFO: stderr: &#34;&#34;&#xA;May 29 22:16:05.391: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:16:10.391: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:16:10.658: INFO: stderr: &#34;&#34;&#xA;May 29 22:16:10.658: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:16:15.658: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:16:15.930: INFO: stderr: &#34;&#34;&#xA;May 29 22:16:15.930: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:16:20.931: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:16:21.212: INFO: stderr: &#34;&#34;&#xA;May 29 22:16:21.212: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:16:26.213: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:16:26.478: INFO: stderr: &#34;&#34;&#xA;May 29 22:16:26.478: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:16:31.479: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:16:31.732: INFO: stderr: &#34;&#34;&#xA;May 29 22:16:31.732: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:16:36.733: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:16:36.995: INFO: stderr: &#34;&#34;&#xA;May 29 22:16:36.995: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:16:41.996: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:16:42.268: INFO: stderr: &#34;&#34;&#xA;May 29 22:16:42.268: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:16:47.269: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:16:47.520: INFO: stderr: &#34;&#34;&#xA;May 29 22:16:47.520: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:16:52.521: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:16:52.775: INFO: stderr: &#34;&#34;&#xA;May 29 22:16:52.776: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:16:57.776: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:16:58.035: INFO: stderr: &#34;&#34;&#xA;May 29 22:16:58.035: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:17:03.035: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:17:03.307: INFO: stderr: &#34;&#34;&#xA;May 29 22:17:03.307: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:17:08.307: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:17:08.562: INFO: stderr: &#34;&#34;&#xA;May 29 22:17:08.562: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:17:13.562: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:17:13.827: INFO: stderr: &#34;&#34;&#xA;May 29 22:17:13.827: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:17:18.827: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:17:19.099: INFO: stderr: &#34;&#34;&#xA;May 29 22:17:19.099: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:17:24.099: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:17:24.354: INFO: stderr: &#34;&#34;&#xA;May 29 22:17:24.354: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:17:29.355: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:17:29.617: INFO: stderr: &#34;&#34;&#xA;May 29 22:17:29.617: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:17:34.617: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:17:34.876: INFO: stderr: &#34;&#34;&#xA;May 29 22:17:34.876: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:17:39.876: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:17:40.133: INFO: stderr: &#34;&#34;&#xA;May 29 22:17:40.133: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:17:45.134: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:17:45.396: INFO: stderr: &#34;&#34;&#xA;May 29 22:17:45.396: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:17:50.397: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:17:50.655: INFO: stderr: &#34;&#34;&#xA;May 29 22:17:50.655: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:17:55.655: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:17:55.930: INFO: stderr: &#34;&#34;&#xA;May 29 22:17:55.930: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:18:00.930: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:18:01.180: INFO: stderr: &#34;&#34;&#xA;May 29 22:18:01.180: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:18:06.181: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:18:06.439: INFO: stderr: &#34;&#34;&#xA;May 29 22:18:06.439: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:18:11.439: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:18:11.697: INFO: stderr: &#34;&#34;&#xA;May 29 22:18:11.697: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:18:16.697: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-472&#39;&#xA;May 29 22:18:16.956: INFO: stderr: &#34;&#34;&#xA;May 29 22:18:16.956: INFO: stdout: &#34;update-demo-nautilus-5pqcg update-demo-nautilus-mfv4h &#34;&#xA;�[1mSTEP�[0m: Replicas for name=update-demo: expected=1 actual=2&#xA;May 29 22:18:21.956: INFO: Timed out after 300 seconds waiting for name=update-demo pods to reach valid state&#xA;�[1mSTEP�[0m: using delete to clean up resources&#xA;May 29 22:18:21.958: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-472&#39;&#xA;May 29 22:18:22.265: INFO: stderr: &#34;warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n&#34;&#xA;May 29 22:18:22.265: INFO: stdout: &#34;replicationcontroller \&#34;update-demo-nautilus\&#34; force deleted\n&#34;&#xA;May 29 22:18:22.265: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-472&#39;&#xA;May 29 22:18:22.564: INFO: stderr: &#34;No resources found.\n&#34;&#xA;May 29 22:18:22.564: INFO: stdout: &#34;&#34;&#xA;May 29 22:18:22.564: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config get pods -l name=update-demo --namespace=kubectl-472 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ &#34;\n&#34; }}{{ end }}{{ end }}&#39;&#xA;May 29 22:18:22.825: INFO: stderr: &#34;&#34;&#xA;May 29 22:18:22.825: INFO: stdout: &#34;&#34;&#xA;[AfterEach] [sig-cli] Kubectl client&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;kubectl-472&#34;.&#xA;�[1mSTEP�[0m: Found 13 events.&#xA;May 29 22:18:22.868: INFO: At 2019-05-29 22:10:34 -0700 PDT - event for update-demo-nautilus: {replication-controller } SuccessfulCreate: Created pod: update-demo-nautilus-5pqcg&#xA;May 29 22:18:22.868: INFO: At 2019-05-29 22:10:34 -0700 PDT - event for update-demo-nautilus: {replication-controller } SuccessfulCreate: Created pod: update-demo-nautilus-mfv4h&#xA;May 29 22:18:22.868: INFO: At 2019-05-29 22:10:34 -0700 PDT - event for update-demo-nautilus-5pqcg: {default-scheduler } Scheduled: Successfully assigned kubectl-472/update-demo-nautilus-5pqcg to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:18:22.868: INFO: At 2019-05-29 22:10:34 -0700 PDT - event for update-demo-nautilus-mfv4h: {default-scheduler } Scheduled: Successfully assigned kubectl-472/update-demo-nautilus-mfv4h to e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:18:22.868: INFO: At 2019-05-29 22:10:36 -0700 PDT - event for update-demo-nautilus-mfv4h: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Created: Created container update-demo&#xA;May 29 22:18:22.868: INFO: At 2019-05-29 22:10:36 -0700 PDT - event for update-demo-nautilus-mfv4h: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Pulled: Container image &#34;e2eteam/nautilus:1.0&#34; already present on machine&#xA;May 29 22:18:22.868: INFO: At 2019-05-29 22:10:37 -0700 PDT - event for update-demo-nautilus-5pqcg: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container update-demo&#xA;May 29 22:18:22.868: INFO: At 2019-05-29 22:10:37 -0700 PDT - event for update-demo-nautilus-5pqcg: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/nautilus:1.0&#34; already present on machine&#xA;May 29 22:18:22.868: INFO: At 2019-05-29 22:10:38 -0700 PDT - event for update-demo-nautilus-mfv4h: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Started: Started container update-demo&#xA;May 29 22:18:22.868: INFO: At 2019-05-29 22:10:39 -0700 PDT - event for update-demo-nautilus-5pqcg: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container update-demo&#xA;May 29 22:18:22.868: INFO: At 2019-05-29 22:13:16 -0700 PDT - event for update-demo-nautilus: {replication-controller } SuccessfulDelete: Deleted pod: update-demo-nautilus-5pqcg&#xA;May 29 22:18:22.868: INFO: At 2019-05-29 22:13:22 -0700 PDT - event for update-demo-nautilus-5pqcg: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod kubectl-472/update-demo-nautilus-5pqcg&#xA;May 29 22:18:22.868: INFO: At 2019-05-29 22:18:22 -0700 PDT - event for update-demo-nautilus-mfv4h: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Killing: Stopping container update-demo&#xA;May 29 22:18:22.958: INFO: POD                                                    NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 22:18:22.958: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 29 22:18:22.958: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:18:22.959: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:18:22.959: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:18:22.959: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:18:22.959: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:18:22.959: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:18:22.959: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 29 22:18:22.959: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 29 22:18:22.959: INFO: update-demo-nautilus-5pqcg                             e2e-test-peterhornyack-windows-node-group-jpxd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:10:34 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:13:14 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:13:14 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:10:34 -0700 PDT  }]&#xA;May 29 22:18:22.959: INFO: update-demo-nautilus-mfv4h                             e2e-test-peterhornyack-windows-node-group-9q9v  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:10:34 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:10:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:10:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:10:34 -0700 PDT  }]&#xA;May 29 22:18:22.959: INFO: pod-751355a7-ab47-4202-ab51-164f6dc0c639               e2e-test-peterhornyack-windows-node-group-jpxd  Failed   30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:53:09 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:13:14 -0700 PDT ContainersNotReady containers with unready status: [test-container test-container-rw]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:13:14 -0700 PDT ContainersNotReady containers with unready status: [test-container test-container-rw]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 21:53:09 -0700 PDT  }]&#xA;May 29 22:18:22.959: INFO: &#xA;May 29 22:18:23.001: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 29 22:18:23.043: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:58564,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:17:56 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:17:56 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:17:56 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:17:56 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 22:18:23.044: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 29 22:18:23.085: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 29 22:18:23.132: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:18:23.132: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:18:23.132: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:18:23.132: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:18:23.132: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:18:23.132: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:18:23.132: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:18:23.132: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 22:18:23.132: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:18:23.132: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:18:23.132: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:18:23.132: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:18:23.132: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 22:18:23.132: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:18:23.302: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 29 22:18:23.302: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:18:23.344: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:58591,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentContainerdRestart False 2019-05-29 22:17:40 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 22:17:40 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-29 22:17:40 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 22:17:40 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-29 22:17:40 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-29 22:17:40 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 22:17:40 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:18:09 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:18:09 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:18:09 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:18:09 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 22:18:23.345: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:18:23.386: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:18:23.932: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:18:23.933: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 29 22:18:23.933: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:18:23.933: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 29 22:18:23.933: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:18:23.933: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:18:23.933: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 29 22:18:23.933: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:18:23.933: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 22:18:23.933: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:18:23.933: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:18:23.933: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:18:23.933: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 22:18:23.933: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:18:23.933: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 29 22:18:23.933: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:18:23.933: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 29 22:18:23.933: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:18:23.933: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 22:18:23.933: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:18:24.081: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:18:24.081: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:18:24.122: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:58528,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{ReadonlyFilesystem False 2019-05-29 22:17:35 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-29 22:17:35 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 22:17:35 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 22:17:35 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 22:17:35 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-29 22:17:35 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-29 22:17:35 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:17:42 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:17:42 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:17:42 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:17:42 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 22:18:24.122: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:18:24.164: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:18:24.215: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:18:24.215: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 22:18:24.215: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:18:24.215: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:18:24.215: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 22:18:24.215: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:18:24.215: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 29 22:18:24.215: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 29 22:18:24.215: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:18:24.215: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 29 22:18:24.215: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 29 22:18:24.215: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:18:24.215: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:18:24.215: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 22:18:24.215: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:18:24.381: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:18:24.381: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:18:24.422: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:58610,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:18:17 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:18:17 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:18:17 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:18:17 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 22:18:24.422: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:18:24.463: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:18:24.666: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:18:24.666: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:18:24.708: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:58519,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:17:37 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:17:37 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:17:37 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:17:37 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 22:18:24.709: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:18:24.750: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:18:24.796: INFO: update-demo-nautilus-mfv4h started at 2019-05-29 22:10:34 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:18:24.796: INFO: &#x9;Container update-demo ready: false, restart count 0&#xA;May 29 22:18:24.968: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:18:24.968: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:18:25.009: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:58617,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:18:21 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:18:21 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:18:21 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:18:21 -0700 PDT 2019-05-29 22:17:21 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 22:18:25.009: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:18:25.050: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:18:25.100: INFO: update-demo-nautilus-5pqcg started at 2019-05-29 22:10:34 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:18:25.100: INFO: &#x9;Container update-demo ready: true, restart count 0&#xA;May 29 22:18:25.100: INFO: pod-751355a7-ab47-4202-ab51-164f6dc0c639 started at 2019-05-29 21:53:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:18:25.100: INFO: &#x9;Container test-container ready: false, restart count 0&#xA;May 29 22:18:25.100: INFO: &#x9;Container test-container-rw ready: false, restart count 0&#xA;May 29 22:18:26.886: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:18:26.886: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;kubectl-472&#34; for this suite.&#xA;May 29 22:28:27.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 22:28:27.729: INFO: namespace: kubectl-472, resource: pods, items remaining: 1&#xA;May 29 22:28:28.729: INFO: namespace: kubectl-472, DeletionTimetamp: 2019-05-29 22:18:26 -0700 PDT, Finalizers: [kubernetes], Phase: Terminating&#xA;May 29 22:28:28.770: INFO: namespace: kubectl-472, total namespaces: 5, active: 4, terminating: 1&#xA;May 29 22:28:28.811: INFO: POD                         NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 22:28:28.811: INFO: update-demo-nautilus-5pqcg  e2e-test-peterhornyack-windows-node-group-jpxd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:10:34 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:13:14 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:13:14 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:10:34 -0700 PDT  }]&#xA;May 29 22:28:28.811: INFO: &#xA;May 29 22:28:28.811: INFO: Couldn&#39;t delete ns: &#34;kubectl-472&#34;: namespace kubectl-472 was not deleted with limit: timed out waiting for the condition, pods remaining: 1 (&amp;errors.errorString{s:&#34;namespace kubectl-472 was not deleted with limit: timed out waiting for the condition, pods remaining: 1&#34;})&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Reboot [Disruptive] [Feature:Reboot] each node by switching off the network interface and ensure they function upon switch on" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver [Feature:Networking-IPv6] Forward external name lookup should forward externalname lookup to upstream nameserver [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should provision storage with different parameters" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should support allow-all policy [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that&#39;s waiting for dependents to be deleted [Conformance]" classname="Kubernetes e2e suite" time="18.686140022"></testcase>
      <testcase name="[sig-network] DNS should provide DNS for services  [Conformance]" classname="Kubernetes e2e suite" time="43.325033955"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [ReplicationController] should recreate pods scheduled on the unreachable node AND allow scheduling of pods on a node after it rejoins the cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] kube-proxy migration [Feature:KubeProxyDaemonSetMigration] Downgrade kube-proxy from a DaemonSet to static pods should maintain a functioning cluster [Feature:KubeProxyDaemonSetDowngrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]" classname="Kubernetes e2e suite" time="35.307169344"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates MaxPods limit number of pods that are allowed to run [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with Custom Metric of type Pod from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify PVC creation with invalid zone specified in storage class fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting an existing configmap should exit with the Forbidden error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap Should fail non-optional pod creation due to the key in the configMap object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] host cleanup with volume mounts [sig-storage][HostCleanup][Flaky] Host cleanup after disrupting NFS volume [NFS] after stopping the nfs-server and deleting the (active) client pod, the NFS mount and the pod&#39;s UID directory should be removed." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should resolve DNS of partial qualified names for the cluster " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes GCEPD should test that deleting a PVC before the pod does not cause pod deletion to fail on PD detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against 2 pods with same priority class." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] ReplicationController light Should scale from 2 pods to 1 pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should provision storage in the allowedTopologies [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Disk Format [Feature:vsphere] verify disk format type - eagerzeroedthick is honored for dynamically provisioned pv using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should implement service.kubernetes.io/service-proxy-name" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver [Feature:Networking-IPv6] Change stubDomain should be able to change stubDomain configuration [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:Performance] should be able to handle 30 pods per node ReplicationController with 0 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.002213385">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should work after restarting apiserver [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t increase cluster size if pending pod is too large [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should support exec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]" classname="Kubernetes e2e suite" time="8.269590276"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.002415931">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001739217">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="906.330621825">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0002b5440&gt;: {&#xA;        s: &#34;timed out waiting for the condition&#34;,&#xA;    }&#xA;    timed out waiting for the condition&#xA;occurred&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/pods.go:112</failure>
          <system-out>[BeforeEach] [k8s.io] Pods&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 22:30:14.410: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename pods&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [k8s.io] Pods&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135&#xA;[It] should support remote command execution over websockets [NodeConformance] [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;May 29 22:30:14.611: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: creating the pod&#xA;�[1mSTEP�[0m: submitting the pod to kubernetes&#xA;May 29 22:35:14.785: INFO: Unexpected error occurred: timed out waiting for the condition&#xA;[AfterEach] [k8s.io] Pods&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;pods-4732&#34;.&#xA;�[1mSTEP�[0m: Found 5 events.&#xA;May 29 22:35:14.828: INFO: At 2019-05-29 22:30:14 -0700 PDT - event for pod-exec-websocket-5ee920d5-eefa-4887-b5e0-09fc5939c69d: {default-scheduler } Scheduled: Successfully assigned pods-4732/pod-exec-websocket-5ee920d5-eefa-4887-b5e0-09fc5939c69d to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:35:14.828: INFO: At 2019-05-29 22:30:17 -0700 PDT - event for pod-exec-websocket-5ee920d5-eefa-4887-b5e0-09fc5939c69d: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/busybox:1.29&#34; already present on machine&#xA;May 29 22:35:14.828: INFO: At 2019-05-29 22:30:17 -0700 PDT - event for pod-exec-websocket-5ee920d5-eefa-4887-b5e0-09fc5939c69d: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container main&#xA;May 29 22:35:14.828: INFO: At 2019-05-29 22:30:19 -0700 PDT - event for pod-exec-websocket-5ee920d5-eefa-4887-b5e0-09fc5939c69d: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container main&#xA;May 29 22:35:14.828: INFO: At 2019-05-29 22:33:27 -0700 PDT - event for pod-exec-websocket-5ee920d5-eefa-4887-b5e0-09fc5939c69d: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod pods-4732/pod-exec-websocket-5ee920d5-eefa-4887-b5e0-09fc5939c69d&#xA;May 29 22:35:14.918: INFO: POD                                                      NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 22:35:14.918: INFO: coredns-5b969f4c88-gsjpw                                 e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: coredns-5b969f4c88-mvhtd                                 e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master     e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: etcd-server-e2e-test-peterhornyack-master                e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: etcd-server-events-e2e-test-peterhornyack-master         e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                   e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                      e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: fluentd-gcp-v3.2.0-fr5zq                                 e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: fluentd-gcp-v3.2.0-r5s9z                                 e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: fluentd-gcp-v3.2.0-wp9vf                                 e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                  e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: kube-addon-manager-e2e-test-peterhornyack-master         e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: kube-apiserver-e2e-test-peterhornyack-master             e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: kube-controller-manager-e2e-test-peterhornyack-master    e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: kube-dns-autoscaler-97df449df-7v474                      e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh      e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6      e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: kube-scheduler-e2e-test-peterhornyack-master             e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: l7-default-backend-8f479dd9-hnbtn                        e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master    e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: metadata-proxy-v0.1-8mhrb                                e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: metadata-proxy-v0.1-gqcgn                                e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: metadata-proxy-v0.1-w99mm                                e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                   e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: update-demo-nautilus-5pqcg                               e2e-test-peterhornyack-windows-node-group-jpxd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:10:34 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:29:19 -0700 PDT ContainersNotReady containers with unready status: [update-demo]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:29:19 -0700 PDT ContainersNotReady containers with unready status: [update-demo]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:10:34 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: pod-exec-websocket-5ee920d5-eefa-4887-b5e0-09fc5939c69d  e2e-test-peterhornyack-windows-node-group-jpxd  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:30:14 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:30:14 -0700 PDT ContainersNotReady containers with unready status: [main]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:30:14 -0700 PDT ContainersNotReady containers with unready status: [main]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:30:14 -0700 PDT  }]&#xA;May 29 22:35:14.919: INFO: &#xA;May 29 22:35:14.961: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 29 22:35:15.003: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:61252,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:34:59 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:34:59 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:34:59 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:34:59 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 22:35:15.003: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 29 22:35:15.044: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 29 22:35:15.091: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:35:15.091: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:35:15.091: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 22:35:15.091: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:35:15.091: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:35:15.091: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:35:15.091: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:35:15.091: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:35:15.091: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:35:15.091: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:35:15.091: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:35:15.091: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:35:15.091: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 22:35:15.091: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:35:15.252: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 29 22:35:15.252: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:35:15.293: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:61277,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{KernelDeadlock False 2019-05-29 22:34:48 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 22:34:48 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-29 22:34:48 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-29 22:34:48 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 22:34:48 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 22:34:48 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 22:34:48 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:35:11 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:35:11 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:35:11 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:35:11 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 22:35:15.294: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:35:15.335: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:35:15.387: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:35:15.387: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 29 22:35:15.387: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:35:15.387: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 29 22:35:15.387: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:35:15.387: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:35:15.387: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 29 22:35:15.387: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:35:15.387: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 22:35:15.387: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:35:15.387: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:35:15.387: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 29 22:35:15.387: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:35:15.387: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 22:35:15.387: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:35:15.387: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:35:15.387: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:35:15.387: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 22:35:15.387: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:35:15.387: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 29 22:35:15.550: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:35:15.550: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:35:15.591: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:61223,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{CorruptDockerOverlay2 False 2019-05-29 22:34:46 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-29 22:34:46 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-29 22:34:46 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 22:34:46 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-29 22:34:46 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 22:34:46 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 22:34:46 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:34:43 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:34:43 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:34:43 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:34:43 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 22:35:15.592: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:35:15.634: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:35:15.683: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:35:15.683: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 29 22:35:15.683: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 29 22:35:15.683: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:35:15.683: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 29 22:35:15.683: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 29 22:35:15.683: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:35:15.684: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:35:15.684: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 22:35:15.684: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:35:15.684: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:35:15.684: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 22:35:15.684: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:35:15.684: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:35:15.684: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 22:35:15.863: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:35:15.864: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:35:15.905: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:61157,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:34:18 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:34:18 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:34:18 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:34:18 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 22:35:15.906: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:35:15.948: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:35:16.152: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:35:16.152: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:35:16.194: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:61204,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:34:39 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:34:39 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:34:39 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:34:39 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 22:35:16.194: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:35:16.235: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:35:16.438: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:35:16.438: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:35:16.481: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:61164,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:34:22 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:34:22 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:34:22 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:34:22 -0700 PDT 2019-05-29 22:33:22 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 22:35:16.481: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:35:16.522: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:35:16.571: INFO: update-demo-nautilus-5pqcg started at 2019-05-29 22:10:34 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:35:16.571: INFO: &#x9;Container update-demo ready: false, restart count 0&#xA;May 29 22:35:16.571: INFO: pod-exec-websocket-5ee920d5-eefa-4887-b5e0-09fc5939c69d started at 2019-05-29 22:30:14 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:35:16.571: INFO: &#x9;Container main ready: false, restart count 0&#xA;May 29 22:35:18.795: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:35:18.795: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;pods-4732&#34; for this suite.&#xA;May 29 22:45:19.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 22:45:20.028: INFO: namespace: pods-4732, resource: pods, items remaining: 1&#xA;May 29 22:45:20.651: INFO: namespace: pods-4732, DeletionTimetamp: 2019-05-29 22:35:18 -0700 PDT, Finalizers: [kubernetes], Phase: Terminating&#xA;May 29 22:45:20.692: INFO: namespace: pods-4732, total namespaces: 5, active: 4, terminating: 1&#xA;May 29 22:45:20.740: INFO: POD                                                      NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 22:45:20.740: INFO: pod-exec-websocket-5ee920d5-eefa-4887-b5e0-09fc5939c69d  e2e-test-peterhornyack-windows-node-group-jpxd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:30:14 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:43:22 -0700 PDT ContainersNotReady containers with unready status: [main]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:43:22 -0700 PDT ContainersNotReady containers with unready status: [main]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:30:14 -0700 PDT  }]&#xA;May 29 22:45:20.740: INFO: &#xA;May 29 22:45:20.740: INFO: Couldn&#39;t delete ns: &#34;pods-4732&#34;: namespace pods-4732 was not deleted with limit: timed out waiting for the condition, pods remaining: 1 (&amp;errors.errorString{s:&#34;namespace pods-4732 was not deleted with limit: timed out waiting for the condition, pods remaining: 1&#34;})&#xA;</system-out>
      </testcase>
      <testcase name="[sig-windows] Hybrid cluster network for all supported CNIs should have stable networking for Linux and Windows pods" classname="Kubernetes e2e suite" time="92.341438525"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl apply should apply a new configuration to an existing RC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for API chunking should return chunks of results for list calls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for endpoint-Service: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for git_repo [Serial] [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule a pod w/ RW PD(s) mounted to 1 or more containers, write to PD, verify content, delete pod, and repeat in rapid succession [Slow] using 1 containers and 2 PDs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking IPerf IPv4 [Experimental] [Feature:Networking-IPv4] [Slow] [Feature:Networking-Performance] should transfer ~ 1GB onto the service endpoint 1 servers (maximum of 1 clients)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceDefinition Watch CustomResourceDefinition Watch watch on custom resource definition objects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.00190213">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify an existing and compatible SPBM policy is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should run Custom Metrics - Stackdriver Adapter for external metrics [Feature:StackdriverExternalMetrics]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl apply apply set/view last-applied" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes Default StorageClass pods that use multiple volumes should be reschedulable [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for node-Service: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support seccomp alpha unconfined annotation on the container [Feature:Seccomp] [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to switch session affinity for service with type clusterIP" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a custom resource." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node Deployment.extensions with 2 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Logging soak [Performance] [Slow] [Disruptive] should survive logging 1KB every 1s seconds, for a duration of 2m0s" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should failover to a different zone when all nodes in one zone become unreachable [Slow] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="282.265521932">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0005d6080&gt;: {&#xA;        s: &#34;pod ran to completion&#34;,&#xA;    }&#xA;    pod ran to completion&#xA;occurred&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/pods.go:112</failure>
          <system-out>[BeforeEach] [sig-storage] ConfigMap&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 22:46:53.086: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename configmap&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[It] updates should be reflected in volume [NodeConformance] [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: Creating configMap with name configmap-test-upd-4ea55185-f48f-494c-b864-9d9d3d208c55&#xA;�[1mSTEP�[0m: Creating the pod&#xA;May 29 22:51:25.472: INFO: Unexpected error occurred: pod ran to completion&#xA;[AfterEach] [sig-storage] ConfigMap&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;configmap-7510&#34;.&#xA;�[1mSTEP�[0m: Found 5 events.&#xA;May 29 22:51:25.515: INFO: At 2019-05-29 22:46:53 -0700 PDT - event for pod-configmaps-f486795c-b412-42c3-9e6a-59ac7b24ea8e: {default-scheduler } Scheduled: Successfully assigned configmap-7510/pod-configmaps-f486795c-b412-42c3-9e6a-59ac7b24ea8e to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:51:25.515: INFO: At 2019-05-29 22:46:56 -0700 PDT - event for pod-configmaps-f486795c-b412-42c3-9e6a-59ac7b24ea8e: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/mounttest:1.0&#34; already present on machine&#xA;May 29 22:51:25.515: INFO: At 2019-05-29 22:46:56 -0700 PDT - event for pod-configmaps-f486795c-b412-42c3-9e6a-59ac7b24ea8e: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container configmap-volume-test&#xA;May 29 22:51:25.515: INFO: At 2019-05-29 22:46:58 -0700 PDT - event for pod-configmaps-f486795c-b412-42c3-9e6a-59ac7b24ea8e: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container configmap-volume-test&#xA;May 29 22:51:25.515: INFO: At 2019-05-29 22:49:37 -0700 PDT - event for pod-configmaps-f486795c-b412-42c3-9e6a-59ac7b24ea8e: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod configmap-7510/pod-configmaps-f486795c-b412-42c3-9e6a-59ac7b24ea8e&#xA;May 29 22:51:25.605: INFO: POD                                                    NODE                                            PHASE      GRACE  CONDITIONS&#xA;May 29 22:51:25.605: INFO: pod-configmaps-f486795c-b412-42c3-9e6a-59ac7b24ea8e    e2e-test-peterhornyack-windows-node-group-jpxd  Succeeded         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:46:53 -0700 PDT PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:46:53 -0700 PDT PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:46:53 -0700 PDT PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:46:53 -0700 PDT  }]&#xA;May 29 22:51:25.605: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:51:25.605: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 29 22:51:25.605: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master                   Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:51:25.605: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master                   Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:51:25.605: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:51:25.605: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:51:25.605: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:51:25.605: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 29 22:51:25.605: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master                   Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:51:25.606: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master                   Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 29 22:51:25.607: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6        Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 29 22:51:25.607: INFO: &#xA;May 29 22:51:25.651: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 29 22:51:25.692: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:63581,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:51:02 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:51:02 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:51:02 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:51:02 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 22:51:25.693: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 29 22:51:25.734: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 29 22:51:25.781: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:51:25.781: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 22:51:25.781: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:51:25.781: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:51:25.781: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:51:25.781: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:51:25.781: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:51:25.781: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:51:25.781: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:51:25.781: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:51:25.781: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 22:51:25.781: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:51:25.781: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:51:25.781: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:51:25.985: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 29 22:51:25.985: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:51:26.027: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:63602,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentContainerdRestart False 2019-05-29 22:50:58 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 22:50:58 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-29 22:50:58 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 22:50:58 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-29 22:50:58 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-29 22:50:58 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 22:50:58 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:51:12 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:51:12 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:51:12 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:51:12 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 22:51:26.028: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:51:26.073: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:51:26.136: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:51:26.136: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 29 22:51:26.136: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:51:26.136: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 22:51:26.136: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:51:26.136: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:51:26.136: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:51:26.136: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 22:51:26.136: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:51:26.136: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 29 22:51:26.136: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:51:26.136: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 29 22:51:26.136: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:51:26.136: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 29 22:51:26.136: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:51:26.136: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:51:26.136: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 29 22:51:26.136: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:51:26.136: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 22:51:26.136: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:51:26.340: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:51:26.340: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:51:26.394: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:63560,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentDockerRestart False 2019-05-29 22:50:54 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 22:50:54 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 22:50:54 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-29 22:50:54 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-29 22:50:54 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 22:50:54 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-29 22:50:54 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:50:44 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:50:44 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:50:44 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:50:44 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[nginx@sha256:0fd68ec4b64b8dbb2bef1f1a5de9d47b658afd3635dc9c45bf0cbeac46e72101 nginx:1.15-alpine] 16087791} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 22:51:26.394: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:51:26.438: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:51:26.485: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:51:26.485: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 22:51:26.485: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:51:26.485: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:51:26.485: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 22:51:26.485: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:51:26.486: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:51:26.486: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 22:51:26.486: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:51:26.486: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 29 22:51:26.486: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 29 22:51:26.486: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:51:26.486: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 29 22:51:26.486: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 29 22:51:26.486: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:51:26.643: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:51:26.643: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:51:26.685: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:63622,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:51:20 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:51:20 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:51:20 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:51:20 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 22:51:26.686: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:51:26.727: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:51:26.931: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:51:26.931: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:51:26.973: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:63528,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:50:40 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:50:40 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:50:40 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:50:40 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 22:51:26.973: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:51:27.015: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:51:27.215: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:51:27.215: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:51:27.260: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:63509,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:50:34 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:50:34 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:50:34 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:50:34 -0700 PDT 2019-05-29 22:49:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 22:51:27.260: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:51:27.301: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:51:27.351: INFO: pod-configmaps-f486795c-b412-42c3-9e6a-59ac7b24ea8e started at 2019-05-29 22:46:53 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:51:27.351: INFO: &#x9;Container configmap-volume-test ready: false, restart count 0&#xA;May 29 22:51:27.585: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:51:27.585: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;configmap-7510&#34; for this suite.&#xA;May 29 22:51:33.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 22:51:35.351: INFO: namespace configmap-7510 deletion completed in 7.723739497s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-windows] [Feature:Windows] [Feature:WindowsGMSA] GMSA [Slow] kubelet GMSA support when creating a pod with correct GMSA credential specs passes the credential specs down to the Pod&#39;s containers" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node ReplicationController with 0 secrets, 0 configmaps and 2 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001962671">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services [Feature:GCEAlphaFeature][Slow] should be able to create and tear down a standard-tier load balancer [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.001787344">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] MetricsGrabber should grab all metrics from a Kubelet." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Pods sharing a single local PV [Serial] all pods should be running" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should provision storage with non-default reclaim policy Retain" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Operations Storm [Feature:vsphere] should create pod with many volumes and verify no attach call fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001715365">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes CSI Topology test using GCE PD driver [Serial] should provision zonal PD with delayed volume binding and AllowedTopologies set and mount the volume to a pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] Multi-AZ Clusters should spread the pods of a replication controller across zones" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [sig-windows] Networking Granular Checks: Pods should function for node-pod communication: udp" classname="Kubernetes e2e suite" time="156.560735396"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should delete jobs and pods created by cronjob" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Generated clientset should create v1beta1 cronJobs, delete cronJobs, watch cronJobs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]" classname="Kubernetes e2e suite" time="481.401129343">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0025ec480&gt;: {&#xA;        s: &#34;Only 0 pods started out of 1&#34;,&#xA;    }&#xA;    Only 0 pods started out of 1&#xA;occurred&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:161</failure>
          <system-out>[BeforeEach] version v1&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 22:54:11.920: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename proxy&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[It] should proxy through a service and a pod  [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: starting an echo server on multiple ports&#xA;�[1mSTEP�[0m: creating replication controller proxy-service-4sk2r in namespace proxy-6895&#xA;[AfterEach] version v1&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;proxy-6895&#34;.&#xA;�[1mSTEP�[0m: Found 7 events.&#xA;May 29 22:59:13.486: INFO: At 2019-05-29 22:54:12 -0700 PDT - event for proxy-service-4sk2r: {replication-controller } SuccessfulCreate: Created pod: proxy-service-4sk2r-fpkgw&#xA;May 29 22:59:13.487: INFO: At 2019-05-29 22:54:12 -0700 PDT - event for proxy-service-4sk2r-fpkgw: {default-scheduler } Scheduled: Successfully assigned proxy-6895/proxy-service-4sk2r-fpkgw to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:59:13.487: INFO: At 2019-05-29 22:54:14 -0700 PDT - event for proxy-service-4sk2r-fpkgw: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulling: Pulling image &#34;e2eteam/porter:1.0&#34;&#xA;May 29 22:59:13.487: INFO: At 2019-05-29 22:54:16 -0700 PDT - event for proxy-service-4sk2r-fpkgw: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Successfully pulled image &#34;e2eteam/porter:1.0&#34;&#xA;May 29 22:59:13.487: INFO: At 2019-05-29 22:54:16 -0700 PDT - event for proxy-service-4sk2r-fpkgw: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container proxy-service-4sk2r&#xA;May 29 22:59:13.487: INFO: At 2019-05-29 22:54:18 -0700 PDT - event for proxy-service-4sk2r-fpkgw: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container proxy-service-4sk2r&#xA;May 29 22:59:13.487: INFO: At 2019-05-29 22:57:37 -0700 PDT - event for proxy-service-4sk2r-fpkgw: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod proxy-6895/proxy-service-4sk2r-fpkgw&#xA;May 29 22:59:13.576: INFO: POD                                                    NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 22:59:13.576: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:59:13.576: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 29 22:59:13.576: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:59:13.576: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:59:13.576: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:59:13.576: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:59:13.576: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:59:13.576: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 29 22:59:13.576: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 29 22:59:13.576: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 29 22:59:13.576: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 29 22:59:13.577: INFO: proxy-service-4sk2r-fpkgw                              e2e-test-peterhornyack-windows-node-group-jpxd  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:54:12 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:54:12 -0700 PDT ContainersNotReady containers with unready status: [proxy-service-4sk2r]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:54:12 -0700 PDT ContainersNotReady containers with unready status: [proxy-service-4sk2r]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 22:54:12 -0700 PDT  }]&#xA;May 29 22:59:13.578: INFO: &#xA;May 29 22:59:13.620: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 29 22:59:13.662: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:64784,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:59:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:59:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:59:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:59:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 22:59:13.662: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 29 22:59:13.704: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 29 22:59:13.750: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:59:13.750: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:59:13.750: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:59:13.750: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 22:59:13.750: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:59:13.750: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:59:13.750: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:59:13.750: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:59:13.750: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:59:13.750: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:59:13.750: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:59:13.750: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:59:13.750: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 22:59:13.750: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:59:13.944: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 29 22:59:13.944: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:59:13.986: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:64805,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentUnregisterNetDevice False 2019-05-29 22:59:01 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-29 22:59:01 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 22:59:01 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 22:59:01 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 22:59:01 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-29 22:59:01 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 22:59:01 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:59:12 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:59:12 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:59:12 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:59:12 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 22:59:13.986: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:59:14.027: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:59:14.079: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:59:14.079: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 29 22:59:14.079: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:59:14.079: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 29 22:59:14.079: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:59:14.079: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:59:14.079: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 29 22:59:14.079: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:59:14.079: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 22:59:14.079: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:59:14.079: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:59:14.079: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:59:14.079: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 22:59:14.079: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:59:14.079: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 29 22:59:14.079: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:59:14.079: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 29 22:59:14.079: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:59:14.079: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 22:59:14.079: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:59:14.243: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 22:59:14.243: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:59:14.285: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:64770,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentUnregisterNetDevice False 2019-05-29 22:58:57 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-29 22:58:57 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 22:58:57 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-29 22:58:57 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 22:58:57 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 22:58:57 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 22:58:57 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:58:45 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:58:45 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:58:45 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:58:45 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[nginx@sha256:0fd68ec4b64b8dbb2bef1f1a5de9d47b658afd3635dc9c45bf0cbeac46e72101 nginx:1.15-alpine] 16087791} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 22:59:14.285: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:59:14.326: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:59:14.377: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:59:14.377: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 29 22:59:14.377: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 29 22:59:14.377: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 22:59:14.377: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:59:14.377: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 22:59:14.377: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:59:14.377: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:59:14.377: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 22:59:14.377: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 22:59:14.377: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:59:14.377: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 22:59:14.377: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 22:59:14.377: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 29 22:59:14.378: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 29 22:59:14.542: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 22:59:14.542: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:59:14.584: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:64755,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:58:51 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:58:51 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:58:51 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:58:51 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 22:59:14.584: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:59:14.626: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:59:14.838: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 22:59:14.838: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:59:14.883: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:64731,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:58:41 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:58:41 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:58:41 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:58:41 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 22:59:14.883: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:59:14.925: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:59:15.124: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 22:59:15.124: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:59:15.166: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:64714,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 22:58:35 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 22:58:35 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 22:58:35 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 22:58:35 -0700 PDT 2019-05-29 22:57:35 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[e2eteam/porter@sha256:f1f16595d44d9a06e851d82135a0ae53fbdf512029c6e3301f140a531778c65d e2eteam/porter:1.0] 4284085058} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 22:59:15.166: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:59:15.207: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:59:15.260: INFO: proxy-service-4sk2r-fpkgw started at 2019-05-29 22:54:12 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 22:59:15.260: INFO: &#x9;Container proxy-service-4sk2r ready: false, restart count 0&#xA;May 29 22:59:17.476: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 22:59:17.476: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;proxy-6895&#34; for this suite.&#xA;May 29 23:02:11.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 23:02:13.321: INFO: namespace proxy-6895 deletion completed in 2m55.801625238s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.001913145">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for Table transformation should return chunks of table results for list calls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using Deployment.extensions with 2 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="327.587604372">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;Unexpected error:&#xA;    &lt;*errors.errorString | 0xc003a4cc10&gt;: {&#xA;        s: &#34;want pod &#39;test-webserver-6cd89928-8100-45e0-bd58-470476a7e51c&#39; on &#39;e2e-test-peterhornyack-windows-node-group-jpxd&#39; to be &#39;Running&#39; but was &#39;Pending&#39;&#34;,&#xA;    }&#xA;    want pod &#39;test-webserver-6cd89928-8100-45e0-bd58-470476a7e51c&#39; on &#39;e2e-test-peterhornyack-windows-node-group-jpxd&#39; to be &#39;Running&#39; but was &#39;Pending&#39;&#xA;occurred&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:67</failure>
          <system-out>[BeforeEach] [k8s.io] Probing container&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 23:02:13.324: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename container-probe&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [k8s.io] Probing container&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51&#xA;[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;May 29 23:07:13.751: INFO: Unexpected error occurred: want pod &#39;test-webserver-6cd89928-8100-45e0-bd58-470476a7e51c&#39; on &#39;e2e-test-peterhornyack-windows-node-group-jpxd&#39; to be &#39;Running&#39; but was &#39;Pending&#39;&#xA;[AfterEach] [k8s.io] Probing container&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;container-probe-7636&#34;.&#xA;�[1mSTEP�[0m: Found 5 events.&#xA;May 29 23:07:13.794: INFO: At 2019-05-29 23:02:13 -0700 PDT - event for test-webserver-6cd89928-8100-45e0-bd58-470476a7e51c: {default-scheduler } Scheduled: Successfully assigned container-probe-7636/test-webserver-6cd89928-8100-45e0-bd58-470476a7e51c to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 23:07:13.794: INFO: At 2019-05-29 23:02:15 -0700 PDT - event for test-webserver-6cd89928-8100-45e0-bd58-470476a7e51c: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/test-webserver:1.0&#34; already present on machine&#xA;May 29 23:07:13.794: INFO: At 2019-05-29 23:02:15 -0700 PDT - event for test-webserver-6cd89928-8100-45e0-bd58-470476a7e51c: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container test-webserver&#xA;May 29 23:07:13.794: INFO: At 2019-05-29 23:02:17 -0700 PDT - event for test-webserver-6cd89928-8100-45e0-bd58-470476a7e51c: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container test-webserver&#xA;May 29 23:07:13.794: INFO: At 2019-05-29 23:05:38 -0700 PDT - event for test-webserver-6cd89928-8100-45e0-bd58-470476a7e51c: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod container-probe-7636/test-webserver-6cd89928-8100-45e0-bd58-470476a7e51c&#xA;May 29 23:07:13.882: INFO: POD                                                    NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 23:07:13.882: INFO: test-webserver-6cd89928-8100-45e0-bd58-470476a7e51c    e2e-test-peterhornyack-windows-node-group-jpxd  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:02:13 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:02:13 -0700 PDT ContainersNotReady containers with unready status: [test-webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:02:13 -0700 PDT ContainersNotReady containers with unready status: [test-webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:02:13 -0700 PDT  }]&#xA;May 29 23:07:13.882: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:07:13.882: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 29 23:07:13.882: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 23:07:13.882: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 23:07:13.882: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 23:07:13.882: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 29 23:07:13.883: INFO: &#xA;May 29 23:07:13.927: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 29 23:07:13.968: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:65950,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 23:07:06 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 23:07:06 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 23:07:06 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 23:07:06 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 23:07:13.968: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 29 23:07:14.012: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 29 23:07:14.061: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:07:14.061: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:07:14.061: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:07:14.061: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:07:14.061: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:07:14.061: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:07:14.061: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 23:07:14.061: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 23:07:14.061: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:07:14.061: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:07:14.061: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:07:14.061: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 23:07:14.061: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 23:07:14.061: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:07:14.222: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 29 23:07:14.222: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 23:07:14.263: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:65967,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{ReadonlyFilesystem False 2019-05-29 23:07:04 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-29 23:07:04 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-29 23:07:04 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 23:07:04 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 23:07:04 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 23:07:04 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-29 23:07:04 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 23:07:13 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 23:07:13 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 23:07:13 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 23:07:13 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 23:07:14.264: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 23:07:14.305: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 23:07:14.357: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 23:07:14.357: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 29 23:07:14.357: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:07:14.357: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 23:07:14.357: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 23:07:14.357: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:07:14.357: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 23:07:14.357: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 23:07:14.357: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 23:07:14.357: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 29 23:07:14.357: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 23:07:14.357: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 29 23:07:14.357: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:07:14.357: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 29 23:07:14.357: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 23:07:14.357: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 23:07:14.357: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 29 23:07:14.357: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:07:14.357: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 23:07:14.357: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 23:07:14.526: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 23:07:14.526: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 23:07:14.568: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:65940,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentContainerdRestart False 2019-05-29 23:07:02 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 23:07:02 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-29 23:07:02 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-29 23:07:02 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 23:07:02 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-29 23:07:02 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 23:07:02 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 23:06:45 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 23:06:45 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 23:06:45 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 23:06:45 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[nginx@sha256:0fd68ec4b64b8dbb2bef1f1a5de9d47b658afd3635dc9c45bf0cbeac46e72101 nginx:1.15-alpine] 16087791} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 23:07:14.568: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 23:07:14.610: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 23:07:14.660: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:07:14.660: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:07:14.660: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 23:07:14.660: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 23:07:14.660: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:07:14.660: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 23:07:14.660: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 23:07:14.661: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 23:07:14.661: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 23:07:14.661: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:07:14.661: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 29 23:07:14.661: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 29 23:07:14.661: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:07:14.661: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 29 23:07:14.661: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 29 23:07:14.832: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 23:07:14.832: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 23:07:14.873: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:65916,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 23:06:51 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 23:06:51 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 23:06:51 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 23:06:51 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 23:07:14.874: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 23:07:14.920: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 23:07:15.124: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 23:07:15.124: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 23:07:15.166: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:65892,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 23:06:42 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 23:06:42 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 23:06:42 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 23:06:42 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 23:07:15.166: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 23:07:15.207: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 23:07:15.412: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 23:07:15.412: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 23:07:15.455: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:65875,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 23:06:36 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 23:06:36 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 23:06:36 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 23:06:36 -0700 PDT 2019-05-29 23:05:35 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[e2eteam/porter@sha256:f1f16595d44d9a06e851d82135a0ae53fbdf512029c6e3301f140a531778c65d e2eteam/porter:1.0] 4284085058} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 23:07:15.455: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 23:07:15.496: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 23:07:15.545: INFO: test-webserver-6cd89928-8100-45e0-bd58-470476a7e51c started at 2019-05-29 23:02:13 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 23:07:15.545: INFO: &#x9;Container test-webserver ready: false, restart count 0&#xA;May 29 23:07:17.124: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 23:07:17.124: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;container-probe-7636&#34; for this suite.&#xA;May 29 23:07:39.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 23:07:40.911: INFO: namespace container-probe-7636 deletion completed in 23.744313854s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] Volume expand should not allow expansion of pvcs without AllowVolumeExpansion property" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] multicluster ingress should get instance group annotation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="192.708293986"></testcase>
      <testcase name="[sig-cluster-lifecycle] Reboot [Disruptive] [Feature:Reboot] each node by triggering kernel panic and ensure they function upon restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.00193973">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on the allowed zones and datastore specified in storage class" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should create endpoints for unready pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against a pod with different priority class (ScopeSelectorOpExists)." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] rolling update backend pods should not cause service disruption" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001583466">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner Default should be disabled by removing the default annotation [Serial] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down when rescheduling a pod is required and pdb allows for it[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="776.648398596">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;May 29 23:23:50.272: Couldn&#39;t delete ns: &#34;container-lifecycle-hook-6689&#34;: namespace container-lifecycle-hook-6689 was not deleted with limit: timed out waiting for the condition, pods remaining: 1 (&amp;errors.errorString{s:&#34;namespace container-lifecycle-hook-6689 was not deleted with limit: timed out waiting for the condition, pods remaining: 1&#34;})&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:334</failure>
          <system-out>[BeforeEach] [k8s.io] Container Lifecycle Hook&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 23:10:53.625: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename container-lifecycle-hook&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] when create a pod with lifecycle hook&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61&#xA;�[1mSTEP�[0m: create the container to handle the HTTPGet hook request.&#xA;[It] should execute poststart http hook properly [NodeConformance] [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: create the pod with lifecycle hook&#xA;�[1mSTEP�[0m: check poststart hook&#xA;�[1mSTEP�[0m: delete the pod with lifecycle hook&#xA;May 29 23:13:44.251: INFO: Waiting for pod pod-with-poststart-http-hook to disappear&#xA;May 29 23:13:44.293: INFO: Pod pod-with-poststart-http-hook still exists&#xA;May 29 23:13:46.293: INFO: Waiting for pod pod-with-poststart-http-hook to disappear&#xA;May 29 23:13:46.335: INFO: Pod pod-with-poststart-http-hook still exists&#xA;May 29 23:13:48.293: INFO: Waiting for pod pod-with-poststart-http-hook to disappear&#xA;May 29 23:13:48.335: INFO: Pod pod-with-poststart-http-hook no longer exists&#xA;[AfterEach] [k8s.io] Container Lifecycle Hook&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;May 29 23:13:48.335: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;container-lifecycle-hook-6689&#34; for this suite.&#xA;May 29 23:23:48.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 23:23:49.517: INFO: namespace: container-lifecycle-hook-6689, resource: pods, items remaining: 1&#xA;May 29 23:23:50.186: INFO: namespace: container-lifecycle-hook-6689, DeletionTimetamp: 2019-05-29 23:13:48 -0700 PDT, Finalizers: [kubernetes], Phase: Terminating&#xA;May 29 23:23:50.228: INFO: namespace: container-lifecycle-hook-6689, total namespaces: 5, active: 4, terminating: 1&#xA;May 29 23:23:50.272: INFO: POD                      NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 23:23:50.272: INFO: pod-handle-http-request  e2e-test-peterhornyack-windows-node-group-jpxd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:10:53 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:13:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:13:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:10:53 -0700 PDT  }]&#xA;May 29 23:23:50.272: INFO: &#xA;May 29 23:23:50.272: INFO: Couldn&#39;t delete ns: &#34;container-lifecycle-hook-6689&#34;: namespace container-lifecycle-hook-6689 was not deleted with limit: timed out waiting for the condition, pods remaining: 1 (&amp;errors.errorString{s:&#34;namespace container-lifecycle-hook-6689 was not deleted with limit: timed out waiting for the condition, pods remaining: 1&#34;})&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI online volume expansion [Feature:ExpandCSIVolumes][Feature:ExpandInUseVolumes] should expand volume without restarting pod if attach=off, nodeExpansion=on" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should create ingress with backend HTTPS" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on the allowed zones and storage policy specified in storage class" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Firewall rule should have correct firewall rules for e2e cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node Job.batch with 0 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide podname as non-root with fsgroup [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="356.638881261">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;Timed out after 240.003s.&#xA;Expected&#xA;    &lt;string&gt;: Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/projected-configmap-volumes/create/data-1: open C:\etc\projected-configmap-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    &#xA;to contain substring&#xA;    &lt;string&gt;: value-1&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:393</failure>
          <system-out>[BeforeEach] [sig-storage] Projected configMap&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 23:23:50.274: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename projected&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[It] optional updates should be reflected in volume [NodeConformance] [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: Creating configMap with name cm-test-opt-del-d127b43e-460a-4cfb-ae0d-ef976f7f5391&#xA;�[1mSTEP�[0m: Creating configMap with name cm-test-opt-upd-5aea60f9-3ba5-401f-95b1-81bf017f07b9&#xA;�[1mSTEP�[0m: Creating the pod&#xA;�[1mSTEP�[0m: Deleting configmap cm-test-opt-del-d127b43e-460a-4cfb-ae0d-ef976f7f5391&#xA;�[1mSTEP�[0m: Updating configmap cm-test-opt-upd-5aea60f9-3ba5-401f-95b1-81bf017f07b9&#xA;�[1mSTEP�[0m: Creating configMap with name cm-test-opt-create-d7ecedcc-e77c-47d7-9df2-8972c7746f44&#xA;�[1mSTEP�[0m: waiting to observe update in volume&#xA;[AfterEach] [sig-storage] Projected configMap&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;projected-5612&#34;.&#xA;�[1mSTEP�[0m: Found 11 events.&#xA;May 29 23:29:37.096: INFO: At 2019-05-29 23:23:50 -0700 PDT - event for pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a: {default-scheduler } Scheduled: Successfully assigned projected-5612/pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 23:29:37.096: INFO: At 2019-05-29 23:23:52 -0700 PDT - event for pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/mounttest:1.0&#34; already present on machine&#xA;May 29 23:29:37.096: INFO: At 2019-05-29 23:23:52 -0700 PDT - event for pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container delcm-volume-test&#xA;May 29 23:29:37.096: INFO: At 2019-05-29 23:23:54 -0700 PDT - event for pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container delcm-volume-test&#xA;May 29 23:29:37.096: INFO: At 2019-05-29 23:23:54 -0700 PDT - event for pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/mounttest:1.0&#34; already present on machine&#xA;May 29 23:29:37.096: INFO: At 2019-05-29 23:23:54 -0700 PDT - event for pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container updcm-volume-test&#xA;May 29 23:29:37.096: INFO: At 2019-05-29 23:23:56 -0700 PDT - event for pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container updcm-volume-test&#xA;May 29 23:29:37.096: INFO: At 2019-05-29 23:23:56 -0700 PDT - event for pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/mounttest:1.0&#34; already present on machine&#xA;May 29 23:29:37.096: INFO: At 2019-05-29 23:23:56 -0700 PDT - event for pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container createcm-volume-test&#xA;May 29 23:29:37.096: INFO: At 2019-05-29 23:23:59 -0700 PDT - event for pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container createcm-volume-test&#xA;May 29 23:29:37.096: INFO: At 2019-05-29 23:25:38 -0700 PDT - event for pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod projected-5612/pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a&#xA;May 29 23:29:37.184: INFO: POD                                                            NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 29 23:29:37.184: INFO: pod-handle-http-request                                        e2e-test-peterhornyack-windows-node-group-jpxd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:10:53 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:13:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:13:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:10:53 -0700 PDT  }]&#xA;May 29 23:29:37.184: INFO: coredns-5b969f4c88-gsjpw                                       e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:29:37.184: INFO: coredns-5b969f4c88-mvhtd                                       e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 29 23:29:37.184: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 23:29:37.184: INFO: etcd-server-e2e-test-peterhornyack-master                      e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 23:29:37.184: INFO: etcd-server-events-e2e-test-peterhornyack-master               e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 23:29:37.184: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                         e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                            e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: fluentd-gcp-v3.2.0-fr5zq                                       e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: fluentd-gcp-v3.2.0-r5s9z                                       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: fluentd-gcp-v3.2.0-wp9vf                                       e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                        e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: kube-addon-manager-e2e-test-peterhornyack-master               e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: kube-apiserver-e2e-test-peterhornyack-master                   e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: kube-controller-manager-e2e-test-peterhornyack-master          e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: kube-dns-autoscaler-97df449df-7v474                            e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh            e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6            e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: kube-scheduler-e2e-test-peterhornyack-master                   e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                          e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: l7-default-backend-8f479dd9-hnbtn                              e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master          e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: metadata-proxy-v0.1-8mhrb                                      e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: metadata-proxy-v0.1-gqcgn                                      e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: metadata-proxy-v0.1-w99mm                                      e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                         e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a  e2e-test-peterhornyack-windows-node-group-jpxd  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:23:50 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:25:36 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:25:36 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 23:23:50 -0700 PDT  }]&#xA;May 29 23:29:37.185: INFO: &#xA;May 29 23:29:37.227: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 29 23:29:37.269: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:69150,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 23:29:10 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 23:29:10 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 23:29:10 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 23:29:10 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 23:29:37.270: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 29 23:29:37.311: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 29 23:29:37.357: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:29:37.357: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:29:37.357: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:29:37.357: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 23:29:37.357: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 23:29:37.357: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:29:37.357: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:29:37.357: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:29:37.357: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:29:37.357: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:29:37.357: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:29:37.357: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:29:37.357: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 23:29:37.357: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 23:29:37.519: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 29 23:29:37.519: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 23:29:37.563: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:69176,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentKubeletRestart False 2019-05-29 23:29:20 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 23:29:20 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 23:29:20 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-29 23:29:20 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-29 23:29:20 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 23:29:20 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-29 23:29:20 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 23:29:15 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 23:29:15 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 23:29:15 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 23:29:15 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 23:29:37.563: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 23:29:37.604: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 23:29:37.656: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:29:37.656: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 23:29:37.656: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 23:29:37.656: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 23:29:37.656: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 29 23:29:37.656: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 23:29:37.656: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 29 23:29:37.656: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:29:37.656: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 23:29:37.656: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 23:29:37.656: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 23:29:37.656: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 29 23:29:37.656: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:29:37.656: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 29 23:29:37.656: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 23:29:37.656: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 23:29:37.656: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 29 23:29:37.656: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:29:37.656: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 23:29:37.656: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 23:29:37.817: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 29 23:29:37.817: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 23:29:37.858: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:69155,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{CorruptDockerOverlay2 False 2019-05-29 23:29:11 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-29 23:29:11 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-29 23:29:11 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-29 23:29:11 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-29 23:29:11 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-29 23:29:11 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-29 23:29:11 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 23:28:47 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 23:28:47 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 23:28:47 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 23:28:47 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[nginx@sha256:0fd68ec4b64b8dbb2bef1f1a5de9d47b658afd3635dc9c45bf0cbeac46e72101 nginx:1.15-alpine] 16087791} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 29 23:29:37.858: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 23:29:37.900: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 23:29:37.955: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:29:37.955: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 29 23:29:37.956: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 23:29:37.956: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:29:37.956: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 29 23:29:37.956: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 29 23:29:37.956: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 23:29:37.956: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 29 23:29:37.956: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:29:37.956: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 29 23:29:37.956: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 29 23:29:37.956: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 29 23:29:37.956: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 29 23:29:37.956: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 29 23:29:37.956: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 29 23:29:38.105: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 29 23:29:38.105: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 23:29:38.146: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:69108,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 23:28:53 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 23:28:53 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 23:28:53 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 23:28:53 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 23:29:38.147: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 23:29:38.188: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 23:29:38.388: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 29 23:29:38.388: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 23:29:38.430: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:69133,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 23:29:03 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 23:29:03 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 23:29:03 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 23:29:03 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 23:29:38.430: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 23:29:38.472: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 23:29:38.681: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 29 23:29:38.681: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 23:29:38.723: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:69217,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[{node.kubernetes.io/not-ready  NoExecute 2019-05-29 23:28:43 -0700 PDT}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-29 23:29:38 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-29 23:29:38 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-29 23:29:38 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-29 23:29:38 -0700 PDT 2019-05-29 23:29:38 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[e2eteam/porter@sha256:f1f16595d44d9a06e851d82135a0ae53fbdf512029c6e3301f140a531778c65d e2eteam/porter:1.0] 4284085058} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 29 23:29:38.723: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 23:29:38.764: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 23:29:38.809: INFO: pod-handle-http-request started at 2019-05-29 23:10:53 -0700 PDT (0+1 container statuses recorded)&#xA;May 29 23:29:38.809: INFO: &#x9;Container pod-handle-http-request ready: true, restart count 0&#xA;May 29 23:29:38.809: INFO: pod-projected-configmaps-4011e193-c5b1-47cf-9d28-2d7652d2309a started at 2019-05-29 23:23:50 -0700 PDT (0+3 container statuses recorded)&#xA;May 29 23:29:38.809: INFO: &#x9;Container createcm-volume-test ready: true, restart count 0&#xA;May 29 23:29:38.809: INFO: &#x9;Container delcm-volume-test ready: true, restart count 0&#xA;May 29 23:29:38.809: INFO: &#x9;Container updcm-volume-test ready: true, restart count 0&#xA;May 29 23:29:39.027: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 29 23:29:39.027: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;May 29 23:29:39.070: INFO: Condition Ready of node e2e-test-peterhornyack-windows-node-group-jpxd is true, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoExecute 2019-05-29 23:28:43 -0700 PDT}]. Failure&#xA;�[1mSTEP�[0m: Destroying namespace &#34;projected-5612&#34; for this suite.&#xA;May 29 23:29:45.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 23:29:46.913: INFO: namespace projected-5612 deletion completed in 7.842496867s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Downgrade [Feature:Downgrade] cluster downgrade should maintain a functioning cluster [Feature:ClusterDowngrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should only allow access from service loadbalancer source ranges [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PVC and non-pre-bound PV: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.000940915">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects a client request should support a client that connects, sends NO DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver [IPv4] Forward PTR lookup should forward PTR records lookup to upstream nameserver [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver [IPv4] Forward external name lookup should forward externalname lookup to upstream nameserver [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Proxy server should support --unix-socket=/path  [Conformance]" classname="Kubernetes e2e suite" time="8.030844481"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere should test that deleting the PV before the pod does not cause pod deletion to fail on vspehre volume detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Disk Size [Feature:vsphere] verify dynamically provisioned pv using storageclass with an invalid disk size fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] ReplicaSet Should scale from 1 pod to 3 pods and from 3 to 5" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]" classname="Kubernetes e2e suite" time="48.408469753"></testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] vcp at scale [Feature:vsphere]  vsphere scale tests" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Feature:CustomResourcePublishOpenAPI] works for CRD without validation schema" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:DynamicAudit] should dynamically audit API calls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]" classname="Kubernetes e2e suite" time="18.817058908"></testcase>
      <testcase name="[k8s.io] [Feature:TTLAfterFinished][NodeAlphaFeature:TTLAfterFinished] job should be deleted once it finishes after TTL seconds" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.000786053">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.00060746">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="286.042336881"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should conform to Ingress spec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 3 pods per node ReplicationController with 0 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should sync endpoints to NEG" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="34.877163289"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.002319056">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.001719544">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] MetricsGrabber should grab all metrics from a ControllerManager." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] NodeLease when the NodeLease feature is enabled the kubelet should report node status infrequently" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.001702338">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should create ingress with pre-shared certificate" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when true [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NoSNAT [Feature:NoSNAT] [Slow] Should be able to send traffic between Pods without SNAT" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="76.16580958"></testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against a pod with same priority class." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.002035398">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]" classname="Kubernetes e2e suite" time="18.546304326"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.00192819">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment iterative rollouts should eventually progress" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should replace jobs when ReplaceConcurrent" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.002069443">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects a client request should support a client that connects, sends NO DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.001844527">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner delayed binding with allowedTopologies [Slow] should create persistent volumes in the same zone as specified in allowedTopologies after a pod mounting the claims is started" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.001704894">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001724439">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should adopt matching orphans and release non-matching pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node Deployment.extensions with 0 secrets, 2 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] volume on default medium should have the correct mode using FSGroup" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update should support rolling-update to same image  [Conformance]" classname="Kubernetes e2e suite" time="95.074708093"></testcase>
      <testcase name="[sig-storage] PersistentVolumes GCEPD should test that deleting the PV before the pod does not cause pod deletion to fail on PD detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should not be able to mutate or prevent deletion of webhook configuration objects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should be able to create a ClusterIP service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Proxy server should support proxy with --port 0  [Conformance]" classname="Kubernetes e2e suite" time="8.205861364"></testcase>
      <testcase name="[k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking should provide Internet connection for containers [Feature:Networking-IPv4]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] vsphere statefulset vsphere statefulset testing" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid diskStripes and objectSpaceReservation values is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.002224655">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl create quota should create a quota without scopes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Reboot [Disruptive] [Feature:Reboot] each node by ordering unclean reboot and ensure they function upon restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale up with two metrics of type Pod from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] vcp-performance [Feature:vsphere] vcp performance tests" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should provision storage with delayed binding [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  StatefulSet with pod affinity [Slow] should use volumes on one node when pod management is parallel and pod has affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="18.849285635"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes [Feature:ReclaimPolicy] [sig-storage] persistentvolumereclaim:vsphere should not detach and unmount PV when associated pvc with delete as reclaimPolicy is deleted when it is in use by the pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GCP Volumes NFSv4 should be mountable for NFSv4" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node Random with 0 secrets, 0 configmaps and 0 daemons with quotas" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks should be able to delete a non-existent PD without error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node Deployment.extensions with 0 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for client IP based session affinity: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should support multiple TLS certs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Reboot [Disruptive] [Feature:Reboot] each node by dropping all outbound packets for a while and ensure they function afterwards" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.001856488">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using Deployment.extensions with 0 secrets, 0 configmaps, 2 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="17.04783911"></testcase>
      <testcase name="[sig-network] Services should be able to change the type and ports of a service [Slow] [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]" classname="Kubernetes e2e suite" time="13.334798846"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : configmap" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-cloud-provider] [Feature:CloudProvider][Disruptive] Nodes should be deleted on API server if it doesn&#39;t exist in the cloud provider" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should run Stackdriver Metadata Agent [Feature:StackdriverMetadataAgent]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.002100735">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]" classname="Kubernetes e2e suite" time="16.566141835"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001905718">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Guestbook application should create and stop a working application  [Conformance]" classname="Kubernetes e2e suite" time="905.512770084">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;May 29 23:55:52.428: Couldn&#39;t delete ns: &#34;kubectl-1179&#34;: namespace kubectl-1179 was not deleted with limit: timed out waiting for the condition, namespace is empty but is not yet removed (&amp;errors.errorString{s:&#34;namespace kubectl-1179 was not deleted with limit: timed out waiting for the condition, namespace is empty but is not yet removed&#34;})&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:334</failure>
          <system-out>[BeforeEach] [sig-cli] Kubectl client&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 29 23:40:46.916: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename kubectl&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-cli] Kubectl client&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214&#xA;[It] should create and stop a working application  [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: creating all guestbook components&#xA;May 29 23:40:47.117: INFO: apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: redis-slave&#xA;  labels:&#xA;    app: redis&#xA;    role: slave&#xA;    tier: backend&#xA;spec:&#xA;  ports:&#xA;  - port: 6379&#xA;  selector:&#xA;    app: redis&#xA;    role: slave&#xA;    tier: backend&#xA;&#xA;May 29 23:40:47.117: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config create -f - --namespace=kubectl-1179&#39;&#xA;May 29 23:40:47.511: INFO: stderr: &#34;&#34;&#xA;May 29 23:40:47.511: INFO: stdout: &#34;service/redis-slave created\n&#34;&#xA;May 29 23:40:47.511: INFO: apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: redis-master&#xA;  labels:&#xA;    app: redis&#xA;    role: master&#xA;    tier: backend&#xA;spec:&#xA;  ports:&#xA;  - port: 6379&#xA;    targetPort: 6379&#xA;  selector:&#xA;    app: redis&#xA;    role: master&#xA;    tier: backend&#xA;&#xA;May 29 23:40:47.511: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config create -f - --namespace=kubectl-1179&#39;&#xA;May 29 23:40:47.904: INFO: stderr: &#34;&#34;&#xA;May 29 23:40:47.904: INFO: stdout: &#34;service/redis-master created\n&#34;&#xA;May 29 23:40:47.904: INFO: apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: frontend&#xA;  labels:&#xA;    app: guestbook&#xA;    tier: frontend&#xA;spec:&#xA;  # if your cluster supports it, uncomment the following to automatically create&#xA;  # an external load-balanced IP for the frontend service.&#xA;  # type: LoadBalancer&#xA;  ports:&#xA;  - port: 80&#xA;  selector:&#xA;    app: guestbook&#xA;    tier: frontend&#xA;&#xA;May 29 23:40:47.904: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config create -f - --namespace=kubectl-1179&#39;&#xA;May 29 23:40:48.291: INFO: stderr: &#34;&#34;&#xA;May 29 23:40:48.291: INFO: stdout: &#34;service/frontend created\n&#34;&#xA;May 29 23:40:48.291: INFO: apiVersion: apps/v1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: frontend&#xA;spec:&#xA;  replicas: 3&#xA;  selector:&#xA;    matchLabels:&#xA;      app: guestbook&#xA;      tier: frontend&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: guestbook&#xA;        tier: frontend&#xA;    spec:&#xA;      containers:&#xA;      - name: php-redis&#xA;        image: e2eteam/gb-frontend:v6&#xA;        resources:&#xA;          requests:&#xA;            cpu: 100m&#xA;            memory: 100Mi&#xA;        env:&#xA;        - name: GET_HOSTS_FROM&#xA;          value: dns&#xA;          # If your cluster config does not include a dns service, then to&#xA;          # instead access environment variables to find service host&#xA;          # info, comment out the &#39;value: dns&#39; line above, and uncomment the&#xA;          # line below:&#xA;          # value: env&#xA;        ports:&#xA;        - containerPort: 80&#xA;&#xA;May 29 23:40:48.291: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config create -f - --namespace=kubectl-1179&#39;&#xA;May 29 23:40:48.655: INFO: stderr: &#34;&#34;&#xA;May 29 23:40:48.655: INFO: stdout: &#34;deployment.apps/frontend created\n&#34;&#xA;May 29 23:40:48.655: INFO: apiVersion: apps/v1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: redis-master&#xA;spec:&#xA;  replicas: 1&#xA;  selector:&#xA;    matchLabels:&#xA;      app: redis&#xA;      role: master&#xA;      tier: backend&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: redis&#xA;        role: master&#xA;        tier: backend&#xA;    spec:&#xA;      containers:&#xA;      - name: master&#xA;        image: e2eteam/redis:1.0&#xA;        resources:&#xA;          requests:&#xA;            cpu: 100m&#xA;            memory: 100Mi&#xA;        ports:&#xA;        - containerPort: 6379&#xA;&#xA;May 29 23:40:48.655: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config create -f - --namespace=kubectl-1179&#39;&#xA;May 29 23:40:49.029: INFO: stderr: &#34;&#34;&#xA;May 29 23:40:49.029: INFO: stdout: &#34;deployment.apps/redis-master created\n&#34;&#xA;May 29 23:40:49.030: INFO: apiVersion: apps/v1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: redis-slave&#xA;spec:&#xA;  replicas: 2&#xA;  selector:&#xA;    matchLabels:&#xA;      app: redis&#xA;      role: slave&#xA;      tier: backend&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: redis&#xA;        role: slave&#xA;        tier: backend&#xA;    spec:&#xA;      containers:&#xA;      - name: slave&#xA;        image: e2eteam/gb-redisslave:v3&#xA;        resources:&#xA;          requests:&#xA;            cpu: 100m&#xA;            memory: 100Mi&#xA;        env:&#xA;        - name: GET_HOSTS_FROM&#xA;          value: dns&#xA;          # If your cluster config does not include a dns service, then to&#xA;          # instead access an environment variable to find the master&#xA;          # service&#39;s host, comment out the &#39;value: dns&#39; line above, and&#xA;          # uncomment the line below:&#xA;          # value: env&#xA;        ports:&#xA;        - containerPort: 6379&#xA;&#xA;May 29 23:40:49.030: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config create -f - --namespace=kubectl-1179&#39;&#xA;May 29 23:40:49.431: INFO: stderr: &#34;&#34;&#xA;May 29 23:40:49.431: INFO: stdout: &#34;deployment.apps/redis-slave created\n&#34;&#xA;�[1mSTEP�[0m: validating guestbook app&#xA;May 29 23:40:49.431: INFO: Waiting for all frontend pods to be Running.&#xA;May 29 23:45:44.500: INFO: Waiting for frontend to serve content.&#xA;May 29 23:45:48.076: INFO: Trying to add a new entry to the guestbook.&#xA;May 29 23:45:48.656: INFO: Verifying that added entry can be retrieved.&#xA;�[1mSTEP�[0m: using delete to clean up resources&#xA;May 29 23:45:48.714: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-1179&#39;&#xA;May 29 23:45:49.014: INFO: stderr: &#34;warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n&#34;&#xA;May 29 23:45:49.014: INFO: stdout: &#34;service \&#34;redis-slave\&#34; force deleted\n&#34;&#xA;�[1mSTEP�[0m: using delete to clean up resources&#xA;May 29 23:45:49.014: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-1179&#39;&#xA;May 29 23:45:49.324: INFO: stderr: &#34;warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n&#34;&#xA;May 29 23:45:49.324: INFO: stdout: &#34;service \&#34;redis-master\&#34; force deleted\n&#34;&#xA;�[1mSTEP�[0m: using delete to clean up resources&#xA;May 29 23:45:49.324: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-1179&#39;&#xA;May 29 23:45:49.630: INFO: stderr: &#34;warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n&#34;&#xA;May 29 23:45:49.630: INFO: stdout: &#34;service \&#34;frontend\&#34; force deleted\n&#34;&#xA;�[1mSTEP�[0m: using delete to clean up resources&#xA;May 29 23:45:49.631: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-1179&#39;&#xA;May 29 23:45:49.965: INFO: stderr: &#34;warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n&#34;&#xA;May 29 23:45:49.965: INFO: stdout: &#34;deployment.apps \&#34;frontend\&#34; force deleted\n&#34;&#xA;�[1mSTEP�[0m: using delete to clean up resources&#xA;May 29 23:45:49.966: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-1179&#39;&#xA;May 29 23:45:50.266: INFO: stderr: &#34;warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n&#34;&#xA;May 29 23:45:50.267: INFO: stdout: &#34;deployment.apps \&#34;redis-master\&#34; force deleted\n&#34;&#xA;�[1mSTEP�[0m: using delete to clean up resources&#xA;May 29 23:45:50.267: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-1179&#39;&#xA;May 29 23:45:50.574: INFO: stderr: &#34;warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n&#34;&#xA;May 29 23:45:50.574: INFO: stdout: &#34;deployment.apps \&#34;redis-slave\&#34; force deleted\n&#34;&#xA;[AfterEach] [sig-cli] Kubectl client&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;May 29 23:45:50.574: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;May 29 23:45:50.617: INFO: Condition Ready of node e2e-test-peterhornyack-windows-node-group-jpxd is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule 2019-05-29 23:44:50 -0700 PDT} {node.kubernetes.io/not-ready  NoExecute 2019-05-29 23:44:53 -0700 PDT}]. Failure&#xA;�[1mSTEP�[0m: Destroying namespace &#34;kubectl-1179&#34; for this suite.&#xA;May 29 23:55:50.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 29 23:55:52.428: INFO: Couldn&#39;t delete ns: &#34;kubectl-1179&#34;: namespace kubectl-1179 was not deleted with limit: timed out waiting for the condition, namespace is empty but is not yet removed (&amp;errors.errorString{s:&#34;namespace kubectl-1179 was not deleted with limit: timed out waiting for the condition, namespace is empty but is not yet removed&#34;})&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should exceed active deadline" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Addon update should propagate add-on file changes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [sig-windows] Networking Granular Checks: Pods should function for node-pod communication: http" classname="Kubernetes e2e suite" time="382.396006577"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on the allowed zones, datastore and storage policy specified in storage class" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify dynamically created pv with allowed zones specified in storage class, shows the right zone information on its labels" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications with PVCs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide container&#39;s memory request [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="16.637448725"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Advanced Audit [DisabledForLargeClusters][Flaky] should audit API calls to get a pod with unauthorized user." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volumes ConfigMap should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="314.47078325">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;Unexpected error:&#xA;    &lt;*errors.errorString | 0xc001d423b0&gt;: {&#xA;        s: &#34;expected pod \&#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3\&#34; success: Gave up after waiting 5m0s for pod \&#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3\&#34; to be \&#34;success or failure\&#34;&#34;,&#xA;    }&#xA;    expected pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34; success: Gave up after waiting 5m0s for pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34; to be &#34;success or failure&#34;&#xA;occurred&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2285</failure>
          <system-out>[BeforeEach] [sig-storage] Projected secret&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 30 00:02:31.464: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename projected&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: Creating secret with name projected-secret-test-13864335-faa0-4222-8010-cdca6a0da112&#xA;�[1mSTEP�[0m: Creating a pod to test consume secrets&#xA;May 30 00:02:31.735: INFO: Waiting up to 5m0s for pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34; in namespace &#34;projected-1213&#34; to be &#34;success or failure&#34;&#xA;May 30 00:02:31.776: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 41.579679ms&#xA;May 30 00:02:33.819: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.083851053s&#xA;May 30 00:02:35.862: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4.126832895s&#xA;May 30 00:02:37.903: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 6.16868728s&#xA;May 30 00:02:39.945: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 8.210688811s&#xA;May 30 00:02:41.987: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 10.252644397s&#xA;May 30 00:02:44.029: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 12.294775868s&#xA;May 30 00:02:46.071: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 14.33675232s&#xA;May 30 00:02:48.114: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 16.379089874s&#xA;May 30 00:02:50.156: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 18.420827506s&#xA;May 30 00:02:52.199: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 20.464017171s&#xA;May 30 00:02:54.241: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 22.505918333s&#xA;May 30 00:02:56.282: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 24.547687638s&#xA;May 30 00:02:58.324: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 26.589568174s&#xA;May 30 00:03:00.366: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 28.63154643s&#xA;May 30 00:03:02.408: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 30.673680478s&#xA;May 30 00:03:04.451: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 32.716528017s&#xA;May 30 00:03:06.493: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 34.758594039s&#xA;May 30 00:03:08.536: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 36.801033406s&#xA;May 30 00:03:10.578: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 38.843038554s&#xA;May 30 00:03:12.621: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 40.88674101s&#xA;May 30 00:03:14.664: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 42.92919223s&#xA;May 30 00:03:16.706: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 44.971386085s&#xA;May 30 00:03:18.748: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 47.013374281s&#xA;May 30 00:03:20.790: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 49.05527132s&#xA;May 30 00:03:22.832: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 51.097288594s&#xA;May 30 00:03:24.874: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 53.138968849s&#xA;May 30 00:03:26.916: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 55.181728275s&#xA;May 30 00:03:28.959: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 57.223882522s&#xA;May 30 00:03:31.001: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 59.266042419s&#xA;May 30 00:03:33.043: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m1.308170556s&#xA;May 30 00:03:35.084: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m3.349426844s&#xA;May 30 00:03:37.126: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m5.391536839s&#xA;May 30 00:03:39.168: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m7.433566551s&#xA;May 30 00:03:41.212: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m9.477232065s&#xA;May 30 00:03:43.254: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m11.519150958s&#xA;May 30 00:03:45.296: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m13.561327077s&#xA;May 30 00:03:47.338: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m15.60310064s&#xA;May 30 00:03:49.380: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m17.645172165s&#xA;May 30 00:03:51.422: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m19.687493178s&#xA;May 30 00:03:53.464: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m21.729136251s&#xA;May 30 00:03:55.506: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m23.771356905s&#xA;May 30 00:03:57.548: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m25.813399615s&#xA;May 30 00:03:59.590: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m27.855558793s&#xA;May 30 00:04:01.633: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m29.898242331s&#xA;May 30 00:04:03.676: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m31.941231909s&#xA;May 30 00:04:05.718: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m33.983137144s&#xA;May 30 00:04:07.760: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m36.024923441s&#xA;May 30 00:04:09.802: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m38.067071629s&#xA;May 30 00:04:11.844: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m40.108849948s&#xA;May 30 00:04:13.886: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m42.151458609s&#xA;May 30 00:04:15.928: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m44.193700094s&#xA;May 30 00:04:17.970: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m46.235437609s&#xA;May 30 00:04:20.012: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m48.277614919s&#xA;May 30 00:04:22.054: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m50.31948362s&#xA;May 30 00:04:24.096: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m52.361552053s&#xA;May 30 00:04:26.144: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m54.408821598s&#xA;May 30 00:04:28.186: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m56.4509838s&#xA;May 30 00:04:30.228: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 1m58.492974998s&#xA;May 30 00:04:32.270: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m0.535182359s&#xA;May 30 00:04:34.312: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m2.577145774s&#xA;May 30 00:04:36.354: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m4.618843683s&#xA;May 30 00:04:38.396: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m6.661104141s&#xA;May 30 00:04:40.438: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m8.703094222s&#xA;May 30 00:04:42.480: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m10.745121339s&#xA;May 30 00:04:44.521: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m12.786818103s&#xA;May 30 00:04:46.563: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m14.828793885s&#xA;May 30 00:04:48.606: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m16.870867483s&#xA;May 30 00:04:50.648: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m18.913346749s&#xA;May 30 00:04:52.690: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m20.955477182s&#xA;May 30 00:04:54.734: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m22.999163388s&#xA;May 30 00:04:56.776: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m25.041293583s&#xA;May 30 00:04:58.818: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m27.083236676s&#xA;May 30 00:05:00.860: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m29.125230199s&#xA;May 30 00:05:02.907: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m31.172090084s&#xA;May 30 00:05:04.949: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m33.21416584s&#xA;May 30 00:05:06.991: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m35.256170837s&#xA;May 30 00:05:09.033: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m37.298186328s&#xA;May 30 00:05:11.076: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m39.341328579s&#xA;May 30 00:05:13.118: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m41.383817087s&#xA;May 30 00:05:15.161: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m43.425996937s&#xA;May 30 00:05:17.202: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m45.467719694s&#xA;May 30 00:05:19.244: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m47.509397268s&#xA;May 30 00:05:21.289: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m49.554015114s&#xA;May 30 00:05:23.340: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m51.605307246s&#xA;May 30 00:05:25.386: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m53.650972381s&#xA;May 30 00:05:27.428: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m55.692883225s&#xA;May 30 00:05:29.469: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m57.73468456s&#xA;May 30 00:05:31.511: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2m59.776460684s&#xA;May 30 00:05:33.553: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m1.818439892s&#xA;May 30 00:05:35.595: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m3.860381208s&#xA;May 30 00:05:37.637: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m5.902332683s&#xA;May 30 00:05:39.679: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m7.944444989s&#xA;May 30 00:05:41.722: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m9.986952357s&#xA;May 30 00:05:43.764: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m12.028990233s&#xA;May 30 00:05:45.806: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m14.071263483s&#xA;May 30 00:05:47.848: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m16.113465853s&#xA;May 30 00:05:49.890: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m18.155742377s&#xA;May 30 00:05:51.933: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m20.198008254s&#xA;May 30 00:05:53.981: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m22.246633763s&#xA;May 30 00:05:56.024: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m24.288849945s&#xA;May 30 00:05:58.066: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m26.330854383s&#xA;May 30 00:06:00.107: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m28.372751091s&#xA;May 30 00:06:02.149: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m30.414597715s&#xA;May 30 00:06:04.194: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m32.458963759s&#xA;May 30 00:06:06.236: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m34.501460973s&#xA;May 30 00:06:08.278: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m36.543442766s&#xA;May 30 00:06:10.320: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m38.584928708s&#xA;May 30 00:06:12.362: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m40.627063581s&#xA;May 30 00:06:14.404: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m42.669345404s&#xA;May 30 00:06:16.453: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m44.718410673s&#xA;May 30 00:06:18.495: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m46.76026353s&#xA;May 30 00:06:20.537: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m48.802066517s&#xA;May 30 00:06:22.579: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m50.844149647s&#xA;May 30 00:06:24.621: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m52.886419918s&#xA;May 30 00:06:26.663: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m54.92876006s&#xA;May 30 00:06:28.705: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m56.970683301s&#xA;May 30 00:06:30.747: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3m59.012551084s&#xA;May 30 00:06:32.789: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m1.054652328s&#xA;May 30 00:06:34.832: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m3.096909902s&#xA;May 30 00:06:36.874: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m5.13901063s&#xA;May 30 00:06:38.916: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m7.18108133s&#xA;May 30 00:06:40.958: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m9.223342509s&#xA;May 30 00:06:43.001: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m11.266382816s&#xA;May 30 00:06:45.043: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m13.308581799s&#xA;May 30 00:06:47.085: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m15.350601557s&#xA;May 30 00:06:49.128: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m17.392840374s&#xA;May 30 00:06:51.170: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m19.435682128s&#xA;May 30 00:06:53.212: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m21.477348398s&#xA;May 30 00:06:55.254: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m23.519099558s&#xA;May 30 00:06:57.296: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m25.561333652s&#xA;May 30 00:06:59.338: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m27.603262203s&#xA;May 30 00:07:01.380: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m29.645549673s&#xA;May 30 00:07:03.422: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m31.687762915s&#xA;May 30 00:07:05.465: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m33.730134714s&#xA;May 30 00:07:07.507: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m35.772252378s&#xA;May 30 00:07:09.549: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m37.81414835s&#xA;May 30 00:07:11.591: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m39.85615237s&#xA;May 30 00:07:13.634: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m41.899752892s&#xA;May 30 00:07:15.677: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m43.942154045s&#xA;May 30 00:07:17.718: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m45.98379725s&#xA;May 30 00:07:19.761: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m48.025955886s&#xA;May 30 00:07:21.802: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m50.067606732s&#xA;May 30 00:07:23.845: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m52.109886892s&#xA;May 30 00:07:25.887: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m54.15192682s&#xA;May 30 00:07:27.929: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m56.193982219s&#xA;May 30 00:07:29.971: INFO: Pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4m58.235927799s&#xA;May 30 00:07:32.061: INFO: Failed to get logs from node &#34;e2e-test-peterhornyack-windows-node-group-jpxd&#34; pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34; container &#34;secret-volume-test&#34;: the server rejected our request for an unknown reason (get pods pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3)&#xA;�[1mSTEP�[0m: delete the pod&#xA;May 30 00:07:32.110: INFO: Waiting for pod pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3 to disappear&#xA;May 30 00:07:32.152: INFO: Pod pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3 still exists&#xA;May 30 00:07:34.153: INFO: Waiting for pod pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3 to disappear&#xA;May 30 00:07:34.195: INFO: Pod pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3 still exists&#xA;May 30 00:07:36.153: INFO: Waiting for pod pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3 to disappear&#xA;May 30 00:07:36.194: INFO: Pod pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3 no longer exists&#xA;May 30 00:07:36.195: INFO: Unexpected error occurred: expected pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34; success: Gave up after waiting 5m0s for pod &#34;pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#34; to be &#34;success or failure&#34;&#xA;[AfterEach] [sig-storage] Projected secret&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;projected-1213&#34;.&#xA;�[1mSTEP�[0m: Found 5 events.&#xA;May 30 00:07:36.237: INFO: At 2019-05-30 00:02:31 -0700 PDT - event for pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3: {default-scheduler } Scheduled: Successfully assigned projected-1213/pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3 to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:07:36.237: INFO: At 2019-05-30 00:02:34 -0700 PDT - event for pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/mounttest:1.0&#34; already present on machine&#xA;May 30 00:07:36.237: INFO: At 2019-05-30 00:02:34 -0700 PDT - event for pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container secret-volume-test&#xA;May 30 00:07:36.237: INFO: At 2019-05-30 00:02:36 -0700 PDT - event for pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container secret-volume-test&#xA;May 30 00:07:36.237: INFO: At 2019-05-30 00:05:53 -0700 PDT - event for pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod projected-1213/pod-projected-secrets-26a8415d-cf38-4e94-8832-f16109066ac3&#xA;May 30 00:07:36.326: INFO: POD                                                    NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 30 00:07:36.326: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:07:36.326: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 30 00:07:36.327: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 30 00:07:36.327: INFO: &#xA;May 30 00:07:36.369: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 30 00:07:36.411: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:75128,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:07:18 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:07:18 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:07:18 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:07:18 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 00:07:36.411: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 30 00:07:36.453: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 30 00:07:36.500: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:07:36.500: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 00:07:36.500: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:07:36.500: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:07:36.500: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:07:36.500: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:07:36.500: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:07:36.500: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:07:36.500: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:07:36.500: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:07:36.500: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 00:07:36.500: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:07:36.500: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:07:36.500: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:07:36.661: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 30 00:07:36.661: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 00:07:36.702: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:75130,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{ReadonlyFilesystem False 2019-05-30 00:06:40 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-30 00:06:40 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-30 00:06:40 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-30 00:06:40 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-30 00:06:40 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-30 00:06:40 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-30 00:06:40 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:07:18 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:07:18 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:07:18 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:07:18 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 00:07:36.703: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 00:07:36.744: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 00:07:36.795: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:07:36.795: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 30 00:07:36.795: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:07:36.795: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 00:07:36.795: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:07:36.795: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:07:36.795: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:07:36.795: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 30 00:07:36.795: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:07:36.795: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 30 00:07:36.795: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:07:36.795: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 30 00:07:36.795: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:07:36.795: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 30 00:07:36.795: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:07:36.795: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:07:36.795: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 30 00:07:36.795: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:07:36.795: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 00:07:36.795: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:07:36.974: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 00:07:36.974: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 00:07:37.016: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:75157,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentContainerdRestart False 2019-05-30 00:07:30 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-30 00:07:30 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-30 00:07:30 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-30 00:07:30 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-30 00:07:30 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-30 00:07:30 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-30 00:07:30 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:06:50 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:06:50 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:06:50 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:06:50 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[nginx@sha256:0fd68ec4b64b8dbb2bef1f1a5de9d47b658afd3635dc9c45bf0cbeac46e72101 nginx:1.15-alpine] 16087791} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 00:07:37.016: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 00:07:37.057: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 00:07:37.107: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:07:37.107: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 00:07:37.107: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:07:37.107: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:07:37.107: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 30 00:07:37.107: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:07:37.107: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 30 00:07:37.107: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 30 00:07:37.107: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:07:37.107: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 30 00:07:37.107: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 30 00:07:37.107: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:07:37.107: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:07:37.107: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 00:07:37.107: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:07:37.271: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 00:07:37.271: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 00:07:37.313: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:75080,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:06:57 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:06:57 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:06:57 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:06:57 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 00:07:37.313: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 00:07:37.354: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 00:07:37.564: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 00:07:37.564: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 00:07:37.609: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:75104,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:07:07 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:07:07 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:07:07 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:07:07 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/gb-redisslave@sha256:5ff9ae76e6abda0b9d7537fc3a5caacffba976e02f1a3bee7c6e83014b1d39d0 e2eteam/gb-redisslave:v3] 4329144223} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 00:07:37.609: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 00:07:37.650: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 00:07:37.856: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 00:07:37.856: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:07:37.898: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:75070,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:06:53 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:06:53 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:06:53 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:06:53 -0700 PDT 2019-05-30 00:05:52 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/gb-redisslave@sha256:5ff9ae76e6abda0b9d7537fc3a5caacffba976e02f1a3bee7c6e83014b1d39d0 e2eteam/gb-redisslave:v3] 4329144223} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[e2eteam/porter@sha256:f1f16595d44d9a06e851d82135a0ae53fbdf512029c6e3301f140a531778c65d e2eteam/porter:1.0] 4284085058} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 00:07:37.898: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:07:37.940: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:07:38.170: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:07:38.171: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;projected-1213&#34; for this suite.&#xA;May 30 00:07:44.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 30 00:07:45.935: INFO: namespace projected-1213 deletion completed in 7.721236749s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with Custom Metric of type Pod from Stackdriver with Prometheus [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don&#39;t match the PodAntiAffinity terms" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001879256">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.001725785">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t scale up when expendable pod is preempted [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should run a job to completion when tasks succeed" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes [Feature:ReclaimPolicy] [sig-storage] persistentvolumereclaim:vsphere should delete persistent volume when reclaimPolicy set to delete and associated claim is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce policy based on NamespaceSelector [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Cluster level logging using Elasticsearch [Feature:Elasticsearch] should check that logs from containers are ingested into Elasticsearch" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="68.579065842"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment deployment should delete old replica sets [Conformance]" classname="Kubernetes e2e suite" time="24.609064217"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="78.110718243"></testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Feature:CustomResourcePublishOpenAPI] works for CRD with validation schema" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.002031989">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Deploy clustered applications [Feature:StatefulSet] [Slow] should creating a working CockroachDB cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets Should fail non-optional pod creation due to secret object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="379.419185789"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] Pods should return to running and ready state after network partition is healed All pods on the unreachable node should be marked as NotReady upon the node turn NotReady AND all pods should be mark back to Ready when the node get back to Ready before pod eviction timeout" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.001730396">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PVC and a pre-bound PV: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="16.40169199"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement test back to back pod creation and deletion with different volume sources on the same worker node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]" classname="Kubernetes e2e suite" time="26.529431588"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Provisioning On Clustered Datastore [Feature:vsphere] verify dynamic provision with default parameter on clustered datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.002037589">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking should provide unchanging, static URL paths for kubernetes api services" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]" classname="Kubernetes e2e suite" time="18.524071722"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for pod-Service: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]" classname="Kubernetes e2e suite" time="16.55486764"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0.00191811">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Node Unregister [Feature:vsphere] [Slow] [Disruptive] node unregister" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="1432.30080633">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/testsuites/volumes.go:136&#xA;Failed to delete pod gcepd-client in namespace volume-8533&#xA;Expected&#xA;    &lt;*errors.errorString | 0xc0026ac8a0&gt;: {&#xA;        s: &#34;pod \&#34;gcepd-client\&#34; was not deleted: timed out waiting for the condition&#34;,&#xA;    }&#xA;to be nil&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/volume/fixtures.go:417</failure>
          <system-out>[BeforeEach] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/testsuites/base.go:76&#xA;[BeforeEach] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 30 00:18:14.678: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename volume&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[It] should be mountable&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/testsuites/volumes.go:136&#xA;�[1mSTEP�[0m: creating a test gce pd volume&#xA;May 30 00:18:18.788: INFO: Successfully created a new PD: &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;.&#xA;May 30 00:18:18.789: INFO: Creating resource for inline volume&#xA;�[1mSTEP�[0m: starting gcepd injector&#xA;May 30 00:18:18.835: INFO: Waiting up to 5m0s for pod &#34;gcepd-injector-457k&#34; in namespace &#34;volume-8533&#34; to be &#34;success or failure&#34;&#xA;May 30 00:18:18.876: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 41.526725ms&#xA;May 30 00:18:20.918: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.083346661s&#xA;May 30 00:18:22.961: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4.126218084s&#xA;May 30 00:18:25.004: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 6.169072524s&#xA;May 30 00:18:27.047: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 8.21215589s&#xA;May 30 00:18:29.089: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 10.254157219s&#xA;May 30 00:18:31.131: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 12.295973553s&#xA;May 30 00:18:33.173: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 14.337914174s&#xA;May 30 00:18:35.215: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 16.380183846s&#xA;May 30 00:18:37.257: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 18.42195241s&#xA;May 30 00:18:39.299: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 20.463980096s&#xA;May 30 00:18:41.341: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 22.505921259s&#xA;May 30 00:18:43.383: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Running&#34;, Reason=&#34;&#34;, readiness=true. Elapsed: 24.547862505s&#xA;May 30 00:18:45.425: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Running&#34;, Reason=&#34;&#34;, readiness=true. Elapsed: 26.589917754s&#xA;May 30 00:18:47.467: INFO: Pod &#34;gcepd-injector-457k&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 28.63186456s&#xA;�[1mSTEP�[0m: Saw pod success&#xA;May 30 00:18:47.467: INFO: Pod &#34;gcepd-injector-457k&#34; satisfied condition &#34;success or failure&#34;&#xA;�[1mSTEP�[0m: starting gcepd-client&#xA;�[1mSTEP�[0m: Checking that text file contents are perfect.&#xA;May 30 00:21:53.649: INFO: Running &#39;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/client/bin/kubectl --server=https://146.148.105.213 --kubeconfig=/usr/local/google/home/peterhornyack/.kube/config exec gcepd-client --namespace=volume-8533 -- powershell /c type /opt/0/index.html&#39;&#xA;May 30 00:21:56.856: INFO: stderr: &#34;&#34;&#xA;May 30 00:21:56.856: INFO: stdout: &#34;Hello from gcepd from namespace volume-8533\r\n&#34;&#xA;�[1mSTEP�[0m: cleaning the environment after gcepd&#xA;May 30 00:21:56.856: INFO: Deleting pod &#34;gcepd-client&#34; in namespace &#34;volume-8533&#34;&#xA;May 30 00:21:56.902: INFO: Wait up to 5m0s for pod &#34;gcepd-client&#34; to be fully deleted&#xA;May 30 00:26:58.665: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:26:58.665: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:04.630: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:04.630: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:10.582: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:10.582: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:16.629: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:16.630: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:22.574: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:22.574: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:28.554: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:28.554: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:34.537: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:34.537: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:40.463: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:40.463: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:46.400: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:46.400: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:52.265: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:52.265: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:58.369: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:27:58.369: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:04.351: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:04.351: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:10.258: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:10.258: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:16.204: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:16.204: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:22.313: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:22.313: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:28.258: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:28.258: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:34.232: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:34.232: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:40.253: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:40.253: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:46.187: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:46.187: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:52.042: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:52.042: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:57.991: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:28:57.991: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:03.880: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:03.880: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:09.836: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:09.836: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:15.774: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:15.774: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:21.708: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:21.708: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:27.575: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:27.575: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:33.626: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:33.626: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:39.877: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:39.877: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:45.800: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:45.800: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:51.771: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:51.771: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:57.730: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:29:57.730: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:03.697: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:03.697: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:09.752: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:09.752: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:15.606: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:15.606: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:21.569: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:21.569: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:27.559: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:27.559: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:33.588: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:33.588: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:39.575: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:39.575: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:45.540: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:45.541: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:51.391: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:51.391: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:57.302: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:30:57.302: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:03.265: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:03.265: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:09.167: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:09.167: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:15.058: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:15.058: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:21.018: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:21.018: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:26.935: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:26.935: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:32.920: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:32.920: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:39.025: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:39.025: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:44.915: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:44.915: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:50.915: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:50.916: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:56.961: INFO: error deleting PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;May 30 00:31:56.961: INFO: Couldn&#39;t delete PD &#34;e2e-5077e179-b499-422a-8431-69b78ceaea56&#34;, sleeping 5s: googleapi: Error 400: The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-jpxd&#39;, resourceInUseByAnotherResource&#xA;[AfterEach] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;volume-8533&#34;.&#xA;�[1mSTEP�[0m: Found 12 events.&#xA;May 30 00:32:02.004: INFO: At 2019-05-30 00:18:18 -0700 PDT - event for gcepd-injector-457k: {default-scheduler } Scheduled: Successfully assigned volume-8533/gcepd-injector-457k to e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 00:32:02.004: INFO: At 2019-05-30 00:18:25 -0700 PDT - event for gcepd-injector-457k: {attachdetach-controller } SuccessfulAttachVolume: AttachVolume.Attach succeeded for volume &#34;gcepd-volume-jjdz&#34; &#xA;May 30 00:32:02.004: INFO: At 2019-05-30 00:18:36 -0700 PDT - event for gcepd-injector-457k: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Pulled: Container image &#34;e2eteam/nettest:1.0&#34; already present on machine&#xA;May 30 00:32:02.004: INFO: At 2019-05-30 00:18:36 -0700 PDT - event for gcepd-injector-457k: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Created: Created container gcepd-injector&#xA;May 30 00:32:02.004: INFO: At 2019-05-30 00:18:38 -0700 PDT - event for gcepd-injector-457k: {kubelet e2e-test-peterhornyack-windows-node-group-9q9v} Started: Started container gcepd-injector&#xA;May 30 00:32:02.004: INFO: At 2019-05-30 00:18:47 -0700 PDT - event for gcepd-client: {default-scheduler } Scheduled: Successfully assigned volume-8533/gcepd-client to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:32:02.004: INFO: At 2019-05-30 00:18:51 -0700 PDT - event for gcepd-client: {attachdetach-controller } FailedAttachVolume: AttachVolume.Attach failed for volume &#34;gcepd-volume-0&#34; : googleapi: Error 400: RESOURCE_IN_USE_BY_ANOTHER_RESOURCE - The disk resource &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/disks/e2e-5077e179-b499-422a-8431-69b78ceaea56&#39; is already being used by &#39;projects/peterhornyack-prod-no-enforcer/zones/us-central1-b/instances/e2e-test-peterhornyack-windows-node-group-9q9v&#39;&#xA;May 30 00:32:02.004: INFO: At 2019-05-30 00:19:04 -0700 PDT - event for gcepd-client: {attachdetach-controller } SuccessfulAttachVolume: AttachVolume.Attach succeeded for volume &#34;gcepd-volume-0&#34; &#xA;May 30 00:32:02.004: INFO: At 2019-05-30 00:19:26 -0700 PDT - event for gcepd-client: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/nettest:1.0&#34; already present on machine&#xA;May 30 00:32:02.004: INFO: At 2019-05-30 00:19:26 -0700 PDT - event for gcepd-client: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container gcepd-client&#xA;May 30 00:32:02.004: INFO: At 2019-05-30 00:19:28 -0700 PDT - event for gcepd-client: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container gcepd-client&#xA;May 30 00:32:02.004: INFO: At 2019-05-30 00:21:58 -0700 PDT - event for gcepd-client: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod volume-8533/gcepd-client&#xA;May 30 00:32:02.091: INFO: POD                                                    NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 30 00:32:02.092: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 30 00:32:02.092: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 30 00:32:02.093: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:32:02.093: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:32:02.093: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:32:02.093: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:32:02.093: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:32:02.093: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:32:02.093: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 30 00:32:02.093: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 30 00:32:02.093: INFO: gcepd-client                                           e2e-test-peterhornyack-windows-node-group-jpxd  Running  1s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 00:18:47 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 00:21:53 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 00:21:53 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 00:18:47 -0700 PDT  }]&#xA;May 30 00:32:02.093: INFO: &#xA;May 30 00:32:02.136: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 30 00:32:02.178: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:78780,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:31:22 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:31:22 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:31:22 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:31:22 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 00:32:02.178: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 30 00:32:02.219: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 30 00:32:02.266: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:32:02.266: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:32:02.266: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:32:02.266: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:32:02.266: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:32:02.266: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:32:02.266: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:32:02.266: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 00:32:02.266: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:32:02.266: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:32:02.266: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:32:02.266: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:32:02.266: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 00:32:02.266: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:32:02.430: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 30 00:32:02.430: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 00:32:02.472: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:78858,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentContainerdRestart False 2019-05-30 00:31:54 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-30 00:31:54 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-30 00:31:54 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-30 00:31:54 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-30 00:31:54 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-30 00:31:54 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-30 00:31:54 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:31:20 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:31:20 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:31:20 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:31:20 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 00:32:02.472: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 00:32:02.520: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 00:32:02.569: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:32:02.569: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 00:32:02.569: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:32:02.569: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:32:02.569: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 30 00:32:02.569: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:32:02.569: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 30 00:32:02.569: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:32:02.569: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:32:02.569: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 30 00:32:02.569: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:32:02.569: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 30 00:32:02.569: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:32:02.569: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 30 00:32:02.569: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:32:02.569: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 00:32:02.569: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:32:02.569: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:32:02.569: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:32:02.569: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 30 00:32:02.730: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 00:32:02.730: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 00:32:02.772: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:78850,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentDockerRestart False 2019-05-30 00:31:42 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-30 00:31:42 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-30 00:31:42 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-30 00:31:42 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-30 00:31:42 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-30 00:31:42 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-30 00:31:42 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:31:52 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:31:52 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:31:52 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:31:52 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[nginx@sha256:0fd68ec4b64b8dbb2bef1f1a5de9d47b658afd3635dc9c45bf0cbeac46e72101 nginx:1.15-alpine] 16087791} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 00:32:02.772: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 00:32:02.815: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 00:32:02.880: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:32:02.880: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:32:02.880: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 00:32:02.880: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:32:02.880: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:32:02.880: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 00:32:02.880: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:32:02.880: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:32:02.880: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 30 00:32:02.880: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:32:02.880: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 30 00:32:02.880: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 30 00:32:02.880: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:32:02.880: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 30 00:32:02.880: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 30 00:32:03.032: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 00:32:03.032: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 00:32:03.074: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:78869,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:31:59 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:31:59 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:31:59 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:31:59 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 00:32:03.074: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 00:32:03.115: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 00:32:03.316: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 00:32:03.316: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 00:32:03.358: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:78844,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:31:49 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:31:49 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:31:49 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:31:49 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/gb-redisslave@sha256:5ff9ae76e6abda0b9d7537fc3a5caacffba976e02f1a3bee7c6e83014b1d39d0 e2eteam/gb-redisslave:v3] 4329144223} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 00:32:03.358: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 00:32:03.400: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 00:32:03.602: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 00:32:03.602: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:32:03.644: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:78859,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:31:55 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:31:55 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:31:55 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:31:55 -0700 PDT 2019-05-30 00:29:55 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/gb-redisslave@sha256:5ff9ae76e6abda0b9d7537fc3a5caacffba976e02f1a3bee7c6e83014b1d39d0 e2eteam/gb-redisslave:v3] 4329144223} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[e2eteam/porter@sha256:f1f16595d44d9a06e851d82135a0ae53fbdf512029c6e3301f140a531778c65d e2eteam/porter:1.0] 4284085058} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[kubernetes.io/gce-pd/e2e-5077e179-b499-422a-8431-69b78ceaea56],VolumesAttached:[{kubernetes.io/gce-pd/e2e-5077e179-b499-422a-8431-69b78ceaea56 /dev/disk/by-id/google-e2e-5077e179-b499-422a-8431-69b78ceaea56}],Config:nil,},}&#xA;May 30 00:32:03.644: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:32:03.685: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:32:03.733: INFO: gcepd-client started at 2019-05-30 00:18:47 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:32:03.734: INFO: &#x9;Container gcepd-client ready: true, restart count 0&#xA;May 30 00:32:05.089: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:32:05.089: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;volume-8533&#34; for this suite.&#xA;May 30 00:42:05.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 30 00:42:06.978: INFO: Couldn&#39;t delete ns: &#34;volume-8533&#34;: namespace volume-8533 was not deleted with limit: timed out waiting for the condition, namespace is empty but is not yet removed (&amp;errors.errorString{s:&#34;namespace volume-8533 was not deleted with limit: timed out waiting for the condition, namespace is empty but is not yet removed&#34;})&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] HA-master [Feature:HAMaster] survive addition/removal replicas same zone [Serial][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] AppArmor load AppArmor profiles should enforce an AppArmor profile" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0.001911014">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (xfs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI mock volume CSI volume limit information using mock driver should report attach limit when limit is bigger than 0 [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set fsGroup for one pod [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] NFSPersistentVolumes[Disruptive][Flaky] when kubelet restarts Should test that a volume mounted to a pod that is force deleted while the kubelet is down unmounts when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide container&#39;s memory limit [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="20.465156388"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets Should fail non-optional pod creation due to the key in the secret object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="220.744941317"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify PVC creation with incompatible storagePolicy and zone combination specified in storage class fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.001886782">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] PrivilegedPod [NodeConformance] should enable privileged commands [LinuxOnly]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]" classname="Kubernetes e2e suite" time="305.140078936">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;gave up waiting for apiservice wardle to come up successfully&#xA;Unexpected error:&#xA;    &lt;*errors.errorString | 0xc0002b5440&gt;: {&#xA;        s: &#34;timed out waiting for the condition&#34;,&#xA;    }&#xA;    timed out waiting for the condition&#xA;occurred&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:389</failure>
          <system-out>[BeforeEach] [sig-api-machinery] Aggregator&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 30 00:46:08.195: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename aggregator&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-api-machinery] Aggregator&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:74&#xA;May 30 00:46:08.397: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: Registering the sample API server.&#xA;May 30 00:46:09.486: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:11.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:13.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:15.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:17.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:19.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:21.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:23.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:25.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:27.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:29.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:31.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:33.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:35.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:37.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:39.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:41.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:43.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:45.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:47.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:49.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:51.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:53.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:55.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:57.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:46:59.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:01.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:03.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:05.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:07.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:09.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:11.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:13.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:15.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:17.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:19.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:21.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:23.531: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:25.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:27.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:29.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:31.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:33.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:35.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:37.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:39.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:41.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:43.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:45.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:47.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:49.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:51.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:53.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:55.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:57.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:47:59.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:01.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:03.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:05.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:07.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:09.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:11.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:13.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:15.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:17.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:19.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:21.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:23.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:25.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:27.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:29.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:31.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:33.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:35.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:37.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:39.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:41.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:43.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:45.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:47.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:49.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:51.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:53.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:55.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:57.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:48:59.535: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:01.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:03.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:05.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:07.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:09.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:11.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:13.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:15.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:17.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:19.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:21.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:23.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:25.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:27.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:29.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:31.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:33.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:35.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:37.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:39.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:41.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:43.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:45.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:47.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:49.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:51.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:53.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:55.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:57.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:49:59.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:&#34;Available&#34;, Status:&#34;False&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;MinimumReplicasUnavailable&#34;, Message:&#34;Deployment does not have minimum availability.&#34;}, v1.DeploymentCondition{Type:&#34;Progressing&#34;, Status:&#34;True&#34;, LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694799169, loc:(*time.Location)(0x8175900)}}, Reason:&#34;ReplicaSetUpdated&#34;, Message:&#34;ReplicaSet \&#34;sample-apiserver-deployment-7f7649696\&#34; is progressing.&#34;}}, CollisionCount:(*int32)(nil)}&#xA;May 30 00:51:01.827: INFO: Waited 1m0.250621231s for the sample-apiserver to be ready to handle requests.&#xA;May 30 00:51:01.827: INFO: current APIService: {&#34;metadata&#34;:{&#34;name&#34;:&#34;v1alpha1.wardle.k8s.io&#34;,&#34;selfLink&#34;:&#34;/apis/apiregistration.k8s.io/v1beta1/apiservices/v1alpha1.wardle.k8s.io&#34;,&#34;uid&#34;:&#34;b25ac719-9b6c-4c37-9b47-ca93c8899305&#34;,&#34;resourceVersion&#34;:&#34;81506&#34;,&#34;creationTimestamp&#34;:&#34;2019-05-30T07:50:01Z&#34;},&#34;spec&#34;:{&#34;service&#34;:{&#34;namespace&#34;:&#34;aggregator-4408&#34;,&#34;name&#34;:&#34;sample-api&#34;},&#34;group&#34;:&#34;wardle.k8s.io&#34;,&#34;version&#34;:&#34;v1alpha1&#34;,&#34;caBundle&#34;:&#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMyRENDQWNDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFkTVJzd0dRWURWUVFERXhKbE1tVXQKYzJWeWRtVnlMV05sY25RdFkyRXdIaGNOTVRrd05UTXdNRGMwTmpBNFdoY05Namt3TlRJM01EYzBOakE0V2pBZApNUnN3R1FZRFZRUURFeEpsTW1VdGMyVnlkbVZ5TFdObGNuUXRZMkV3Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBCkE0SUJEd0F3Z2dFS0FvSUJBUUNzK0gra1ZSNmFvZ1ZFck9Vd3MzQlk1MDNmUU5PR21tVUttMW5lbElpRmlhUWQKKzFQK3JaaUJybEJpS2FBWlh3a0ZsWk9nZWtydmJ5R0JlVUlkK0Y5QjZwOXU0aTZzTFVFVnU1dDVlU3Q2NTRYUwpVWWpET05DTmVUbXpWUm91KzMwRVFyVlZod2RpNExsbkVGWTkxdkcwcXBQNEFpUEt2RVZranZETGJNWTZpVlZmCko1ZnhRYUJBQTlNSjNmbjFoSmcxM2MxRjVZOVlYRENSanYwRGRTNkZtSGVNci95ZGEzUFRRYm0zN3ZoeHE0cnMKUnQ4RnN6dUU4UE5mQmROSVcwb0VvaWlGMDd4bisrOXZYZmF6Nld6SEg0VXM2UjBuTUVUeTJjVCtHaXlTb09xLwpIYlBxR3lEU2twRFJqZVFIYVBXOTJwb0FrRWI0Vm1FU212K0FyajA3QWdNQkFBR2pJekFoTUE0R0ExVWREd0VCCi93UUVBd0lDcERBUEJnTlZIUk1CQWY4RUJUQURBUUgvTUEwR0NTcUdTSWIzRFFFQkN3VUFBNElCQVFCMlJNNncKUnRQY2NNeExpYjYveGhrQmNTMXNlbnJjVEdRc1F4cWkxSUh1dEI4QUFFaHVSZHp6ZUZkRlNkUTUrWGpKTlp3MQpFM1RNeVlFcGRRQ0NyTGpQUGFmTVRIcWo1aW1YZXJZUUg1N1ZwMXcxSERpdFhqRzFKNDlCVXZFdkhrbGQra0ltClNERWFzMWRYSm5tUEo2SVliN0dHMTUwN2Q2YjVFNGVQeEJMVWdMK1daQmRqcEN6RUFHaGZmUmZtd3BsTU5SYWYKL0lTUFJ1eDNLYXYycXAxSGV3R1UvTFEzRjlHU3g3NFVZRXJONWtpU04vd3JDdVozSFlMNy9pdlhldUZWamtLaApHYmQ2Vm1NMnRqNFdBcE9BWjliVnNncWJpbE0xM1F2bGY0Vm1OSGVDRmwvNGgzWUxPbTkxNkZNamFRMHdrTlUzCkplbnZ3ZFI3TS9xYmRmUWYKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=&#34;,&#34;groupPriorityMinimum&#34;:2000,&#34;versionPriority&#34;:200},&#34;status&#34;:{&#34;conditions&#34;:[{&#34;type&#34;:&#34;Available&#34;,&#34;status&#34;:&#34;False&#34;,&#34;lastTransitionTime&#34;:&#34;2019-05-30T07:50:01Z&#34;,&#34;reason&#34;:&#34;ServicePortError&#34;,&#34;message&#34;:&#34;service/sample-api in \&#34;aggregator-4408\&#34; is not listening on port 443&#34;}]}}&#xA;May 30 00:51:01.827: INFO: current pods: {&#34;metadata&#34;:{&#34;selfLink&#34;:&#34;/api/v1/namespaces/aggregator-4408/pods&#34;,&#34;resourceVersion&#34;:&#34;81648&#34;},&#34;items&#34;:[{&#34;metadata&#34;:{&#34;name&#34;:&#34;sample-apiserver-deployment-7f7649696-c5hg4&#34;,&#34;generateName&#34;:&#34;sample-apiserver-deployment-7f7649696-&#34;,&#34;namespace&#34;:&#34;aggregator-4408&#34;,&#34;selfLink&#34;:&#34;/api/v1/namespaces/aggregator-4408/pods/sample-apiserver-deployment-7f7649696-c5hg4&#34;,&#34;uid&#34;:&#34;047962f1-73ed-427e-860a-3349c59e4d0e&#34;,&#34;resourceVersion&#34;:&#34;81501&#34;,&#34;creationTimestamp&#34;:&#34;2019-05-30T07:46:09Z&#34;,&#34;labels&#34;:{&#34;apiserver&#34;:&#34;true&#34;,&#34;app&#34;:&#34;sample-apiserver&#34;,&#34;pod-template-hash&#34;:&#34;7f7649696&#34;},&#34;ownerReferences&#34;:[{&#34;apiVersion&#34;:&#34;apps/v1&#34;,&#34;kind&#34;:&#34;ReplicaSet&#34;,&#34;name&#34;:&#34;sample-apiserver-deployment-7f7649696&#34;,&#34;uid&#34;:&#34;3a75d097-eb20-4981-9277-cd9b8e751eb9&#34;,&#34;controller&#34;:true,&#34;blockOwnerDeletion&#34;:true}]},&#34;spec&#34;:{&#34;volumes&#34;:[{&#34;name&#34;:&#34;apiserver-certs&#34;,&#34;secret&#34;:{&#34;secretName&#34;:&#34;sample-apiserver-secret&#34;,&#34;defaultMode&#34;:420}},{&#34;name&#34;:&#34;default-token-2grpw&#34;,&#34;secret&#34;:{&#34;secretName&#34;:&#34;default-token-2grpw&#34;,&#34;defaultMode&#34;:420}}],&#34;containers&#34;:[{&#34;name&#34;:&#34;sample-apiserver&#34;,&#34;image&#34;:&#34;e2eteam/sample-apiserver:1.10&#34;,&#34;args&#34;:[&#34;--etcd-servers=http://localhost:2379&#34;,&#34;--tls-cert-file=/apiserver.local.config/certificates/tls.crt&#34;,&#34;--tls-private-key-file=/apiserver.local.config/certificates/tls.key&#34;,&#34;--audit-log-path=-&#34;,&#34;--audit-log-maxage=0&#34;,&#34;--audit-log-maxbackup=0&#34;],&#34;resources&#34;:{},&#34;volumeMounts&#34;:[{&#34;name&#34;:&#34;apiserver-certs&#34;,&#34;readOnly&#34;:true,&#34;mountPath&#34;:&#34;/apiserver.local.config/certificates&#34;},{&#34;name&#34;:&#34;default-token-2grpw&#34;,&#34;readOnly&#34;:true,&#34;mountPath&#34;:&#34;/var/run/secrets/kubernetes.io/serviceaccount&#34;}],&#34;terminationMessagePath&#34;:&#34;/dev/termination-log&#34;,&#34;terminationMessagePolicy&#34;:&#34;File&#34;,&#34;imagePullPolicy&#34;:&#34;IfNotPresent&#34;},{&#34;name&#34;:&#34;etcd&#34;,&#34;image&#34;:&#34;e2eteam/etcd:3.3.10&#34;,&#34;command&#34;:[&#34;/usr/local/bin/etcd&#34;],&#34;resources&#34;:{},&#34;volumeMounts&#34;:[{&#34;name&#34;:&#34;default-token-2grpw&#34;,&#34;readOnly&#34;:true,&#34;mountPath&#34;:&#34;/var/run/secrets/kubernetes.io/serviceaccount&#34;}],&#34;terminationMessagePath&#34;:&#34;/dev/termination-log&#34;,&#34;terminationMessagePolicy&#34;:&#34;File&#34;,&#34;imagePullPolicy&#34;:&#34;IfNotPresent&#34;}],&#34;restartPolicy&#34;:&#34;Always&#34;,&#34;terminationGracePeriodSeconds&#34;:0,&#34;dnsPolicy&#34;:&#34;ClusterFirst&#34;,&#34;serviceAccountName&#34;:&#34;default&#34;,&#34;serviceAccount&#34;:&#34;default&#34;,&#34;nodeName&#34;:&#34;e2e-test-peterhornyack-windows-node-group-jpxd&#34;,&#34;securityContext&#34;:{},&#34;schedulerName&#34;:&#34;default-scheduler&#34;,&#34;tolerations&#34;:[{&#34;key&#34;:&#34;node.kubernetes.io/not-ready&#34;,&#34;operator&#34;:&#34;Exists&#34;,&#34;effect&#34;:&#34;NoExecute&#34;,&#34;tolerationSeconds&#34;:300},{&#34;key&#34;:&#34;node.kubernetes.io/unreachable&#34;,&#34;operator&#34;:&#34;Exists&#34;,&#34;effect&#34;:&#34;NoExecute&#34;,&#34;tolerationSeconds&#34;:300}],&#34;priority&#34;:0,&#34;enableServiceLinks&#34;:true},&#34;status&#34;:{&#34;phase&#34;:&#34;Running&#34;,&#34;conditions&#34;:[{&#34;type&#34;:&#34;Initialized&#34;,&#34;status&#34;:&#34;True&#34;,&#34;lastProbeTime&#34;:null,&#34;lastTransitionTime&#34;:&#34;2019-05-30T07:46:09Z&#34;},{&#34;type&#34;:&#34;Ready&#34;,&#34;status&#34;:&#34;True&#34;,&#34;lastProbeTime&#34;:null,&#34;lastTransitionTime&#34;:&#34;2019-05-30T07:50:01Z&#34;},{&#34;type&#34;:&#34;ContainersReady&#34;,&#34;status&#34;:&#34;True&#34;,&#34;lastProbeTime&#34;:null,&#34;lastTransitionTime&#34;:&#34;2019-05-30T07:50:01Z&#34;},{&#34;type&#34;:&#34;PodScheduled&#34;,&#34;status&#34;:&#34;True&#34;,&#34;lastProbeTime&#34;:null,&#34;lastTransitionTime&#34;:&#34;2019-05-30T07:46:09Z&#34;}],&#34;hostIP&#34;:&#34;10.40.0.3&#34;,&#34;podIP&#34;:&#34;10.64.1.130&#34;,&#34;startTime&#34;:&#34;2019-05-30T07:46:09Z&#34;,&#34;containerStatuses&#34;:[{&#34;name&#34;:&#34;etcd&#34;,&#34;state&#34;:{&#34;running&#34;:{&#34;startedAt&#34;:&#34;2019-05-30T07:46:25Z&#34;}},&#34;lastState&#34;:{},&#34;ready&#34;:true,&#34;restartCount&#34;:0,&#34;image&#34;:&#34;e2eteam/etcd:3.3.10&#34;,&#34;imageID&#34;:&#34;docker-pullable://e2eteam/etcd@sha256:462024728f8360157455091329616d9b55d4718533b0812b230d612a5bc5ce30&#34;,&#34;containerID&#34;:&#34;docker://e5f36e625a5b19db98a771d40b079cf2566f722b9f02398d2ecbb6a264e4349a&#34;},{&#34;name&#34;:&#34;sample-apiserver&#34;,&#34;state&#34;:{&#34;running&#34;:{&#34;startedAt&#34;:&#34;2019-05-30T07:46:16Z&#34;}},&#34;lastState&#34;:{},&#34;ready&#34;:true,&#34;restartCount&#34;:0,&#34;image&#34;:&#34;e2eteam/sample-apiserver:1.10&#34;,&#34;imageID&#34;:&#34;docker-pullable://e2eteam/sample-apiserver@sha256:a11a8656e8013ce21acd0074574f8ac9deaccc3b99486c7d59f764fe4d4f2d55&#34;,&#34;containerID&#34;:&#34;docker://323ec8774663bd169c5cd7fd0c8eb1a2e6114691466b2c0a5c12cd294ccc8d2c&#34;}],&#34;qosClass&#34;:&#34;BestEffort&#34;}}]}&#xA;May 30 00:51:01.906: INFO: logs of sample-apiserver-deployment-7f7649696-c5hg4/sample-apiserver (error: &lt;nil&gt;): I0530 07:46:20.188077    8676 plugins.go:149] Loaded 3 admission controller(s) successfully in the following order: NamespaceLifecycle,MutatingAdmissionWebhook,ValidatingAdmissionWebhook.&#xA;I0530 07:46:27.185735    8676 serve.go:116] Serving securely on [::]:443&#xA;E0530 07:46:27.396681    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:27.397658    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:27.397658    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:28.402472    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:28.405428    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:28.406362    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:29.410239    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:29.411218    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:29.412192    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:30.417976    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:30.418947    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:30.430675    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:31.422851    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:31.425739    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:31.436475    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:32.429598    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:32.432535    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:32.442309    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:33.436379    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:33.436379    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:33.448091    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:34.442184    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:34.443170    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:34.452963    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:35.448996    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:35.449981    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:35.456783    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:36.455782    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:36.456739    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:36.461635    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:37.487430    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:37.495259    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:37.495259    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:38.494228    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:38.534245    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:38.559683    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:39.500995    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:39.541070    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:39.565444    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:40.508796    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:40.547858    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:40.572235    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:41.515131    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:41.578533    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:41.598630    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:42.520849    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:42.585311    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:42.606835    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:43.527863    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:43.592342    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:43.612894    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:44.533096    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:44.598515    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:44.620030    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:45.537771    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:45.610050    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:45.625218    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:46.556633    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:46.617202    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:46.631245    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:47.562325    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:47.622917    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:47.635563    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:48.568480    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:48.629049    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:48.642783    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:49.574141    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:49.634726    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:49.649367    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:50.579870    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:50.640470    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:50.654689    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:51.585989    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:51.647535    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:51.660246    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:52.593121    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:52.653707    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:52.665426    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:53.599335    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:53.660835    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:53.679422    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:54.606418    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:54.667952    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:54.686393    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:55.613565    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:55.674128    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:55.694781    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:56.631911    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:56.680774    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:56.701546    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:57.649323    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:57.686422    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:57.707102    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:58.668171    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:58.692583    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:58.713245    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:59.673932    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:46:59.702191    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:46:59.718876    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:00.687833    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:00.709283    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:00.723941    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:01.694943    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:01.717407    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:01.729161    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:02.702080    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:02.724554    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:02.736235    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:03.721483    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:03.731162    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:03.742878    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:04.731982    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:04.737792    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:04.749558    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:05.755190    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:05.756130    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:05.756130    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:06.762795    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:06.763830    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:06.764752    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:07.769382    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:07.770411    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:07.772381    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:08.865926    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:08.865926    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:08.867875    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:09.875905    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:09.880804    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:09.880804    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:10.884015    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:10.886949    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:10.888042    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:11.890633    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:11.892577    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:11.895511    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:12.897725    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:12.900684    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:12.902662    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:13.908507    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:13.908507    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:13.909238    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:14.913915    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:14.915860    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:14.916845    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:15.935956    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:15.935956    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:15.935956    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:16.941342    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:16.943272    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:16.945216    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:17.949396    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:17.950363    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:17.951390    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:18.956504    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:18.957460    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:18.959427    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:19.975369    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:19.976841    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:19.976841    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:20.982940    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:20.984847    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:20.984847    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:21.988541    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:21.991475    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:21.994418    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:22.995177    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:22.996111    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:22.998108    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:24.002244    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:24.005166    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:24.006160    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:25.007374    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:25.011288    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:25.014213    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:26.014509    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:26.016445    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:26.019369    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:27.028550    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:27.028550    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:27.029393    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:28.044810    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:28.045766    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:28.057469    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:29.060695    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:29.062226    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:29.062226    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:30.068236    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:30.069225    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:30.070195    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:31.076300    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:31.077284    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:31.084118    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:32.082926    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:32.083874    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:32.088796    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:33.089032    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:33.089996    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:33.092952    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:34.096105    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:34.097079    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:34.100000    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:35.113949    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:35.113949    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:35.114912    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:36.133215    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:36.133215    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:36.133215    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:37.143233    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:37.143233    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:37.143233    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:38.164243    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:38.164243    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:38.165449    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:39.175953    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:39.175953    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:39.175953    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:40.182080    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:40.183064    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:40.184018    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:41.188667    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:41.189667    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:41.191589    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:42.195314    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:42.197164    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:42.197164    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:43.201311    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:43.203299    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:43.204230    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:44.209392    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:44.209392    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:44.212294    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:45.216430    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:45.217409    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:45.219353    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:46.222559    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:46.226439    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:46.227444    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:47.228692    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:47.232570    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:47.233545    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:48.234228    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:48.237161    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:48.240131    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:49.241284    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:49.243262    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:49.244221    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:50.249821    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:50.249821    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:50.250796    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:51.255451    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:51.257376    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:51.259322    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:52.263468    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:52.263468    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:52.266383    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:53.269545    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:53.270555    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:53.273549    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:54.277595    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:54.277595    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:54.277595    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:55.282200    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:55.283252    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:55.286102    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:56.286826    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:56.287808    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:56.290727    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:57.294859    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:57.296830    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:57.297777    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:58.301432    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:58.302404    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:58.303387    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:47:59.309455    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:59.310479    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:47:59.311470    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:00.317516    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:00.317516    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:00.317516    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:01.431023    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:01.431023    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:01.431023    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:02.438084    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:02.439068    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:02.440043    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:03.504719    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:03.508631    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:03.508631    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:04.514214    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:04.514214    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:04.514214    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:05.536394    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:05.536394    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:05.536394    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:06.556826    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:06.558154    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:06.558595    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:07.574416    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:07.575398    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:07.575398    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:08.605895    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:08.605895    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:08.610789    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:09.612953    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:09.613926    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:09.616827    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:10.621921    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:10.626824    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:10.629756    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:11.628474    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:11.633390    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:11.634339    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:12.635505    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:12.639440    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:12.642390    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:13.656060    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:13.657253    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:13.657253    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:14.663776    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:14.663776    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:14.664751    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:15.672765    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:15.673750    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:15.692302    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:16.678818    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:16.679850    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:16.699325    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:17.684901    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:17.686832    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:17.705418    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:18.691905    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:18.691905    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:18.712421    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:19.697992    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:19.698946    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:19.721410    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:20.710863    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:20.710863    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:20.727647    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:21.722283    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:21.722283    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:21.735688    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:22.730338    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:22.730338    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:22.741055    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:23.741744    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:23.741744    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:23.746752    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:24.763114    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:24.763114    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:24.764414    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:25.769951    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:25.770995    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:25.771927    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:26.777977    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:26.777977    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:26.778956    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:27.784008    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:27.784991    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:27.786976    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:28.789604    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:28.791554    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:28.793502    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:29.801511    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:29.803437    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:29.803437    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:30.806075    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:30.808034    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:30.808999    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:31.813100    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:31.814076    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:31.815042    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:32.819656    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:32.820623    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:32.822550    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:33.825723    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:33.827630    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:33.828657    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:34.830755    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:34.832705    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:34.832705    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:35.837770    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:35.840724    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:35.841689    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:36.848220    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:36.848220    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:36.849275    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:37.856178    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:37.856178    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:37.856178    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:38.860802    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:38.862748    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:38.863740    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:39.867770    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:39.867770    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:39.870739    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:40.873832    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:40.875815    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:40.876760    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:41.881820    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:41.881820    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:41.881820    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:42.886876    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:42.887881    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:42.890789    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:43.894889    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:43.894889    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:43.895864    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:44.902391    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:44.902870    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:44.903826    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:45.909363    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:45.909363    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:45.910413    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:46.914925    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:46.915910    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:46.916864    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:47.921457    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:47.922434    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:47.922434    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:48.926038    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:48.928966    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:48.929925    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:49.930620    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:49.933520    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:49.963800    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:50.937575    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:50.937575    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:50.969806    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:51.944583    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:51.945601    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:51.975902    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:52.951641    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:52.952596    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:52.982838    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:53.957603    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:53.960540    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:53.989907    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:54.963643    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:54.965598    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:54.995866    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:55.969186    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:55.970169    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:56.002381    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:56.975276    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:56.976230    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:57.008948    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:57.981761    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:57.983670    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:58.015918    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:58.986763    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:58.988700    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:48:59.021945    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:59.992800    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:48:59.994776    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:00.028938    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:01.000879    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:01.000879    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:01.034947    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:02.006285    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:02.007323    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:02.055621    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:03.013333    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:03.013333    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:03.063748    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:04.018875    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:04.021752    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:04.071677    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:05.038063    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:05.038063    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:05.076742    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:06.044530    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:06.045499    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:06.084107    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:07.049085    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:07.051029    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:07.090105    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:08.057063    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:08.069044    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:08.130310    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:09.064052    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:09.073819    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:09.135374    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:10.070107    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:10.078900    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:10.141398    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:11.084860    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:11.090742    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:11.148347    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:12.091850    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:12.099655    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:12.155347    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:13.097370    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:13.112026    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:13.160887    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:14.103472    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:14.118228    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:14.167834    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:15.109414    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:15.125026    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:15.173867    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:16.116364    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:16.133004    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:16.179860    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:17.122438    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:17.139024    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:17.185877    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:18.127907    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:18.144510    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:18.192362    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:19.141238    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:19.150033    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:19.198855    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:20.150683    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:20.153620    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:20.204471    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:21.164490    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:21.164490    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:21.211386    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:22.169522    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:22.172450    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:22.218333    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:23.175542    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:23.177509    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:23.225330    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:24.181041    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:24.183003    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:24.229512    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:25.187023    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:25.187996    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:25.234996    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:26.193998    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:26.193998    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:26.242077    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:27.200011    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:27.201992    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:27.248071    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:28.206032    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:28.207017    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:28.253735    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:29.212053    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:29.214005    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:29.257924    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:30.218025    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:30.218975    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:30.263937    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:31.224995    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:31.224995    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:31.270890    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:32.231936    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:32.232927    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:32.276893    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:33.238939    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:33.240977    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:33.282876    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:34.245915    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:34.245915    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:34.287904    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:35.252878    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:35.253849    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:35.330032    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:36.271562    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:36.271562    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:36.336017    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:37.278565    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:37.278565    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:37.342991    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:38.285507    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:38.285507    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:38.347998    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:39.291501    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:39.291501    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:39.355022    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:40.297477    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:40.297477    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:40.361004    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:41.303953    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:41.303953    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:41.366469    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:42.311897    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:42.312921    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:42.488693    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:43.319870    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:43.319870    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:43.493701    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:44.325832    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:44.327808    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:44.498717    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:45.332802    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:45.334753    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:45.503731    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:46.337842    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:46.351489    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:46.525332    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:47.343808    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:47.356514    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:47.532338    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:48.349791    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:48.364424    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:48.539250    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:49.356753    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:49.371402    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:49.545296    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:50.362721    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:50.376482    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:50.552231    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:51.368714    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:51.383400    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:51.558241    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:52.374687    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:52.389360    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:52.564210    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:53.381687    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:53.395325    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:53.570187    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:54.388620    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:54.402263    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:54.576144    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:55.394613    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:55.410238    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:55.582108    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:56.400571    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:56.422056    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:56.589089    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:57.405615    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:57.430070    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:57.595079    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:58.413546    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:58.436961    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:58.602019    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:59.418490    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:49:59.443892    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:49:59.608944    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:00.424532    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:00.450893    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:00.614985    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:01.436315    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:01.457805    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:01.680132    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:02.505771    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:02.525344    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:02.756302    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:03.511772    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:03.532254    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:03.769409    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:04.518216    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:04.537780    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:04.775082    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:05.525206    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:05.544692    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:05.782033    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:06.532164    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:06.550676    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:06.788973    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:07.551755    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:07.556652    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:07.794460    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:08.573745    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:08.573745    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:08.802870    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:09.594396    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:09.594396    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:09.809812    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:10.600439    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:10.600439    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:10.815381    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:11.604936    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:11.605924    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:11.820789    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:12.611908    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:12.612865    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:12.826767    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:13.619805    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:13.621789    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:13.832758    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:14.625811    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:14.627782    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:14.842629    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:15.650851    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:15.650851    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:15.856409    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:16.665632    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:16.665632    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:16.866138    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:17.673061    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:17.675441    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:17.870472    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:18.685918    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:18.687358    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:18.877021    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:19.705799    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:19.705799    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:19.881390    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:20.715328    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:20.717238    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:20.909683    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:21.721276    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:21.721276    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:21.914658    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:22.727245    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:22.728193    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:22.922570    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:23.732165    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:23.734156    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:23.931474    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:24.736810    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:24.740141    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:24.936431    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:25.743179    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:25.745130    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:25.941408    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:26.750077    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:26.751589    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:26.949358    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:27.755563    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:27.755563    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:27.960655    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:28.762524    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:28.763499    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:28.967657    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:29.768438    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:29.771376    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:29.976498    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:30.787136    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:30.789066    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:30.982471    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:31.807738    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:31.807738    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:31.988420    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:32.836691    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:32.836691    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:32.994392    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:33.843089    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:33.843089    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:34.000327    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:34.848537    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:34.849515    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:35.005779    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:35.856450    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:35.857424    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:36.012710    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:36.862412    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:36.863379    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:37.018655    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:37.869371    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:37.871294    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:38.024620    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:38.873845    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:38.874824    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:39.030584    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:39.880751    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:39.881748    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:40.044807    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:40.886753    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:40.888712    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:41.049307    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:41.893632    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:41.893632    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:42.055280    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:42.900083    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:42.901068    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:43.060262    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:43.909931    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:43.910909    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:44.066191    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:44.916892    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:44.917852    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:45.073109    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:45.923798    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:45.923798    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:46.082014    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:46.929252    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:46.930212    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:47.088430    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:47.935758    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:47.936797    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:48.093225    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:48.942220    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:48.943158    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:49.110002    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:49.948143    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:49.951196    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:50.116593    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:50.955569    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:50.957444    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:51.129813    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:51.962467    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:51.964583    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:52.135305    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:52.969359    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:52.970350    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:53.141228    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:53.975303    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:53.977231    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:54.148208    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:54.985656    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:54.987571    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:55.166307    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:56.005010    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:56.005010    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:56.179604    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:57.021335    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:57.021448    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:57.186971    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:58.026883    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:58.029810    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:58.192408    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:59.033311    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:50:59.035263    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:50:59.271412    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:51:00.040303    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:51:00.042205    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:51:00.277557    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:51:01.056011    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.ValidatingWebhookConfiguration: unknown (get validatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;E0530 07:51:01.056011    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1.Namespace: unknown (get namespaces)&#xA;E0530 07:51:01.283508    8676 reflector.go:322] k8s.io/sample-apiserver/vendor/k8s.io/client-go/informers/factory.go:87: Failed to watch *v1beta1.MutatingWebhookConfiguration: unknown (get mutatingwebhookconfigurations.admissionregistration.k8s.io)&#xA;&#xA;May 30 00:51:01.969: INFO: logs of sample-apiserver-deployment-7f7649696-c5hg4/etcd (error: &lt;nil&gt;): 2019-05-30 07:46:25.886990 I | etcdmain: etcd Version: 3.3.10&#xA;2019-05-30 07:46:25.889916 I | etcdmain: Git SHA: 27fc7e2&#xA;2019-05-30 07:46:25.889916 I | etcdmain: Go Version: go1.10.4&#xA;2019-05-30 07:46:25.889916 I | etcdmain: Go OS/Arch: windows/amd64&#xA;2019-05-30 07:46:25.889916 I | etcdmain: setting maximum number of CPUs to 2, total number of available CPUs is 2&#xA;2019-05-30 07:46:25.889916 N | etcdmain: failed to detect default host (default host not supported on windows_amd64)&#xA;2019-05-30 07:46:25.889916 W | etcdmain: no data-dir provided, using default data-dir ./default.etcd&#xA;2019-05-30 07:46:25.904567 I | embed: listening for peers on http://localhost:2380&#xA;2019-05-30 07:46:25.910426 I | embed: listening for client requests on localhost:2379&#xA;2019-05-30 07:46:25.928009 I | etcdserver: name = default&#xA;2019-05-30 07:46:25.928009 I | etcdserver: data dir = default.etcd&#xA;2019-05-30 07:46:25.928009 I | etcdserver: member dir = default.etcd\member&#xA;2019-05-30 07:46:25.928009 I | etcdserver: heartbeat = 100ms&#xA;2019-05-30 07:46:25.928009 I | etcdserver: election = 1000ms&#xA;2019-05-30 07:46:25.928009 I | etcdserver: snapshot count = 100000&#xA;2019-05-30 07:46:25.928009 I | etcdserver: advertise client URLs = http://localhost:2379&#xA;2019-05-30 07:46:25.928009 I | etcdserver: initial advertise peer URLs = http://localhost:2380&#xA;2019-05-30 07:46:25.928009 I | etcdserver: initial cluster = default=http://localhost:2380&#xA;2019-05-30 07:46:25.939716 I | wal: releasing file lock to rename &#34;default.etcd\\member\\wal.tmp&#34; to &#34;default.etcd\\member\\wal&#34;&#xA;2019-05-30 07:46:25.955508 I | etcdserver: starting member 8e9e05c52164694d in cluster cdf818194e3a8c32&#xA;2019-05-30 07:46:25.955508 I | raft: 8e9e05c52164694d became follower at term 0&#xA;2019-05-30 07:46:25.955508 I | raft: newRaft 8e9e05c52164694d [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]&#xA;2019-05-30 07:46:25.955508 I | raft: 8e9e05c52164694d became follower at term 1&#xA;2019-05-30 07:46:25.989047 W | auth: simple token is not cryptographically signed&#xA;2019-05-30 07:46:26.016384 I | etcdserver: starting server... [version: 3.3.10, cluster version: to_be_decided]&#xA;2019-05-30 07:46:26.048599 I | etcdserver: 8e9e05c52164694d as single-node; fast-forwarding 9 ticks (election ticks 10)&#xA;2019-05-30 07:46:26.050554 E | etcdserver: cannot monitor file descriptor usage (cannot get FDUsage on windows)&#xA;2019-05-30 07:46:26.051532 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32&#xA;2019-05-30 07:46:26.058387 I | raft: 8e9e05c52164694d is starting a new election at term 1&#xA;2019-05-30 07:46:26.058387 I | raft: 8e9e05c52164694d became candidate at term 2&#xA;2019-05-30 07:46:26.058387 I | raft: 8e9e05c52164694d received MsgVoteResp from 8e9e05c52164694d at term 2&#xA;2019-05-30 07:46:26.058387 I | raft: 8e9e05c52164694d became leader at term 2&#xA;2019-05-30 07:46:26.058387 I | raft: raft.node: 8e9e05c52164694d elected leader 8e9e05c52164694d at term 2&#xA;2019-05-30 07:46:26.060327 I | etcdserver: published {Name:default ClientURLs:[http://localhost:2379]} to cluster cdf818194e3a8c32&#xA;2019-05-30 07:46:26.060327 I | etcdserver: setting up the initial cluster version to 3.3&#xA;2019-05-30 07:46:26.060327 I | embed: ready to serve client requests&#xA;2019-05-30 07:46:26.062266 N | embed: serving insecure client requests on 127.0.0.1:2379, this is strongly discouraged!&#xA;2019-05-30 07:46:26.068152 N | etcdserver/membership: set the initial cluster version to 3.3&#xA;2019-05-30 07:46:26.068152 I | etcdserver/api: enabled capabilities for version 3.3&#xA;&#xA;May 30 00:51:01.969: INFO: Unexpected error occurred: timed out waiting for the condition&#xA;[AfterEach] [sig-api-machinery] Aggregator&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:65&#xA;[AfterEach] [sig-api-machinery] Aggregator&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;aggregator-4408&#34;.&#xA;�[1mSTEP�[0m: Found 13 events.&#xA;May 30 00:51:03.625: INFO: At 2019-05-30 00:46:09 -0700 PDT - event for sample-apiserver-deployment: {deployment-controller } ScalingReplicaSet: Scaled up replica set sample-apiserver-deployment-7f7649696 to 1&#xA;May 30 00:51:03.625: INFO: At 2019-05-30 00:46:09 -0700 PDT - event for sample-apiserver-deployment-7f7649696: {replicaset-controller } SuccessfulCreate: Created pod: sample-apiserver-deployment-7f7649696-c5hg4&#xA;May 30 00:51:03.625: INFO: At 2019-05-30 00:46:09 -0700 PDT - event for sample-apiserver-deployment-7f7649696-c5hg4: {default-scheduler } Scheduled: Successfully assigned aggregator-4408/sample-apiserver-deployment-7f7649696-c5hg4 to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:51:03.625: INFO: At 2019-05-30 00:46:11 -0700 PDT - event for sample-apiserver-deployment-7f7649696-c5hg4: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulling: Pulling image &#34;e2eteam/sample-apiserver:1.10&#34;&#xA;May 30 00:51:03.625: INFO: At 2019-05-30 00:46:15 -0700 PDT - event for sample-apiserver-deployment-7f7649696-c5hg4: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container sample-apiserver&#xA;May 30 00:51:03.625: INFO: At 2019-05-30 00:46:15 -0700 PDT - event for sample-apiserver-deployment-7f7649696-c5hg4: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Successfully pulled image &#34;e2eteam/sample-apiserver:1.10&#34;&#xA;May 30 00:51:03.625: INFO: At 2019-05-30 00:46:16 -0700 PDT - event for sample-apiserver-deployment-7f7649696-c5hg4: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container sample-apiserver&#xA;May 30 00:51:03.625: INFO: At 2019-05-30 00:46:16 -0700 PDT - event for sample-apiserver-deployment-7f7649696-c5hg4: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulling: Pulling image &#34;e2eteam/etcd:3.3.10&#34;&#xA;May 30 00:51:03.625: INFO: At 2019-05-30 00:46:23 -0700 PDT - event for sample-apiserver-deployment-7f7649696-c5hg4: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container etcd&#xA;May 30 00:51:03.625: INFO: At 2019-05-30 00:46:23 -0700 PDT - event for sample-apiserver-deployment-7f7649696-c5hg4: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Successfully pulled image &#34;e2eteam/etcd:3.3.10&#34;&#xA;May 30 00:51:03.625: INFO: At 2019-05-30 00:46:25 -0700 PDT - event for sample-apiserver-deployment-7f7649696-c5hg4: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container etcd&#xA;May 30 00:51:03.625: INFO: At 2019-05-30 00:50:09 -0700 PDT - event for sample-apiserver-deployment-7f7649696-c5hg4: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod aggregator-4408/sample-apiserver-deployment-7f7649696-c5hg4&#xA;May 30 00:51:03.625: INFO: At 2019-05-30 00:51:03 -0700 PDT - event for sample-apiserver-deployment-7f7649696-c5hg4: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Killing: Stopping container etcd&#xA;May 30 00:51:03.719: INFO: POD                                                    NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 30 00:51:03.719: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 30 00:51:03.719: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 30 00:51:03.720: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:51:03.720: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:51:03.720: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:51:03.720: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 00:51:03.720: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:51:03.720: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 00:51:03.720: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 30 00:51:03.720: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 30 00:51:03.720: INFO: &#xA;May 30 00:51:03.762: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 30 00:51:03.804: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:81567,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:50:26 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:50:26 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:50:26 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:50:26 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 00:51:03.804: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 30 00:51:03.846: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 30 00:51:03.894: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:51:03.894: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:51:03.894: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:51:03.894: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:51:03.894: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 00:51:03.895: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:51:03.895: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:51:03.895: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:51:03.895: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:51:03.895: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:51:03.895: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:51:03.895: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:51:03.895: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 00:51:03.895: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:51:04.064: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 30 00:51:04.064: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 00:51:04.106: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:81555,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentContainerdRestart False 2019-05-30 00:50:05 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-30 00:50:05 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-30 00:50:05 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-30 00:50:05 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-30 00:50:05 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-30 00:50:05 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-30 00:50:05 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:50:21 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:50:21 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:50:21 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:50:21 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 00:51:04.106: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 00:51:04.147: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 00:51:04.199: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:51:04.199: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 30 00:51:04.199: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:51:04.199: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 00:51:04.199: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:51:04.199: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:51:04.199: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 30 00:51:04.199: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:51:04.199: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 30 00:51:04.199: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:51:04.199: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:51:04.199: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 30 00:51:04.199: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:51:04.199: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 30 00:51:04.200: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:51:04.200: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 30 00:51:04.200: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:51:04.200: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 00:51:04.200: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:51:04.200: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:51:04.363: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 00:51:04.363: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 00:51:04.404: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:81637,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{CorruptDockerOverlay2 False 2019-05-30 00:50:56 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-30 00:50:56 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-30 00:50:56 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-30 00:50:56 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-30 00:50:56 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-30 00:50:56 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-30 00:50:56 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:50:55 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:50:55 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:50:55 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:50:55 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[nginx@sha256:0fd68ec4b64b8dbb2bef1f1a5de9d47b658afd3635dc9c45bf0cbeac46e72101 nginx:1.15-alpine] 16087791} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 00:51:04.405: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 00:51:04.446: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 00:51:04.496: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 00:51:04.496: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 30 00:51:04.496: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:51:04.496: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 30 00:51:04.496: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 30 00:51:04.496: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:51:04.496: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 30 00:51:04.496: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 30 00:51:04.496: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 00:51:04.496: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:51:04.496: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 00:51:04.496: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:51:04.496: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 00:51:04.496: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 00:51:04.496: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 00:51:04.655: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 00:51:04.655: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 00:51:04.697: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:81646,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:51:01 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:51:01 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:51:01 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:51:01 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 00:51:04.697: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 00:51:04.738: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 00:51:04.937: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 00:51:04.937: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 00:51:04.979: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:81623,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:50:51 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:50:51 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:50:51 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:50:51 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/gb-redisslave@sha256:5ff9ae76e6abda0b9d7537fc3a5caacffba976e02f1a3bee7c6e83014b1d39d0 e2eteam/gb-redisslave:v3] 4329144223} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 00:51:04.979: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 00:51:05.021: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 00:51:05.231: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 00:51:05.231: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:51:05.273: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:81529,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 00:50:07 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 00:50:07 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 00:50:07 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 00:50:07 -0700 PDT 2019-05-30 00:50:07 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/etcd@sha256:462024728f8360157455091329616d9b55d4718533b0812b230d612a5bc5ce30 e2eteam/etcd:3.3.10] 4350004937} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/sample-apiserver@sha256:a11a8656e8013ce21acd0074574f8ac9deaccc3b99486c7d59f764fe4d4f2d55 e2eteam/sample-apiserver:1.10] 4331611162} {[e2eteam/gb-redisslave@sha256:5ff9ae76e6abda0b9d7537fc3a5caacffba976e02f1a3bee7c6e83014b1d39d0 e2eteam/gb-redisslave:v3] 4329144223} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[e2eteam/porter@sha256:f1f16595d44d9a06e851d82135a0ae53fbdf512029c6e3301f140a531778c65d e2eteam/porter:1.0] 4284085058} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 00:51:05.273: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:51:05.318: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:51:05.564: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 00:51:05.564: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;aggregator-4408&#34; for this suite.&#xA;May 30 00:51:11.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 30 00:51:13.335: INFO: namespace aggregator-4408 deletion completed in 7.728844306s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="298.337424149"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001926957">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify PVC creation fails if only datastore is specified in the storage class (No shared datastores exist among all the nodes)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [StatefulSet] should come back up if node goes down [Slow] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pending pods are small and there is another node pool that is not autoscaled [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide container&#39;s cpu limit [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.341751047"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="350.542493562">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;Timed out after 240.001s.&#xA;Expected&#xA;    &lt;string&gt;: Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    Error reading file /etc/secret-volumes/create/data-1: open C:\etc\secret-volumes\create\data-1: The system cannot find the file specified., retrying&#xA;    &#xA;to contain substring&#xA;    &lt;string&gt;: value-1&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:366</failure>
          <system-out>[BeforeEach] [sig-storage] Secrets&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 30 00:56:26.018: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename secrets&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[It] optional updates should be reflected in volume [NodeConformance] [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: Creating secret with name s-test-opt-del-c866a44b-7b18-40c8-8043-a16e415fff27&#xA;�[1mSTEP�[0m: Creating secret with name s-test-opt-upd-a5c19a7a-0472-478a-a601-99a6415c526c&#xA;�[1mSTEP�[0m: Creating the pod&#xA;�[1mSTEP�[0m: Deleting secret s-test-opt-del-c866a44b-7b18-40c8-8043-a16e415fff27&#xA;�[1mSTEP�[0m: Updating secret s-test-opt-upd-a5c19a7a-0472-478a-a601-99a6415c526c&#xA;�[1mSTEP�[0m: Creating secret with name s-test-opt-create-94bab775-6f3a-424d-b7bf-ecea49b5ef82&#xA;�[1mSTEP�[0m: waiting to observe update in volume&#xA;[AfterEach] [sig-storage] Secrets&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;secrets-2860&#34;.&#xA;�[1mSTEP�[0m: Found 11 events.&#xA;May 30 01:02:06.836: INFO: At 2019-05-30 00:56:26 -0700 PDT - event for pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e: {default-scheduler } Scheduled: Successfully assigned secrets-2860/pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:02:06.836: INFO: At 2019-05-30 00:56:28 -0700 PDT - event for pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/mounttest:1.0&#34; already present on machine&#xA;May 30 01:02:06.836: INFO: At 2019-05-30 00:56:28 -0700 PDT - event for pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container dels-volume-test&#xA;May 30 01:02:06.836: INFO: At 2019-05-30 00:56:31 -0700 PDT - event for pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container dels-volume-test&#xA;May 30 01:02:06.836: INFO: At 2019-05-30 00:56:31 -0700 PDT - event for pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/mounttest:1.0&#34; already present on machine&#xA;May 30 01:02:06.836: INFO: At 2019-05-30 00:56:31 -0700 PDT - event for pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container upds-volume-test&#xA;May 30 01:02:06.836: INFO: At 2019-05-30 00:56:33 -0700 PDT - event for pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container upds-volume-test&#xA;May 30 01:02:06.836: INFO: At 2019-05-30 00:56:33 -0700 PDT - event for pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/mounttest:1.0&#34; already present on machine&#xA;May 30 01:02:06.836: INFO: At 2019-05-30 00:56:33 -0700 PDT - event for pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container creates-volume-test&#xA;May 30 01:02:06.836: INFO: At 2019-05-30 00:56:34 -0700 PDT - event for pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container creates-volume-test&#xA;May 30 01:02:06.836: INFO: At 2019-05-30 00:58:09 -0700 PDT - event for pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod secrets-2860/pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e&#xA;May 30 01:02:06.925: INFO: POD                                                    NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 30 01:02:06.926: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:02:06.926: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 30 01:02:06.927: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 30 01:02:06.927: INFO: pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e       e2e-test-peterhornyack-windows-node-group-jpxd  Failed          [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 00:56:26 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:02:05 -0700 PDT ContainersNotReady containers with unready status: [dels-volume-test upds-volume-test creates-volume-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:02:05 -0700 PDT ContainersNotReady containers with unready status: [dels-volume-test upds-volume-test creates-volume-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 00:56:26 -0700 PDT  }]&#xA;May 30 01:02:06.927: INFO: &#xA;May 30 01:02:06.976: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 30 01:02:07.018: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:83197,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:01:28 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:01:28 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:01:28 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:01:28 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 01:02:07.019: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 30 01:02:07.060: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 30 01:02:07.118: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:02:07.118: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:02:07.118: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:02:07.118: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 01:02:07.118: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:02:07.118: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:02:07.118: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:02:07.118: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:02:07.118: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 01:02:07.118: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:02:07.118: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:02:07.118: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:02:07.118: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:02:07.118: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:02:07.269: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 30 01:02:07.269: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 01:02:07.311: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:83182,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{KernelDeadlock False 2019-05-30 01:01:14 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-30 01:01:14 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-30 01:01:14 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-30 01:01:14 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-30 01:01:14 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-30 01:01:14 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-30 01:01:14 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:01:22 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:01:22 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:01:22 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:01:22 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 01:02:07.311: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 01:02:07.352: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 01:02:07.404: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:02:07.404: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 30 01:02:07.404: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:02:07.404: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 01:02:07.404: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:02:07.404: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:02:07.404: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:02:07.404: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 30 01:02:07.404: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:02:07.404: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 30 01:02:07.404: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:02:07.404: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 30 01:02:07.404: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:02:07.404: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 30 01:02:07.404: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:02:07.404: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:02:07.404: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 30 01:02:07.404: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:02:07.404: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 01:02:07.404: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:02:07.565: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 01:02:07.565: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 01:02:07.606: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:83276,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentContainerdRestart False 2019-05-30 01:02:00 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-30 01:02:00 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-30 01:02:00 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-30 01:02:00 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-30 01:02:00 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-30 01:02:00 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-30 01:02:00 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:01:56 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:01:56 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:01:56 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:01:56 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[nginx@sha256:0fd68ec4b64b8dbb2bef1f1a5de9d47b658afd3635dc9c45bf0cbeac46e72101 nginx:1.15-alpine] 16087791} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 01:02:07.607: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 01:02:07.648: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 01:02:07.702: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:02:07.702: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 30 01:02:07.702: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 30 01:02:07.702: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:02:07.702: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:02:07.702: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 01:02:07.702: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:02:07.702: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:02:07.702: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 01:02:07.702: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:02:07.702: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:02:07.702: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 30 01:02:07.702: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:02:07.703: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 30 01:02:07.703: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 30 01:02:07.872: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 01:02:07.872: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 01:02:07.914: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:83278,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:02:01 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:02:01 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:02:01 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:02:01 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 01:02:07.914: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 01:02:07.956: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 01:02:08.161: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 01:02:08.161: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 01:02:08.203: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:83254,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:01:52 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:01:52 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:01:52 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:01:52 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/gb-redisslave@sha256:5ff9ae76e6abda0b9d7537fc3a5caacffba976e02f1a3bee7c6e83014b1d39d0 e2eteam/gb-redisslave:v3] 4329144223} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 01:02:08.203: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 01:02:08.244: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 01:02:08.448: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 01:02:08.448: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:02:08.489: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:83272,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[{node.kubernetes.io/not-ready  NoSchedule 2019-05-30 01:01:09 -0700 PDT} {node.kubernetes.io/not-ready  NoExecute 2019-05-30 01:01:14 -0700 PDT}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:01:59 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:01:59 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:01:59 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready False 2019-05-30 01:01:59 -0700 PDT 2019-05-30 01:01:09 -0700 PDT KubeletNotReady PLEG is not healthy: pleg was last seen active 3m55.6762396s ago; threshold is 3m0s.}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/etcd@sha256:462024728f8360157455091329616d9b55d4718533b0812b230d612a5bc5ce30 e2eteam/etcd:3.3.10] 4350004937} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/sample-apiserver@sha256:a11a8656e8013ce21acd0074574f8ac9deaccc3b99486c7d59f764fe4d4f2d55 e2eteam/sample-apiserver:1.10] 4331611162} {[e2eteam/gb-redisslave@sha256:5ff9ae76e6abda0b9d7537fc3a5caacffba976e02f1a3bee7c6e83014b1d39d0 e2eteam/gb-redisslave:v3] 4329144223} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[e2eteam/porter@sha256:f1f16595d44d9a06e851d82135a0ae53fbdf512029c6e3301f140a531778c65d e2eteam/porter:1.0] 4284085058} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 01:02:08.489: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:02:08.531: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:02:08.574: INFO: pod-secrets-5deea6d7-7a71-4532-ae39-7ab5d07d788e started at 2019-05-30 00:56:26 -0700 PDT (0+3 container statuses recorded)&#xA;May 30 01:02:08.574: INFO: &#x9;Container creates-volume-test ready: false, restart count 0&#xA;May 30 01:02:08.574: INFO: &#x9;Container dels-volume-test ready: false, restart count 0&#xA;May 30 01:02:08.574: INFO: &#x9;Container upds-volume-test ready: false, restart count 0&#xA;May 30 01:02:08.771: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:02:08.771: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;May 30 01:02:08.813: INFO: Condition Ready of node e2e-test-peterhornyack-windows-node-group-jpxd is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule 2019-05-30 01:01:09 -0700 PDT} {node.kubernetes.io/not-ready  NoExecute 2019-05-30 01:01:14 -0700 PDT}]. Failure&#xA;�[1mSTEP�[0m: Destroying namespace &#34;secrets-2860&#34; for this suite.&#xA;May 30 01:02:14.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 30 01:02:16.560: INFO: namespace secrets-2860 deletion completed in 7.746943108s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should scale up correct target pool [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere should test that deleting a PVC before the pod does not cause pod deletion to fail on vsphere volume detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GenericPersistentVolume[Disruptive] When kubelet restarts Should test that a volume mounted to a pod that is force deleted while the kubelet is down unmounts when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceConversionWebhook [Feature:CustomResourceWebhookConversion] Should be able to convert from CR v1 to CR v2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Flexvolumes should be mountable when non-attachable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should allow substituting values in a volume subpath [Feature:VolumeSubpathEnvExpansion][NodeAlphaFeature:VolumeSubpathEnvExpansion]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (ntfs)][sig-windows] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourcePublishOpenAPI [Feature:CustomResourcePublishOpenAPI] works for multiple CRDs of different groups" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] etcd Upgrade [Feature:EtcdUpgrade] etcd upgrade should maintain a functioning cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for Table transformation should return pod details" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="16.421277311"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] PodSecurityPolicy should enforce the restricted policy.PodSecurityPolicy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should be able to deny pod and configmap creation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] should scale up at all [Feature:ClusterAutoscalerScalability1]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]" classname="Kubernetes e2e suite" time="7.96867289"></testcase>
      <testcase name="[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should exceed backoffLimit" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with snapshot data source [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] ServiceAccounts should ensure a single API token exists" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="934.060420471">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;Timed out after 120.006s.&#xA;Expected&#xA;    &lt;string&gt;: content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    content of file &#34;/etc/podinfo/annotations&#34;: builder=&#34;bar&#34;&#xA;    kubernetes.io/config.seen=&#34;2019-05-30T08:02:41.1135909Z&#34;&#xA;    kubernetes.io/config.source=&#34;api&#34;&#xA;    &#xA;to contain substring&#xA;    &lt;string&gt;: builder=&#34;foo&#34;&#xA;    &#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:181</failure>
          <system-out>[BeforeEach] [sig-storage] Projected downwardAPI&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 30 01:02:40.952: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename projected&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-storage] Projected downwardAPI&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39&#xA;[It] should update annotations on modification [NodeConformance] [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: Creating the pod&#xA;May 30 01:06:09.986: INFO: Successfully updated pod &#34;annotationupdate9b64442f-e9d2-4ce3-a148-636df8c1bf3a&#34;&#xA;[AfterEach] [sig-storage] Projected downwardAPI&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;projected-7885&#34;.&#xA;�[1mSTEP�[0m: Found 5 events.&#xA;May 30 01:08:10.035: INFO: At 2019-05-30 01:02:41 -0700 PDT - event for annotationupdate9b64442f-e9d2-4ce3-a148-636df8c1bf3a: {default-scheduler } Scheduled: Successfully assigned projected-7885/annotationupdate9b64442f-e9d2-4ce3-a148-636df8c1bf3a to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:08:10.035: INFO: At 2019-05-30 01:02:43 -0700 PDT - event for annotationupdate9b64442f-e9d2-4ce3-a148-636df8c1bf3a: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/mounttest:1.0&#34; already present on machine&#xA;May 30 01:08:10.035: INFO: At 2019-05-30 01:02:43 -0700 PDT - event for annotationupdate9b64442f-e9d2-4ce3-a148-636df8c1bf3a: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container client-container&#xA;May 30 01:08:10.035: INFO: At 2019-05-30 01:02:45 -0700 PDT - event for annotationupdate9b64442f-e9d2-4ce3-a148-636df8c1bf3a: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container client-container&#xA;May 30 01:08:10.035: INFO: At 2019-05-30 01:06:14 -0700 PDT - event for annotationupdate9b64442f-e9d2-4ce3-a148-636df8c1bf3a: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod projected-7885/annotationupdate9b64442f-e9d2-4ce3-a148-636df8c1bf3a&#xA;May 30 01:08:10.123: INFO: POD                                                    NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 30 01:08:10.123: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:08:10.123: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 30 01:08:10.123: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:08:10.123: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:08:10.123: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:08:10.123: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master                   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: annotationupdate9b64442f-e9d2-4ce3-a148-636df8c1bf3a   e2e-test-peterhornyack-windows-node-group-jpxd  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:02:41 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:06:07 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:06:07 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:02:41 -0700 PDT  }]&#xA;May 30 01:08:10.124: INFO: &#xA;May 30 01:08:10.168: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 30 01:08:10.209: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:84100,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:07:29 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:07:29 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:07:29 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:07:29 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 01:08:10.210: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 30 01:08:10.251: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 30 01:08:10.297: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:08:10.297: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:08:10.297: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:08:10.297: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 01:08:10.297: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:08:10.297: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:08:10.297: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:08:10.297: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:08:10.297: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:08:10.297: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:08:10.297: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:08:10.297: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:08:10.297: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 01:08:10.297: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:08:10.483: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 30 01:08:10.483: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 01:08:10.524: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:84082,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentDockerRestart False 2019-05-30 01:07:18 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-30 01:07:18 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-30 01:07:18 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-30 01:07:18 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-30 01:07:18 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-30 01:07:18 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-30 01:07:18 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:07:23 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:07:23 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:07:23 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:07:23 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 01:08:10.524: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 01:08:10.565: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 01:08:10.614: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:08:10.614: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 30 01:08:10.614: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:08:10.614: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:08:10.614: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 30 01:08:10.614: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:08:10.614: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 01:08:10.614: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:08:10.614: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:08:10.614: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 30 01:08:10.614: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:08:10.614: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 30 01:08:10.614: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:08:10.614: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 30 01:08:10.614: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:08:10.614: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 30 01:08:10.614: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:08:10.615: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 01:08:10.615: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:08:10.615: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:08:10.790: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 01:08:10.790: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 01:08:10.832: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:84178,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{KernelDeadlock False 2019-05-30 01:08:04 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-30 01:08:04 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-30 01:08:04 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-30 01:08:04 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-30 01:08:04 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-30 01:08:04 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-30 01:08:04 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:07:56 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:07:56 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:07:56 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:07:56 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[nginx@sha256:0fd68ec4b64b8dbb2bef1f1a5de9d47b658afd3635dc9c45bf0cbeac46e72101 nginx:1.15-alpine] 16087791} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 01:08:10.832: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 01:08:10.873: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 01:08:10.921: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:08:10.921: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:08:10.921: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 01:08:10.921: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:08:10.921: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:08:10.921: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 01:08:10.921: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:08:10.921: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:08:10.921: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 30 01:08:10.921: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:08:10.921: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 30 01:08:10.922: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 30 01:08:10.922: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:08:10.922: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 30 01:08:10.922: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 30 01:08:11.094: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 01:08:11.094: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 01:08:11.136: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:84174,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:08:02 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:08:02 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:08:02 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:08:02 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 01:08:11.136: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 01:08:11.178: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 01:08:11.377: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 01:08:11.377: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 01:08:11.419: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:84150,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:07:53 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:07:53 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:07:53 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:07:53 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/gb-redisslave@sha256:5ff9ae76e6abda0b9d7537fc3a5caacffba976e02f1a3bee7c6e83014b1d39d0 e2eteam/gb-redisslave:v3] 4329144223} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 01:08:11.419: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 01:08:11.461: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 01:08:11.666: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 01:08:11.666: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:08:11.707: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:84194,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:08:09 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:08:09 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:08:09 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:08:09 -0700 PDT 2019-05-30 01:06:09 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/etcd@sha256:462024728f8360157455091329616d9b55d4718533b0812b230d612a5bc5ce30 e2eteam/etcd:3.3.10] 4350004937} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/sample-apiserver@sha256:a11a8656e8013ce21acd0074574f8ac9deaccc3b99486c7d59f764fe4d4f2d55 e2eteam/sample-apiserver:1.10] 4331611162} {[e2eteam/gb-redisslave@sha256:5ff9ae76e6abda0b9d7537fc3a5caacffba976e02f1a3bee7c6e83014b1d39d0 e2eteam/gb-redisslave:v3] 4329144223} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[e2eteam/porter@sha256:f1f16595d44d9a06e851d82135a0ae53fbdf512029c6e3301f140a531778c65d e2eteam/porter:1.0] 4284085058} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 01:08:11.707: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:08:11.749: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:08:11.793: INFO: annotationupdate9b64442f-e9d2-4ce3-a148-636df8c1bf3a started at 2019-05-30 01:02:41 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:08:11.793: INFO: &#x9;Container client-container ready: true, restart count 0&#xA;May 30 01:08:13.083: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:08:13.083: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;projected-7885&#34; for this suite.&#xA;May 30 01:18:13.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 30 01:18:14.342: INFO: namespace: projected-7885, resource: pods, items remaining: 1&#xA;May 30 01:18:14.928: INFO: namespace: projected-7885, DeletionTimetamp: 2019-05-30 01:08:13 -0700 PDT, Finalizers: [kubernetes], Phase: Terminating&#xA;May 30 01:18:14.970: INFO: namespace: projected-7885, total namespaces: 5, active: 4, terminating: 1&#xA;May 30 01:18:15.011: INFO: POD                                                   NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 30 01:18:15.011: INFO: annotationupdate9b64442f-e9d2-4ce3-a148-636df8c1bf3a  e2e-test-peterhornyack-windows-node-group-jpxd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:02:41 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:06:07 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:06:07 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:02:41 -0700 PDT  }]&#xA;May 30 01:18:15.011: INFO: &#xA;May 30 01:18:15.011: INFO: Couldn&#39;t delete ns: &#34;projected-7885&#34;: namespace projected-7885 was not deleted with limit: timed out waiting for the condition, pods remaining: 1 (&amp;errors.errorString{s:&#34;namespace projected-7885 was not deleted with limit: timed out waiting for the condition, pods remaining: 1&#34;})&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against a pod with different priority class (ScopeSelectorOpNotIn)." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pod requesting EmptyDir volume is pending [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down by draining system pods with pdb[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Zone Support Verify PVC creation with incompatible datastore and zone combination specified in storage class fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] ReplicationController Should scale from 1 pod to 3 pods and from 3 to 5 and verify decision stability" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] multiVolume [Slow] should access to two volumes with different volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should verify container cannot write to subpath readonly volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0.003259394">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should work from pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost should support forwarding over websockets" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should be restarted with a exec &#34;cat /tmp/health&#34; liveness probe [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="489.110319997">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;May 30 01:26:14.352: pod container-probe-4561/liveness-exec - expected number of restarts: 1, found restarts: 0&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:524</failure>
          <system-out>[BeforeEach] [k8s.io] Probing container&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 30 01:18:15.018: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename container-probe&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [k8s.io] Probing container&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51&#xA;[It] should be restarted with a exec &#34;cat /tmp/health&#34; liveness probe [NodeConformance] [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: Creating pod liveness-exec in namespace container-probe-4561&#xA;May 30 01:22:13.325: INFO: Started pod liveness-exec in namespace container-probe-4561&#xA;�[1mSTEP�[0m: checking the pod&#39;s current state and verifying that restartCount is present&#xA;May 30 01:22:13.367: INFO: Initial restart count of pod liveness-exec is 0&#xA;May 30 01:26:14.352: INFO: pod container-probe-4561/liveness-exec - expected number of restarts: 1, found restarts: 0&#xA;�[1mSTEP�[0m: deleting the pod&#xA;[AfterEach] [k8s.io] Probing container&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;container-probe-4561&#34;.&#xA;�[1mSTEP�[0m: Found 7 events.&#xA;May 30 01:26:14.446: INFO: At 2019-05-30 01:18:15 -0700 PDT - event for liveness-exec: {default-scheduler } Scheduled: Successfully assigned container-probe-4561/liveness-exec to e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:26:14.446: INFO: At 2019-05-30 01:18:18 -0700 PDT - event for liveness-exec: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Pulled: Container image &#34;e2eteam/busybox:1.29&#34; already present on machine&#xA;May 30 01:26:14.446: INFO: At 2019-05-30 01:18:18 -0700 PDT - event for liveness-exec: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Created: Created container liveness&#xA;May 30 01:26:14.446: INFO: At 2019-05-30 01:18:20 -0700 PDT - event for liveness-exec: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Started: Started container liveness&#xA;May 30 01:26:14.446: INFO: At 2019-05-30 01:22:20 -0700 PDT - event for liveness-exec: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Unhealthy: Liveness probe failed: cat: can&#39;t open &#39;/tmp/health&#39;: No such file or directory&#xA;&#xA;May 30 01:26:14.446: INFO: At 2019-05-30 01:22:24 -0700 PDT - event for liveness-exec: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod container-probe-4561/liveness-exec&#xA;May 30 01:26:14.446: INFO: At 2019-05-30 01:26:14 -0700 PDT - event for liveness-exec: {kubelet e2e-test-peterhornyack-windows-node-group-jpxd} Killing: Stopping container liveness&#xA;May 30 01:26:14.535: INFO: POD                                                    NODE                                      PHASE    GRACE  CONDITIONS&#xA;May 30 01:26:14.535: INFO: coredns-5b969f4c88-gsjpw                               e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: coredns-5b969f4c88-mvhtd                               e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:39 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:25 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master   e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: etcd-server-e2e-test-peterhornyack-master              e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:34 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: etcd-server-events-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w                 e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz                    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: fluentd-gcp-v3.2.0-fr5zq                               e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:23 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:20 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: fluentd-gcp-v3.2.0-r5s9z                               e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:06:10 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: fluentd-gcp-v3.2.0-wp9vf                               e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:52 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:48 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55                e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:41 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: kube-addon-manager-e2e-test-peterhornyack-master       e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:35 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: kube-apiserver-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:45 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: kube-controller-manager-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:17 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:10 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: kube-dns-autoscaler-97df449df-7v474                    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:26:14.535: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh    e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:13 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  }]&#xA;May 30 01:26:14.536: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6    e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:12 -0700 PDT  }]&#xA;May 30 01:26:14.536: INFO: kube-scheduler-e2e-test-peterhornyack-master           e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:26:14.536: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v                  e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:26:14.536: INFO: l7-default-backend-8f479dd9-hnbtn                      e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:26:14.536: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master  e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:31 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:01:11 -0700 PDT  }]&#xA;May 30 01:26:14.536: INFO: metadata-proxy-v0.1-8mhrb                              e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:20 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:26:14.536: INFO: metadata-proxy-v0.1-gqcgn                              e2e-test-peterhornyack-minion-group-5wdh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:09 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:33 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:08 -0700 PDT  }]&#xA;May 30 01:26:14.536: INFO: metadata-proxy-v0.1-w99mm                              e2e-test-peterhornyack-master             Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:02:04 -0700 PDT  }]&#xA;May 30 01:26:14.536: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6                 e2e-test-peterhornyack-minion-group-fzx6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 16:05:32 -0700 PDT  }]&#xA;May 30 01:26:14.536: INFO: &#xA;May 30 01:26:14.578: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-master&#xA;May 30 01:26:14.625: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-master,UID:5a2fc962-90c7-4013-ae90-fb4b902fb4df,ResourceVersion:86676,Generation:0,CreationTimestamp:2019-05-29 16:02:04 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-1,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-master,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.0.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-master,Unschedulable:true,Taints:[{node-under-test false NoSchedule &lt;nil&gt;} {node.kubernetes.io/unschedulable  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{16684785664 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3878420480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{32 0} {&lt;nil&gt;} 32 DecimalSI},cpu: {{1 0} {&lt;nil&gt;} 1 DecimalSI},ephemeral-storage: {{15016307073 0} {&lt;nil&gt;} 15016307073 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{3616276480 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:02:04 -0700 PDT 2019-05-29 16:02:04 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:25:32 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:25:32 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:25:32 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:25:32 -0700 PDT 2019-05-29 16:02:04 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.2} {ExternalIP 146.148.105.213} {InternalDNS e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-master.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:804d0c88641256f447b18f4e6b74052c,SystemUUID:804D0C88-6412-56F4-47B1-8F4E6B74052C,BootID:435bd611-79d4-413f-84f2-66457dad30cc,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7 k8s.gcr.io/etcd:3.3.10-0] 258116302} {[k8s.gcr.io/kube-apiserver:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 231270667} {[k8s.gcr.io/kube-controller-manager:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 176865349} {[k8s.gcr.io/kube-scheduler:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87506773} {[k8s.gcr.io/kube-addon-manager@sha256:672794ee3582521eb8bc4f257d0f70c92893f1989f39a200f9c84bcfe1aea7c9 k8s.gcr.io/kube-addon-manager:v9.0] 83077558} {[k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3] 71797285} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/etcd-empty-dir-cleanup@sha256:a10c61bd700a14b43b3a45a1791612ef9907c3ef3ba3b1731e0ab0675248d351 k8s.gcr.io/etcd-empty-dir-cleanup:3.3.10.0] 32791339} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 01:26:14.626: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-master&#xA;May 30 01:26:14.667: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-master&#xA;May 30 01:26:14.715: INFO: kube-apiserver-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:26:14.715: INFO: kube-controller-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:26:14.715: INFO: kube-scheduler-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:26:14.715: INFO: etcd-empty-dir-cleanup-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:26:14.715: INFO: etcd-server-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:26:14.715: INFO: l7-lb-controller-v1.2.3-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:26:14.715: INFO: metadata-proxy-v0.1-w99mm started at 2019-05-29 16:02:04 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:26:14.715: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 01:26:14.715: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:26:14.715: INFO: etcd-server-events-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:26:14.715: INFO: kube-addon-manager-e2e-test-peterhornyack-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:26:14.715: INFO: fluentd-gcp-v3.2.0-r5s9z started at 2019-05-29 16:06:10 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:26:14.715: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 01:26:14.715: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:26:14.884: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-master&#xA;May 30 01:26:14.884: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 01:26:14.927: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-5wdh,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-5wdh,UID:9ca19318-399c-4041-8925-ef1f19470ecf,ResourceVersion:86665,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-5wdh,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.5.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-5wdh,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{ReadonlyFilesystem False 2019-05-30 01:25:29 -0700 PDT 2019-05-29 16:05:06 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentUnregisterNetDevice False 2019-05-30 01:25:29 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {FrequentKubeletRestart False 2019-05-30 01:25:29 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-30 01:25:29 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {FrequentContainerdRestart False 2019-05-30 01:25:29 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-30 01:25:29 -0700 PDT 2019-05-29 16:10:08 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {KernelDeadlock False 2019-05-30 01:25:29 -0700 PDT 2019-05-29 16:05:06 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {NetworkUnavailable False 2019-05-29 16:05:09 -0700 PDT 2019-05-29 16:05:09 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:25:24 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:25:24 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:25:24 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:25:24 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.7} {ExternalIP 104.154.141.122} {InternalDNS e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-5wdh.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:befab8e595f78d9542bb248f3fde62a0,SystemUUID:BEFAB8E5-95F7-8D95-42BB-248F3FDE62A0,BootID:a69f529f-06bd-42a6-82e4-d48b95d347ef,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff k8s.gcr.io/fluentd-gcp-scaler:0.5.1] 86637208} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc k8s.gcr.io/event-exporter:v0.2.4] 47261019} {[k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:12370202895b621a2ac28226292e4578598f13c1502aa4d3ee90fff4325d9275 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0] 45853555} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 k8s.gcr.io/defaultbackend-amd64:1.5] 5132544} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 01:26:14.927: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 01:26:14.968: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 01:26:15.016: INFO: coredns-5b969f4c88-gsjpw started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:26:15.017: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 30 01:26:15.017: INFO: kube-dns-autoscaler-97df449df-7v474 started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:26:15.017: INFO: &#x9;Container autoscaler ready: true, restart count 0&#xA;May 30 01:26:15.017: INFO: fluentd-gcp-scaler-7db4984bf4-nj6gz started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:26:15.017: INFO: &#x9;Container fluentd-gcp-scaler ready: true, restart count 0&#xA;May 30 01:26:15.017: INFO: metadata-proxy-v0.1-gqcgn started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:26:15.017: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 01:26:15.017: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:26:15.017: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-5wdh started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:26:15.017: INFO: l7-default-backend-8f479dd9-hnbtn started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:26:15.017: INFO: &#x9;Container default-http-backend ready: true, restart count 0&#xA;May 30 01:26:15.017: INFO: fluentd-gcp-v3.2.0-wp9vf started at 2019-05-29 16:05:48 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:26:15.017: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 01:26:15.017: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:26:15.017: INFO: kubernetes-dashboard-85bcf5dbf8-4ks9v started at 2019-05-29 16:05:09 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:26:15.017: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;May 30 01:26:15.017: INFO: event-exporter-v0.2.4-65d8d98768-v6s2w started at 2019-05-29 16:05:09 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:26:15.017: INFO: &#x9;Container event-exporter ready: true, restart count 0&#xA;May 30 01:26:15.017: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:26:15.182: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-5wdh&#xA;May 30 01:26:15.182: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 01:26:15.223: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-minion-group-fzx6,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-minion-group-fzx6,UID:7b64a731-1eb9-4ca7-bcf0-c6b4bfbc801d,ResourceVersion:86779,Generation:0,CreationTimestamp:2019-05-29 16:05:08 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/metadata-proxy-ready: true,beta.kubernetes.io/os: linux,cloud.google.com/metadata-proxy-ready: true,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-minion-group-fzx6,kubernetes.io/os: linux,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.4.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-minion-group-fzx6,Unschedulable:false,Taints:[{node-under-test false NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{101241290752 0} {&lt;nil&gt;}  BinarySI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7841865728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{91117161526 0} {&lt;nil&gt;} 91117161526 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{7579721728 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{FrequentContainerdRestart False 2019-05-30 01:26:14 -0700 PDT 2019-05-29 16:10:10 -0700 PDT FrequentContainerdRestart containerd is functioning properly} {CorruptDockerOverlay2 False 2019-05-30 01:26:14 -0700 PDT 2019-05-29 16:10:07 -0700 PDT CorruptDockerOverlay2 docker overlay2 is functioning properly} {FrequentUnregisterNetDevice False 2019-05-30 01:26:14 -0700 PDT 2019-05-29 16:10:08 -0700 PDT UnregisterNetDevice node is functioning properly} {KernelDeadlock False 2019-05-30 01:26:14 -0700 PDT 2019-05-29 16:05:07 -0700 PDT KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem False 2019-05-30 01:26:14 -0700 PDT 2019-05-29 16:05:07 -0700 PDT FilesystemIsNotReadOnly Filesystem is not read-only} {FrequentKubeletRestart False 2019-05-30 01:26:14 -0700 PDT 2019-05-29 16:10:08 -0700 PDT FrequentKubeletRestart kubelet is functioning properly} {FrequentDockerRestart False 2019-05-30 01:26:14 -0700 PDT 2019-05-29 16:10:09 -0700 PDT FrequentDockerRestart docker is functioning properly} {NetworkUnavailable False 2019-05-29 16:05:08 -0700 PDT 2019-05-29 16:05:08 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:25:58 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:25:58 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:25:58 -0700 PDT 2019-05-29 16:05:08 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:25:58 -0700 PDT 2019-05-29 16:05:09 -0700 PDT KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.40.0.6} {ExternalIP 35.222.68.239} {InternalDNS e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-minion-group-fzx6.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:468d9744b5596c194192400073e124a9,SystemUUID:468D9744-B559-6C19-4192-400073E124A9,BootID:43b38037-6082-435e-8848-6c41ac58f8d2,KernelVersion:4.14.94+,OSImage:Container-Optimized OS from Google,ContainerRuntimeVersion:docker://18.9.3,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8] 264721247} {[nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451 nginx:1.7.9] 91664166} {[k8s.gcr.io/kube-proxy:v1.15.0-alpha.0.1887_bae0630ef897d6-dirty] 87361539} {[k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521 k8s.gcr.io/heapster-amd64:v1.6.0-beta.1] 76016169} {[ubuntu@sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 ubuntu:latest] 69859102} {[k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e k8s.gcr.io/prometheus-to-sd:v0.5.0] 41861013} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4 k8s.gcr.io/coredns:1.3.1] 40303560} {[k8s.gcr.io/addon-resizer@sha256:a31822f30e947885d038812f4a5a5675e72f92c06cef17b1989c80426aa89012 k8s.gcr.io/addon-resizer:1.8.4] 38349857} {[nginx@sha256:0fd68ec4b64b8dbb2bef1f1a5de9d47b658afd3635dc9c45bf0cbeac46e72101 nginx:1.15-alpine] 16087791} {[k8s.gcr.io/metadata-proxy@sha256:80fb815ae179d4f177a880f0b9af65fe9742c94d626d1904f642eba8610be5bf k8s.gcr.io/metadata-proxy:v0.1.11] 8965381} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:&amp;NodeConfigStatus{Assigned:nil,Active:nil,LastKnownGood:nil,Error:,},},}&#xA;May 30 01:26:15.224: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 01:26:15.265: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 01:26:15.313: INFO: kube-proxy-e2e-test-peterhornyack-minion-group-fzx6 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;May 30 01:26:15.314: INFO: fluentd-gcp-v3.2.0-fr5zq started at 2019-05-29 16:06:20 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:26:15.314: INFO: &#x9;Container fluentd-gcp ready: true, restart count 0&#xA;May 30 01:26:15.314: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:26:15.314: INFO: metadata-proxy-v0.1-8mhrb started at 2019-05-29 16:05:08 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:26:15.314: INFO: &#x9;Container metadata-proxy ready: true, restart count 0&#xA;May 30 01:26:15.314: INFO: &#x9;Container prometheus-to-sd-exporter ready: true, restart count 0&#xA;May 30 01:26:15.314: INFO: coredns-5b969f4c88-mvhtd started at 2019-05-29 16:05:25 -0700 PDT (0+1 container statuses recorded)&#xA;May 30 01:26:15.314: INFO: &#x9;Container coredns ready: true, restart count 0&#xA;May 30 01:26:15.314: INFO: metrics-server-v0.3.1-7d9cf58c5c-dksn6 started at 2019-05-29 16:05:32 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:26:15.314: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;May 30 01:26:15.314: INFO: &#x9;Container metrics-server-nanny ready: true, restart count 0&#xA;May 30 01:26:15.314: INFO: heapster-v1.6.0-beta.1-6796bf47bf-cqs55 started at 2019-05-29 16:05:33 -0700 PDT (0+2 container statuses recorded)&#xA;May 30 01:26:15.314: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;May 30 01:26:15.314: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;May 30 01:26:15.473: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-minion-group-fzx6&#xA;May 30 01:26:15.473: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 01:26:15.514: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-1vjk,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-1vjk,UID:39214e7b-3bfd-490b-9a21-eb35214c3d48,ResourceVersion:86748,Generation:0,CreationTimestamp:2019-05-29 16:14:34 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-1vjk,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.2.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-1vjk,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:35 -0700 PDT 2019-05-29 16:14:35 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:26:04 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:26:04 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:26:04 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:26:04 -0700 PDT 2019-05-29 16:14:34 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.5} {ExternalIP 104.197.5.20} {InternalDNS e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-1vjk.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-1vjk,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/redis@sha256:8c9fd0656356dcad4ed60c16931ea928cc6dc97a4a100cdf7a26f7446fa5c9f1 e2eteam/redis:1.0] 4349854258} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/authenticated-image-pulling/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/authenticated-image-pulling/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 01:26:15.514: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 01:26:15.557: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 01:26:15.757: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-1vjk&#xA;May 30 01:26:15.757: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 01:26:15.798: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-9q9v,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-9q9v,UID:cb28431f-383d-412b-96a8-334b9465f2ab,ResourceVersion:86726,Generation:0,CreationTimestamp:2019-05-29 16:14:14 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-9q9v,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.3.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-9q9v,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:14 -0700 PDT 2019-05-29 16:14:14 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:25:55 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:25:55 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:25:55 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-05-30 01:25:55 -0700 PDT 2019-05-29 16:14:14 -0700 PDT KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.40.0.4} {ExternalIP 35.225.201.100} {InternalDNS e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-9q9v.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-9q9v,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine e2eteam/nginx:1.15-alpine] 4340785269} {[e2eteam/gb-redisslave@sha256:5ff9ae76e6abda0b9d7537fc3a5caacffba976e02f1a3bee7c6e83014b1d39d0 e2eteam/gb-redisslave:v3] 4329144223} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/liveness@sha256:15512c0338c5142c217f50f2e9913ccea639069284b1f8bf45a8e74c0d299d9c e2eteam/liveness:1.1] 4288934732} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/kitten@sha256:43e92993512cd4b1439c54979ec444d3bc7460183cb90cb6073c543075d6501a e2eteam/kitten:1.0] 4284294634} {[e2eteam/entrypoint-tester@sha256:1a37af31b33bf9a6c90597e17433b14cfa84a0825ae204adc029714ac0ced9e0 e2eteam/entrypoint-tester:1.0] 4281099802} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 01:26:15.798: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 01:26:15.840: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 01:26:16.042: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-9q9v&#xA;May 30 01:26:16.042: INFO: &#xA;Logging node info for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:26:16.083: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-test-peterhornyack-windows-node-group-jpxd,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/e2e-test-peterhornyack-windows-node-group-jpxd,UID:be2c16bf-c52a-4f33-8e29-353ee370eb68,ResourceVersion:86767,Generation:0,CreationTimestamp:2019-05-29 16:14:43 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: n1-standard-2,beta.kubernetes.io/os: windows,failure-domain.beta.kubernetes.io/region: us-central1,failure-domain.beta.kubernetes.io/zone: us-central1-b,kubernetes.io/arch: amd64,kubernetes.io/hostname: e2e-test-peterhornyack-windows-node-group-jpxd,kubernetes.io/os: windows,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:NodeSpec{PodCIDR:10.64.1.0/24,DoNotUse_ExternalID:,ProviderID:gce://peterhornyack-prod-no-enforcer/us-central1-b/e2e-test-peterhornyack-windows-node-group-jpxd,Unschedulable:false,Taints:[{node.kubernetes.io/not-ready  NoSchedule 2019-05-30 01:25:21 -0700 PDT} {node.kubernetes.io/not-ready  NoExecute 2019-05-30 01:25:24 -0700 PDT}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{107372081152 0} {&lt;nil&gt;} 104855548Ki BinarySI},memory: {{8052645888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-gce-pd: {{64 0} {&lt;nil&gt;} 64 DecimalSI},cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{96634872877 0} {&lt;nil&gt;} 96634872877 DecimalSI},memory: {{7790501888 0} {&lt;nil&gt;}  BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-05-29 16:14:43 -0700 PDT 2019-05-29 16:14:43 -0700 PDT RouteCreated NodeController create implicit route} {MemoryPressure False 2019-05-30 01:26:11 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-05-30 01:26:11 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-05-30 01:26:11 -0700 PDT 2019-05-29 16:14:42 -0700 PDT KubeletHasSufficientPID kubelet has sufficient PID available} {Ready False 2019-05-30 01:26:11 -0700 PDT 2019-05-30 01:25:21 -0700 PDT KubeletNotReady PLEG is not healthy: pleg was last seen active 3m59.9244995s ago; threshold is 3m0s.}],Addresses:[{InternalIP 10.40.0.3} {ExternalIP 104.197.45.22} {InternalDNS e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal} {Hostname e2e-test-peterhornyack-windows-node-group-jpxd.c.peterhornyack-prod-no-enforcer.internal}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:e2e-test-peterhornyack-windows-node-group-jpxd,SystemUUID:,BootID:,KernelVersion:10.0.17763.529,OSImage:Windows Server Datacenter,ContainerRuntimeVersion:docker://18.9.6,KubeletVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,KubeProxyVersion:v1.15.0-alpha.0.1887+bae0630ef897d6-dirty,OperatingSystem:windows,Architecture:amd64,},Images:[{[e2eteam/gb-frontend@sha256:edb34ee23621ce91b79cb71ca5b8b18a01c450d2b15669595ddfeddacb4bd7ee e2eteam/gb-frontend:v6] 4638421988} {[e2eteam/dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/jessie-dnsutils@sha256:041dfc954a7b2c895b6649ba82bb44ff6edabf34a298ffb1fd5940982ca59e50 e2eteam/dnsutils:1.1 e2eteam/jessie-dnsutils:1.0] 4448740419} {[mcr.microsoft.com/windows/servercore@sha256:8d9b3f92bf3ca6660df64537753788d589d401ff088ac6f492505c7bfa98733b mcr.microsoft.com/windows/servercore:1809] 4432279302} {[e2eteam/etcd@sha256:462024728f8360157455091329616d9b55d4718533b0812b230d612a5bc5ce30 e2eteam/etcd:3.3.10] 4350004937} {[e2eteam/nginx@sha256:31014429d17373c204a65373c84517d0a45be53bc522c0c49eba01464aee7a14 e2eteam/nginx:1.14-alpine] 4340785269} {[e2eteam/sample-apiserver@sha256:a11a8656e8013ce21acd0074574f8ac9deaccc3b99486c7d59f764fe4d4f2d55 e2eteam/sample-apiserver:1.10] 4331611162} {[e2eteam/gb-redisslave@sha256:5ff9ae76e6abda0b9d7537fc3a5caacffba976e02f1a3bee7c6e83014b1d39d0 e2eteam/gb-redisslave:v3] 4329144223} {[e2eteam/nettest@sha256:a961fd86e44b2efa64c5b661a1d3601ed1fc8fb164b38a005927c94ed94c1ed5 e2eteam/nettest:1.0] 4316104218} {[e2eteam/netexec@sha256:796b59adff72873b27b84ab5bda3fc548192261a2f19e6711e799d2454599e9e e2eteam/netexec:1.1] 4311441436} {[e2eteam/hostexec@sha256:ce9db034f977e33c83b87a0e298c8334ad1c0432024d9a5cf3d7418c4167623c e2eteam/hostexec:1.1] 4298305042} {[e2eteam/busybox@sha256:4ddc67f22aef70f00513ee5331aa4d2c668c461cb59b841d281662da5d948fc4 e2eteam/busybox:1.29] 4298264082} {[e2eteam/nautilus@sha256:826dff3a54d96064bc597ebc3af47ee95ec4118b3af138af296d9fc127f2337b e2eteam/nautilus:1.0] 4284301098} {[e2eteam/test-webserver@sha256:9c8c2e6e40de89a80cde1f53f5a3e87d2095f83ea4b02fc4f7ceb9c93b8bb3d2 e2eteam/test-webserver:1.0] 4284197914} {[e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b e2eteam/serve-hostname:1.1] 4284091418} {[e2eteam/porter@sha256:f1f16595d44d9a06e851d82135a0ae53fbdf512029c6e3301f140a531778c65d e2eteam/porter:1.0] 4284085058} {[e2eteam/mounttest@sha256:9522bf8c9b421f8ff59e6294673e7cb39cd08e656d6da0e5f7b2c4efe99531b4 e2eteam/mounttest:1.0] 4279865259} {[e2eteam/pause@sha256:35643fb259badf62336f5518e1373b3de6dea7cf095c783f5df8aed0a2d4150d e2eteam/pause:3.1] 4278932506} {[mcr.microsoft.com/k8s/core/pause@sha256:83f2ee107beee5d5a7eda651106b3519e7dacda7e43ca89d6f26e775db76fa5b mcr.microsoft.com/k8s/core/pause:1.0.0] 346743616} {[gcr.io/kubernetes-e2e-test-images/windows-nanoserver@sha256:350f41a51a1fb33f1b0361a4247808a4b332e151a6970306822c4fd0fe98d628 gcr.io/kubernetes-e2e-test-images/windows-nanoserver:v1] 346702656} {[mcr.microsoft.com/windows/nanoserver@sha256:618899238737d4f2a78d87f91d5599de220ca076797523c1118a2fc6a1acb82c mcr.microsoft.com/windows/nanoserver:1809] 250275043}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;May 30 01:26:16.083: INFO: &#xA;Logging kubelet events for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:26:16.125: INFO: &#xA;Logging pods the kubelet thinks is on node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:26:16.364: INFO: &#xA;Latency metrics for node e2e-test-peterhornyack-windows-node-group-jpxd&#xA;May 30 01:26:16.364: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;May 30 01:26:16.406: INFO: Condition Ready of node e2e-test-peterhornyack-windows-node-group-jpxd is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule 2019-05-30 01:25:21 -0700 PDT} {node.kubernetes.io/not-ready  NoExecute 2019-05-30 01:25:24 -0700 PDT}]. Failure&#xA;�[1mSTEP�[0m: Destroying namespace &#34;container-probe-4561&#34; for this suite.&#xA;May 30 01:26:22.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 30 01:26:24.128: INFO: namespace container-probe-4561 deletion completed in 7.722104503s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-api-machinery] Discovery [Feature:StorageVersionHash] Custom resource should have storage version hash" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should scale down when expendable pod is running [Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]" classname="Kubernetes e2e suite" time="8.289025116"></testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]" classname="Kubernetes e2e suite" time="830.487742843">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;May 30 01:40:22.904: Couldn&#39;t delete ns: &#34;events-3047&#34;: namespace events-3047 was not deleted with limit: timed out waiting for the condition, pods remaining: 1 (&amp;errors.errorString{s:&#34;namespace events-3047 was not deleted with limit: timed out waiting for the condition, pods remaining: 1&#34;})&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:334</failure>
          <system-out>[BeforeEach] [k8s.io] [sig-node] Events&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 30 01:26:32.417: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename events&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: creating the pod&#xA;�[1mSTEP�[0m: submitting the pod to kubernetes&#xA;�[1mSTEP�[0m: verifying the pod is in kubernetes&#xA;�[1mSTEP�[0m: retrieving the pod&#xA;May 30 01:30:16.837: INFO: &amp;Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-3ac4cdb7-0242-41a3-a533-01f9f32da136,GenerateName:,Namespace:events-3047,SelfLink:/api/v1/namespaces/events-3047/pods/send-events-3ac4cdb7-0242-41a3-a533-01f9f32da136,UID:e41f83b3-0d59-4143-9822-046f0f2c67f3,ResourceVersion:87371,Generation:0,CreationTimestamp:2019-05-30 01:26:32 -0700 PDT,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 620313969,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sprxr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sprxr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p e2eteam/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-sprxr true /var/run/secrets/kubernetes.io/serviceaccount  &lt;nil&gt; }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:e2e-test-peterhornyack-windows-node-group-jpxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&amp;PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002bd2730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002bd2750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:26:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:30:14 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:30:14 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:26:32 -0700 PDT  }],Message:,Reason:,HostIP:10.40.0.3,PodIP:10.64.1.135,StartTime:2019-05-30 01:26:32 -0700 PDT,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-05-30 01:26:36 -0700 PDT,} nil} {nil nil nil} true 0 e2eteam/serve-hostname:1.1 docker-pullable://e2eteam/serve-hostname@sha256:28b71faa361ce5672ec2bd8bf852bca7d235e8b85736e21c2bcc42ba7df1db2b docker://09aab00d8e388b398c5b8498ce206319b1985301a077a7d3894053b591e2d88c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}&#xA;&#xA;�[1mSTEP�[0m: checking for scheduler event about the pod&#xA;May 30 01:30:18.879: INFO: Saw scheduler event for our pod.&#xA;�[1mSTEP�[0m: checking for kubelet event about the pod&#xA;May 30 01:30:20.921: INFO: Saw kubelet event for our pod.&#xA;�[1mSTEP�[0m: deleting the pod&#xA;[AfterEach] [k8s.io] [sig-node] Events&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;May 30 01:30:20.966: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;May 30 01:30:21.009: INFO: Condition Ready of node e2e-test-peterhornyack-windows-node-group-jpxd is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule 2019-05-30 01:29:22 -0700 PDT} {node.kubernetes.io/not-ready  NoExecute 2019-05-30 01:29:24 -0700 PDT}]. Failure&#xA;�[1mSTEP�[0m: Destroying namespace &#34;events-3047&#34; for this suite.&#xA;May 30 01:40:21.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 30 01:40:21.942: INFO: namespace: events-3047, resource: pods, items remaining: 1&#xA;May 30 01:40:22.820: INFO: namespace: events-3047, DeletionTimetamp: 2019-05-30 01:30:21 -0700 PDT, Finalizers: [kubernetes], Phase: Terminating&#xA;May 30 01:40:22.862: INFO: namespace: events-3047, total namespaces: 5, active: 4, terminating: 1&#xA;May 30 01:40:22.904: INFO: POD                                               NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 30 01:40:22.904: INFO: send-events-3ac4cdb7-0242-41a3-a533-01f9f32da136  e2e-test-peterhornyack-windows-node-group-jpxd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:26:32 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:30:14 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:30:14 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:26:32 -0700 PDT  }]&#xA;May 30 01:40:22.904: INFO: &#xA;May 30 01:40:22.904: INFO: Couldn&#39;t delete ns: &#34;events-3047&#34;: namespace events-3047 was not deleted with limit: timed out waiting for the condition, pods remaining: 1 (&amp;errors.errorString{s:&#34;namespace events-3047 was not deleted with limit: timed out waiting for the condition, pods remaining: 1&#34;})&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ntfs)][sig-windows] provisioning should access volume from different nodes" classname="Kubernetes e2e suite" time="0.001789473">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (ntfs)][sig-windows] volumes should be mountable" classname="Kubernetes e2e suite" time="0.001623143">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  StatefulSet with pod affinity [Slow] should use volumes on one node when pod has affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should access to two volumes with the same volume mode and retain data across pod recreation on different node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic Snapshot] snapshottable should create snapshot with defaults [Feature:VolumeSnapshotDataSource]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should serve a basic endpoint from pods  [Conformance]" classname="Kubernetes e2e suite" time="719.572574558">
          <failure type="Failure">/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;May 30 01:52:22.480: Couldn&#39;t delete ns: &#34;services-4216&#34;: namespace services-4216 was not deleted with limit: timed out waiting for the condition, pods remaining: 1 (&amp;errors.errorString{s:&#34;namespace services-4216 was not deleted with limit: timed out waiting for the condition, pods remaining: 1&#34;})&#xA;/usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:334</failure>
          <system-out>[BeforeEach] [sig-network] Services&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;May 30 01:40:22.909: INFO: &gt;&gt;&gt; kubeConfig: /usr/local/google/home/peterhornyack/.kube/config&#xA;�[1mSTEP�[0m: Building a namespace api object, basename services&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-network] Services&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84&#xA;[It] should serve a basic endpoint from pods  [Conformance]&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:696&#xA;�[1mSTEP�[0m: creating service endpoint-test2 in namespace services-4216&#xA;�[1mSTEP�[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-4216 to expose endpoints map[]&#xA;May 30 01:40:23.175: INFO: successfully validated that service endpoint-test2 in namespace services-4216 exposes endpoints map[] (41.522038ms elapsed)&#xA;�[1mSTEP�[0m: Creating pod pod1 in namespace services-4216&#xA;�[1mSTEP�[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-4216 to expose endpoints map[pod1:[80]]&#xA;May 30 01:40:27.638: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.415698696s elapsed, will retry)&#xA;May 30 01:40:30.892: INFO: successfully validated that service endpoint-test2 in namespace services-4216 exposes endpoints map[pod1:[80]] (7.670113825s elapsed)&#xA;�[1mSTEP�[0m: Creating pod pod2 in namespace services-4216&#xA;�[1mSTEP�[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-4216 to expose endpoints map[pod1:[80] pod2:[80]]&#xA;May 30 01:40:35.564: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (4.626011432s elapsed, will retry)&#xA;May 30 01:40:41.195: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (10.257466089s elapsed, will retry)&#xA;May 30 01:40:46.822: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (15.88454493s elapsed, will retry)&#xA;May 30 01:40:52.451: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (21.512723069s elapsed, will retry)&#xA;May 30 01:40:58.077: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (27.139406122s elapsed, will retry)&#xA;May 30 01:41:03.705: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (32.767121421s elapsed, will retry)&#xA;May 30 01:41:09.330: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (38.392090211s elapsed, will retry)&#xA;May 30 01:41:14.973: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (44.035163732s elapsed, will retry)&#xA;May 30 01:41:20.606: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (49.66802178s elapsed, will retry)&#xA;May 30 01:41:26.243: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (55.30562536s elapsed, will retry)&#xA;May 30 01:41:31.869: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (1m0.931348898s elapsed, will retry)&#xA;May 30 01:41:37.497: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (1m6.559388074s elapsed, will retry)&#xA;May 30 01:41:43.127: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (1m12.189207952s elapsed, will retry)&#xA;May 30 01:41:48.750: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (1m17.812151536s elapsed, will retry)&#xA;May 30 01:41:54.382: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (1m23.443967866s elapsed, will retry)&#xA;May 30 01:42:00.016: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (1m29.077978023s elapsed, will retry)&#xA;May 30 01:42:05.641: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (1m34.702968584s elapsed, will retry)&#xA;May 30 01:42:11.267: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (1m40.329240969s elapsed, will retry)&#xA;May 30 01:42:16.892: INFO: Unexpected endpoints: found map[f36a6439-ebd7-4c4d-aca8-9d39b63e18bf:[80]], expected map[pod1:[80] pod2:[80]] (1m45.953764563s elapsed, will retry)&#xA;May 30 01:42:20.272: INFO: successfully validated that service endpoint-test2 in namespace services-4216 exposes endpoints map[pod1:[80] pod2:[80]] (1m49.333933351s elapsed)&#xA;�[1mSTEP�[0m: Deleting pod pod1 in namespace services-4216&#xA;�[1mSTEP�[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-4216 to expose endpoints map[pod2:[80]]&#xA;May 30 01:42:20.402: INFO: successfully validated that service endpoint-test2 in namespace services-4216 exposes endpoints map[pod2:[80]] (83.768528ms elapsed)&#xA;�[1mSTEP�[0m: Deleting pod pod2 in namespace services-4216&#xA;�[1mSTEP�[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-4216 to expose endpoints map[]&#xA;May 30 01:42:20.489: INFO: successfully validated that service endpoint-test2 in namespace services-4216 exposes endpoints map[] (41.153498ms elapsed)&#xA;[AfterEach] [sig-network] Services&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150&#xA;May 30 01:42:20.546: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready&#xA;May 30 01:42:20.588: INFO: Condition Ready of node e2e-test-peterhornyack-windows-node-group-jpxd is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule 2019-05-30 01:41:23 -0700 PDT} {node.kubernetes.io/not-ready  NoExecute 2019-05-30 01:41:24 -0700 PDT}]. Failure&#xA;�[1mSTEP�[0m: Destroying namespace &#34;services-4216&#34; for this suite.&#xA;May 30 01:52:20.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;May 30 01:52:21.518: INFO: namespace: services-4216, resource: pods, items remaining: 1&#xA;May 30 01:52:22.396: INFO: namespace: services-4216, DeletionTimetamp: 2019-05-30 01:42:20 -0700 PDT, Finalizers: [kubernetes], Phase: Terminating&#xA;May 30 01:52:22.438: INFO: namespace: services-4216, total namespaces: 6, active: 4, terminating: 2&#xA;May 30 01:52:22.480: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS&#xA;May 30 01:52:22.480: INFO: pod2  e2e-test-peterhornyack-windows-node-group-jpxd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:40:30 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:42:19 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:42:19 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-30 01:40:30 -0700 PDT  }]&#xA;May 30 01:52:22.480: INFO: &#xA;May 30 01:52:22.480: INFO: Couldn&#39;t delete ns: &#34;services-4216&#34;: namespace services-4216 was not deleted with limit: timed out waiting for the condition, pods remaining: 1 (&amp;errors.errorString{s:&#34;namespace services-4216 was not deleted with limit: timed out waiting for the condition, pods remaining: 1&#34;})&#xA;[AfterEach] [sig-network] Services&#xA;  /usr/local/google/home/peterhornyack/go/src/github.com/pjh/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Nodes [Disruptive] Resize [Slow] should be able to add nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s multiple priority class scope (quota set to pod count: 2) against 2 pods with same priority classes." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using Deployment.extensions with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root with FSGroup [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] should concurrently access the single volume from pods on the same node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
  </testsuite>